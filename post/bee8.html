<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="IT JAVA CODER" />
       
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>Vision Transformer译文 |  Yore&#39;s Blog</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/images/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?82f869b65da41ff28f5159b24adaabf4";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


      <!-- mermaid -->
      
    <link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Vision Transformer译文"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Vision Transformer译文
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/post/bee8.html" class="article-date">
  <time datetime="2021-06-25T07:26:44.000Z" itemprop="datePublished">2021-06-25</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Transformer/">Transformer</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">20.5k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">86 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="视觉transformer综述"><strong>视觉Transformer综述</strong></h1>
<h1 id="摘要">摘要</h1>
<p>​
Transformer是一种基于最初应用在自然语言处理领域上的自注意力机制的深度神经网络。受Transformer强表征能力的启发，研究人员将它扩展到计算机视觉任务上。与卷积网络和循环网络等其他网络类型相比，基于Transformer的模型在各种视觉基准上表现出竞争性甚至更好的性能。在这篇文献综述，我们通过将这些可视化Transformer模型应用在不同类别的任务中，并且对它们的优缺点进行了分析。特别地，主要类别包括基本图像分类、高级视觉、低级视觉和视频处理。也会简要回顾计算机视觉中的自注意力机制，因为自注意力机制是Transformer的基本组成部分。有效的Transformer模型推动了Transformer进入实际应用。最后，讨论了可视化Transformer的进一步研究方向。</p>
<span id="more"></span>
<h1 id="引言">1. 引言：</h1>
<p>深层神经网络已经成为现代人工智能系统的基础设施。已经提出了各种网络类型来处理不同的任务。由线性层和非线性激活组成的多层感知机或者说全连接层是经典的神经网络。卷积神经网络[CNN]引入了卷积层和池化层，用于处理像图像这样移位不变的数据。循环神经网络[RNN]利用循环单元来处理序列数据或时间序列数据。Transformer是一种新提出的网络，主要运用自注意力机制提取内在特征。在这些网络中，Transformer是最近发明的神经网络，但却在广泛的人工智能应用领域显示出了的巨大潜力。</p>
<p>Transformer最初应用于自然语言处理(NLP)任务，并带来了显著的改进。例如，V
aswani等人首先提出了仅基于注意力机制的Transformer模型用于机器翻译和英语选区解析任务。Devlin等人介绍了一种新的语言表示模型，称为BERT，它通过对左右两个上下文的联合调节，从未标记的文本中预先训练一个转换器。BERT获得了当时11个自然语言处理任务的最好结果。Brown等人在45TB压缩明文数据上预训练了基于GPT-3模型的具有1750亿个参数的巨大Transformer模型，并且在不同类型的下游自然语言任务上实现了强性能而无需微调。这些基于Transformer的模型表现出很强的表现能力，并在自然语言处理领域取得了突破。</p>
<p>受自然语言处理中Transformer能力的启发，最近研究人员将Transformer扩展到计算机视觉任务。CNN过去是视觉应用的基本组成部分，但是Transformer正在展示它作为CNN之外的另一种选择的能力。Chen等人训练一个序列Transformer对像素进行自回归预测，并与CNN相比在图像分类任务上取得有竞争力的结果。ViT是Dosovitskiy等人最近提出的视觉Transformer模型。它将单纯的Transformer模型直接应用于小块图像序列，并在多个图像识别基准上获得最先进的性能。除了基本的图像分类之外，Transformer还被用来解决更多的计算机视觉问题，如对象检测、语义分割、图像处理和视频理解。由于其优异的性能，越来越多的基于Transformer的模型被提出用于改进各种视觉任务。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113030319.png" alt="image-20210615113030319"  /></p>
<p>基于Transformer的视觉模型如雨后春笋般涌现，这导致我们难以跟上新进展的速度。因此，对现有的视觉模型进行调查是很有必要的，并且也对整个社区是有益的。在这篇文章中，我们重点提供了视觉Transformer的最新进展的综合概述，并讨论了进一步改进的潜在方向。为了更好地存档和方便不同主题的研究人员，我们按照应用场景对Transformer模型进行分类，如表1所示。特别地，主要场景包括基本图像分类、高级视觉、低级视觉和视频处理。高级视觉处理图像[121]中所见内容的解释和使用，例如对象检测、分割和车道检测。有许多Transformer模型解决了这些高层次的视觉任务，如DETR
[14]，用于对象检测的可变形DETR [155]和用于分割的Max-DeepLab [126]。</p>
<p>低级图像处理主要涉及从图像(通常表示为图像本身)中提取描述[35]，其典型应用包括超分辨率、图像去噪和风格转换。低级视觉中很少有作品[17，92]使用Transformer，需要更多的研究。视频处理是计算机视觉中除了基于图像的任务之外的重要组成部分。由于视频的顺序特性，Transformer可以自然地应用于视频[154，144]。与传统神经网络CNNs或者RNNs相比，Transformer在这些任务上开始显示出竞争优势。在这里，我们对这些基于Transformer的可视化模型的工作进行了调查，以跟上这一领域的进展。视觉Transformer的发展时间表如图1所示，我们相信越来越多的优秀作品将被镌刻在里程碑上。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113127167.png" alt="image-20210615113127167"  /></p>
<p>论文的其余部分组织如下。第二节首先介绍自注意力机制和标准Transformer。我们在第3节中描述了自然语言处理中的Transformer方法，因为研究经验可能对视觉任务有益。接下来，第四部分是论文的主要部分，总结了图像分类、高级视觉、低级视觉和视频任务的视觉转换模型。我们还简要回顾了自注意力机制的CV和有效的Transformer方法，因为它们与我们的主题密切相关。最后，我们给出了结论并讨论了几个研究方向和挑战。</p>
<h1 id="transformer概述">2. Transformer概述：</h1>
<p>Transformer
[123]首次应用于自然语言处理中的机器翻译任务。如图2所示，它由一个编码器模块和一个解码器模块组成，具有几个相同结构的编码器/解码器，每个编码器由自注意力层和前馈神经网络组成，而每个解码器由自注意力层、编码解码器注意力层和前馈神经网络组成。在用Transformer翻译句子之前，句子中的每个单词都会被嵌入到一个512维的向量中</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113136453.png" alt="image-20210615113136453"  /></p>
<h2 id="自注意力层">2.1 自注意力层：</h2>
<p>在自注意力层，输入向量首先被转换成三个不同的向量，即查询向量q、关键向量k和值向量v，向量的维数均为512维。从不同输入得到的向量然后被打包成三个不同的矩阵Q、K、V。之后，通过以下步骤计算不同输入向量之间的注意函数(如图3左侧所示):</p>
<ol type="1">
<li><p>通过公式<span class="math inline">\(S = Q \bullet
K^{T}\)</span>计算不同输入向量之间的得分。</p></li>
<li><p>通过公式<span class="math inline">\(S_{n} =
\frac{S}{\sqrt{d_{k}}}\)</span>将梯度稳定性的分数标准化.</p></li>
<li><p>使用softmax函数将分数转换为概率: <span class="math inline">\(P =
softmax(S_{n})\)</span></p></li>
<li><p>通过公式<span class="math inline">\(Z = V \bullet
P\)</span>得到加权矩阵Z，整个过程可以统一为单个函数：<span
class="math inline">\(\text{Attention}(Q,\ K,\ V\ ) = \
\text{softmax}\left( \frac{Q \bullet K^{T}}{\sqrt{d_{k}}} \right)
\bullet V\)</span> （1）</p></li>
</ol>
<p>直觉上公式(1)比较简单，第一步计算两个不同向量之间的得分，得分是为了确定当在当前位置对单词进行编码时，我们对其他单词的关注程度。第二步，为了更好的训练，将分数标准化，使其具有更稳定的梯度。第三步，将分数转换成概率。最后，每个值向量乘以加总概率，具有较大概率的向量将被随后的层更多地关注。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113151684.png" alt="image-20210615113151684"  /></p>
<p>解码器模块中的编解码注意力层与编码模块中的自注意力层几乎相同，只是键矩阵K和值矩阵V是从编码器模块中导出的，查询矩阵Q是从前一层中导出的。</p>
<p>注意，上述过程与每个单词的位置无关，因此自注意力层缺乏捕捉一个句子中单词位置信息的能力。为了解决这个问题，在原始输入中添加了一个带有维度<span
class="math inline">\(d_{\text{model}}\)</span>的位置编码，以获得单词的最终输入向量。具体而言，该位置用以下等式编码:</p>
<p><span class="math inline">\({PE}(pos,2i) =
sin(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}})\)</span>（2）</p>
<p><span class="math inline">\(PE(pos,2i + 1) =
cos(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}})\)</span>
（3）</p>
<p>其中pos表示单词在句子中的位置，i表示位置编码的当前维度。</p>
<h2 id="多头注意力">2.2 多头注意力：</h2>
<p>自注意力层通过添加一种称为多头注意力的机制来进一步改进，以提高普通自注意力的性能。注意，对于一个给定的参考词，我们在通读句子的时候，往往要重点关注其他几个词。因此，尽管不影响其他位置的注意力也是同等重要的，但是单头自注意力层限制了集中在特定位置(或几个特定位置)的能力。这是通过给注意力层不同的表示子空间来实现的。具体来说，不同的头使用不同的查询、键和值矩阵，并且由于随机初始化，它们可以在训练后将输入向量投影到不同的表示子空间中。</p>
<p>具体来说，给定一个输入向量和头数h，输入向量首先被转换成三组不同的向量，即查询组、关键字组和值组。每组有h个向量，向量维度是<span
class="math inline">\(d_{q^{&#39;} } = d_{k^{&#39;} } = d_{v^{&#39;} } =
\frac{d_{\text{model} } }{h} =
64\)</span>。然后，从不同输入得到的向量被打包成三组不同的矩阵 <span
class="math inline">\({ \{ Q_{i}\} }_{i = 1}^{h}、{ \{ K_{i}\} }_{i =
1}^{h}、{ \{ V_{i}\} }_{i =
1}^{h}\)</span>，然后多头注意力的处理过程如下：</p>
<p><span class="math inline">\(\text{Multi}\text{Head}\left(
Q^{&#39;},K^{&#39;},V^{&#39;} \right) = Concat\left(
\text{head}_{1},\ldots,\text{head}_{h} \right)W^{0},\)</span> (4)</p>
<p><span class="math display">\[\text{where }\text{head}_{i} =
Attention(Q_{i},K_{i},V_{i})\]</span></p>
<p>where<span class="math inline">\(Q^{&#39;}是{ \{ Q_{i}\}}_{i =
1}^{h}的连接,K^{&#39;},V^{&#39;}同理\)</span>。<span
class="math inline">\(w^{0} \in \
R^{d_{\text{model}}*d_{\text{model}}}\)</span></p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113218314.png" alt="image-20210615113218314"  /></p>
<h2 id="transformer其他部分">2.3 Transformer其他部分：</h2>
<p><strong>编码器和解码器中的其他部分。</strong>如图4所示，为了加强信息流并获得更好的性能，在编码器和解码器的每个子层中添加了残差连接。随后是层规范化[4]。上述操作的输出可以描述为:</p>
<p><span class="math inline">\(\text{LayerNorm}\left( X\  + \
Attention(X) \right)\)</span>（5）</p>
<p>需要注意的是，这里使用X作为自注意力层的输入，因为查询、键和值矩阵Q、K和V都来自于同一个输入矩阵X</p>
<p><strong>前馈神经网络。</strong>在每个编码器和解码器的自注意力层之后采用前馈神经网络。具体来说，前馈神经网络由两个线性变换层和其中一个ReLU激活函数组成，可以表示为:</p>
<p><span class="math inline">\(\text{FFNN}(X) = \ w_{2}\sigma\left(
W_{1}X \right)\)</span> (6)</p>
<p>其中，w1、w2为两个线性变换层的两个参数矩阵，σ为ReLU激活函数。隐层的维数是2048。</p>
<p><strong>解码器的最后一层。</strong>解码器的最后一层的目的是将向量的堆栈转换回单词。这是通过一个线性层和一个softmax层来实现的。线性层将向量投射到具有<span
class="math inline">\(d_{\text{word}}\)</span>维的向量中，其中<span
class="math inline">\(d_{\text{word}}\)</span>是词汇表中的单词数。然后，利用softmax层将该向量转换为概率。</p>
<p>大多数用于计算机视觉任务的Transformer都利用了原Transformer的编码器模块。简而言之，它是一种新的特征选择器，不同于卷积神经网络(CNNs)和循环神经网络(RNNs)。相比CNN只关注局部特征，变压器能够捕捉到长距离特征，这意味着Transformer可以很容易地获取全局信息。与RNN的隐状态顺序计算相比，Transformer的自注意力层和全连接层的输出可以并行计算，且易于加速，因此效率更高。因此，进一步研究Transformer在自然语言处理和计算机视觉领域的应用具有重要意义。</p>
<h1 id="再看nlp中的transformer">3. 再看NLP中的Transformer:</h1>
<p>在Transformer出现之前，具有附加功能的循环神经网络(例如GRU[26]和LSTM[50])增强了大多数最先进的语言模型。然而，在RNN中，信息流需要从前一个隐藏状态到下一个隐藏状态的顺序处理，这就妨碍了训练过程中的加速和并行化，从而阻碍了RNN处理更长的序列或构建更大的模型的潜力。2017年，Vaswani等人[123]提出了Transformer，这是一种全新的编码器-解码器架构，完全建立在多头自注意力机制和前馈神经网络上，旨在解决序列到序列的自然语言任务(如机器翻译)，轻松获取全局依赖关系。Transformer的成功证明，仅利用注意力机制就可以获得与专注的RNN相当的性能。此外，Transformer的体系结构支持大规模并行计算，允许在更大的数据集上进行训练，从而导致大量用于自然语言处理的大型预训练模型(PTM)。</p>
<p>BERT[29]及其变体(如SpanBERT [63]，
RoBERTa[82])是一系列基于多层Transformer编码器架构的PTM。在BERT训练前阶段，分别对图书语料库[156]和英文维基百科数据集进行两项任务:1)遮挡语言建模(MLM)，首先随机遮挡输入中的一些标记，然后训练模型进行预测;2)下一个句子预测使用成对句子作为输入，并预测第二句是否为文档中的原始句子。经过预训练后，BERT可以通过在较宽的下游任务范围内单独添加一个输出层来进行微调。更具体地说，在执行序列级任务(例如，情感分析)时，BERT使用第一个标记的表示进行分类;而对于令牌级任务(例如，名称实体识别)，所有令牌都被送入softmax层进行分类。在发布时，BERT在11个自然语言处理任务上取得了最先进的结果，在预先训练的语言模型中建立了一个里程碑。生成式预训练Transformer系列(例如，GPT
[99]，
GPT-2[100])是另一种基于Transformer解码器架构的预训练模型，它使用遮挡的自注意力机制。GPT系列与BERT系列的主要区别在于训练前的方式。与BERT不同，GPT系列是经过从左到右(LTR)语言建模预处理的单向语言模型。此外，句子分隔符(SEP)和分类符号(CLS)只参与GPT的微调阶段，BERT在训练前学习了这些嵌入。由于GPT的单向预训练策略，它在许多自然语言生成任务中表现出了优越性。最近，一个巨大的基于Transformer的模型，GPT-3，具有令人难以置信的1750亿个参数被引入[10]。通过对45TB压缩明文数据进行预训练，GPT-3能够直接处理不同类型的下游自然语言任务，无需微调，在许多自然语言数据集上实现了强大的性能，包括自然语言理解和生成。除了上述基于Transformer的PTMs，自Transformer引入以来，许多其他模型也被提出。因为这不是我们调查的主要主题，我们只是在表2中为感兴趣的读者列出了一些有代表性的模型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113502666.png" alt="image-20210615113502666"  /></p>
<p>除了在大型语料库上训练用于一般自然语言处理任务的PTMs之外，基于Transformer的模型已经应用于许多其他自然语言处理相关领域或多模态任务。BioNLP域。基于Transformer的模型已经超过了许多传统的生物医学方法。BioBERT[69]使用Transformer架构进行生物医学文本挖掘任务;SciBERT[7]是由训练Transformer针对114M涵盖生物医学和计算机科学领域的科学文章开发的，旨在更精确地执行与科学领域相关的自然语言处理任务;
Huang等人[55]提出ClinicalBERT利用Transformer开发和评估临床记录的连续表示，作为一个副作用，ClinicalBERT的注意力图可以用来解释预测，从而发现不同医疗内容之间的高质量联系。Multi-Modal
Tasks。由于Transformer在基于文本的NLP任务上的成功，许多研究致力于探索基于Transformer处理多模式的任务(例如，视频文本，图像文本和音频文本)的可能性。VideoBERT[115]使用一个基于CNN的模块对视频进行预处理以获得表示标记，基于此，Transformer编码器被训练学习用于下游任务的视频-文本表示，如视频标题。VisualBERT[72]和VL-BERT[114]提出了单流统一Transformer来捕获视觉元素和图像-文本关系，用于诸如视觉问答(VQA)和视觉共性推理(VCR)等下游任务。此外，一些研究(如Speech
bert[24])探索了使用Transformer编码器编码音频和文本对以处理语音问答(SQA)等自动文本任务的可能性。</p>
<p>基于Transformer的模型在各种自然语言处理和相关任务上的快速发展显示了其结构优势和通用性。这使得Transformer成为自然语言处理之外的许多其他AI领域的通用模块。接下来的部分将集中讨论Transformer在过去两年中出现的各种计算机视觉任务中的应用。</p>
<h1 id="视觉transformer">4. 视觉Transformer：</h1>
<p>在本节中，我们将全面回顾计算机视觉中基于Transformer的模型，包括在图像分类、高级视觉、低级视觉和视频处理中的应用。简要介绍了自注意力机制和模型压缩方法在高效Transformer中的应用。</p>
<h2 id="图像分类">4.1 图像分类：</h2>
<p>受Transformer在自然语言处理方面巨大成功的启发，一些研究人员试图检验类似的模型是否可以学习有用的图像表示。图像是一种比文本维度更高、噪声更大、冗余度更高的形态，因此生成式建模比较困难。iGPT[18]和ViT[31]是两个纯粹使用Transformer进行图像分类的模型。</p>
<h3 id="igpt">4.1.1 iGPT:</h3>
<p>从原始的图像生成预训练方法开始已经有很长一段时间了，Chen等人[18]结合自监督方法的最新进展重新审视了这类方法。该方法包括一个训练前阶段，然后是一个微调阶段。在训练前，我们探讨了自回归和BERT目标。此外，在自然语言处理中，使用序列转换体系结构代替语言标记来预测像素。当与早期停止结合使用时，预训练可以被视为一个有利的初始化或正则化。在微调过程中，他们将一个小的分类头添加到模型中，用于优化分类目标，并适应所有的权重。</p>
<p>给定一个由高维数据X组成的未标记数据集X =
(x1,...,xn)。他们通过最小化数据的负对数似然来训练模型:</p>
<div data-align="center">
<span class="math inline">\(L_{\text{AR}} = E\lbrack -
log(p(x))\rbrack\)</span> (7)
</div>
<p>其中p(x)是图像数据的密度，可以建模为:</p>
<p><span class="math inline">\(p(x) = \ \prod_{i =
1}^{n}{p(x_{\pi_{i}}|x_{\pi_{1}},\ldots,x_{\pi_{i -
1}},\theta)}\)</span> (8)</p>
<p>他们还考虑了BERT目标，标记一个子序列样本M⊂(1,n),每个索引独立且有0.15的概率出现在
M中， M叫做BERT遮挡,和模型训练通过最小化的负对数似然"遮挡"元素<span
class="math inline">\(x_{M}\)</span> ，以 "非遮挡"的<span
class="math inline">\(x_{\lbrack 1,n\rbrack\backslash
M}\)</span>为条件:</p>
<p><span class="math inline">\(L_{\text{BERT}} = EE\sum_{i \in
M}^{}{\lbrack - \log{p\left( x_{i}|x_{\lbrack 1,n\rbrack\backslash M}
\right)}\rbrack}\)</span> (9)</p>
<p>在预训练中，他们选择<span class="math inline">\(L_{\text{AR}}\ 或者\
L_{\text{BERT}}\)</span>中的一个，并尽量降低损失。</p>
<p>他们使用GPT-2
[100]公式的Transformer解码器块。特别地，层规范化先于注意力和多层感知器(MLP)操作，并且所有操作都严格地位于残差路径上。跨序列元素的唯一混合发生在注意力操作中，为了确保在训练增强现实目标时进行适当的调节，他们将标准的上三角掩模应用于n×n矩阵的注意力逻辑。当使用BERT目标时，不需要注意逻辑遮挡:在将内容嵌入应用到输入序列之后，它们将位置归零。</p>
<p>在最后的转换器层之后，他们应用层规范化，并从输出中学习投影，以对每个序列元素的条件分布进行参数化。当训练BERT时，他们只是简单地忽略未加遮挡位置的逻辑。</p>
<p>在微调过程中，它们平均汇集最终层规范化的输出，跨越序列维度提取每个示例特征的三维向量:</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114108165.png"
alt="image-20210615114108165" />
<figcaption aria-hidden="true">image-20210615114108165</figcaption>
</figure>
<p>他们从<span
class="math inline">\(f_{L}\)</span>类逻辑中学习投影，用于最小化交叉熵损失<span
class="math inline">\(L_{\text{CLF}}\)</span>。在实践中，他们根据经验发现联合目标<span
class="math inline">\(L_{\text{GEN}} +
L_{\text{CLF}}\)</span>的效果更好，其中<span
class="math inline">\(L_{\text{GEN\ }} \in \left\{
L_{\text{AR}},L_{\text{BERT}} \right\}\)</span></p>
<p><img src="C:\Users\Yore\AppData\Roaming\Typora\typora-user-images\image-20210615114117893.png" alt="image-20210615114117893"  /></p>
<h3 id="vit">4.1.2 ViT:</h3>
<p>最近，Dosovitskiy等人[31]提出了一种纯净的Transformer模型，即Vision
Transformer(ViT)，当直接应用于图像块序列时，它在图像分类任务中表现良好。他们尽可能地遵循原始Transformer的设计。图5显示了ViT的框架。</p>
<p>为了处理2D图像，图像<span class="math inline">\(x \in
R^{H*W*C}\)</span>被重新成形为一系列平坦的2D块<span
class="math inline">\(x_{p} \in
R^{N*(P^{2}*C)}\)</span>。(H，W)是原始图像的分辨率,
(P，P)是每个图像块的分辨率，<span class="math inline">\(N = \
\frac{\text{HW}}{P^{2}}\)</span>则是Transformer的有效序列长度。由于Transformer在其所有层中使用恒定的宽度，可训练的线性投影将每个矢量化路径映射到模型维度D，其输出被称为块嵌入。</p>
<p>类似于BERT的[class]记号，可学习的嵌入被用于嵌入块的序列，其在Transformer编码器输出端的状态用作图像表示。在预训练和微调过程中，分类头的大小相同。此外，1D位置嵌入被添加到块嵌入以保留位置信息。他们探索了位置嵌入的不同2D感知变体，这并没有获得比标准1D位置嵌入更大的收益。联合嵌入作为编码器的输入被切断。值得注意的是，视觉Transformer仅采用标准Transformer的编码器，并且Transformer编码器的输出后面是MLP磁头。</p>
<p>通常情况下，ViT首先在大型数据集上进行预训练，并针对较小的下游任务进行微调。为此，移除预包含的预测头，并附加一个零初始化的D
×
K前馈层，其中K是下游类别的数量。相比于预训练，通常时是对高分辨率微调有益的。当输入更高分辨率的图像时，图片快大小保持不变，这导致更大的有效序列长度。视觉Transformer可以处理任意序列长度，然而，预先训练的位置嵌入可能不再有意义。作者根据它们在原始图像中的位置对预先训练的位置嵌入进行插值。请注意，分辨率调整和图片块提取是手动将图像2D结构的感应偏差注入视觉Transformer的唯一点。</p>
<p>当在中型数据集(如ImageNet)上进行训练时，这种模型产生的结果一般，精度比同等规模的ResNets低几个百分点。Transformer缺乏CNNs固有的一些归纳偏差，如翻译等方差和局部性，因此在数据量不足的情况下训练时不能很好地概括。然而，如果模型是在大数据集(14M-300M图像)上训练的，图片就会改变。作者发现大规模训练胜过归纳偏见。Transformer在经过足够规模的预训练并转移到数据点较少的任务时，可以获得出色的效果。在JFT300M数据集上预处理的视觉Transformer接近或超过了多种图像识别基准的先进水平，在ImageNet上达到88.36%的准确率，在CIFAR-10上达到99.50%，在CIFAR-100上达到94.55%，在VTAB套件的19项任务中达到77.16%。iGPT和ViT的详细结果如表3所示。</p>
<p><img src="C:\Users\Yore\AppData\Roaming\Typora\typora-user-images\image-20210615114130119.png" alt="image-20210615114130119"  /></p>
<p>总之，iGPT回忆了生成性预训练方法，并将其与自监督方法相结合，结果并不十分令人满意。ViT取得了更好的结果，尤其是当它使用更大的数据集(JFT-300)时。但是ViT的结构和NLP中的Transformer基本相同，如何表述块内和块间的关系仍然是一个具有挑战性的问题。此外，相同大小的块在ViT中被同等对待。众所周知，每个块的复杂性是不同的，这个特点现在还没有被充分利用。</p>
<h2 id="高级视觉">4.2 高级视觉：</h2>
<p>近来，人们对采用Transformer进行高级计算机视觉任务越来越感兴趣，例如目标检测[15，155，23]，车道检测[81]和分割[129，126]。在本节中，我们对这些方法进行了回顾。</p>
<h3 id="目标检测">4.2.1 目标检测：</h3>
<p>根据采用Transformer架构的模块，基于Transformer的目标检测方法可以粗略地分为基于颈部、基于头部和基于框架的方法。</p>
<p>像特征金字塔网络(FPN)
[77]这样的多尺度特征融合模块(在现代检测框架中被称为颈部)已经被广泛用于目标检测以获得更好的检测性能。张等人[145]提出传统方法不能交互跨尺度特征，因此提出特征金字塔变换(FPT)来充分利用跨空间和尺度的特征交互。FPT由自transformer、地transformer和渲染transformer三种类型的transformer组成，分别对特征金字塔的自层次、自顶向下和自底向上路径的信息进行编码。FPT基本上利用Transformer中的自注意力模块来增强特征金字塔网络的特征融合。</p>
<p>预测头在目标检测器中起着重要的作用。现有的检测方法通常利用单一的视觉表示(例如，边界框和角点)来预测最终结果。Chi等人[23]提出了桥接视觉表征(BVR)，通过多头注意模块将不同的异质表征组合成一个单一的表征。具体地，主表示被视为查询输入，辅助表示被视为键输入。通过类似于Transformer中的注意力模块，可以获得用于主表示的增强特征，这桥接了来自辅助表示的信息，并且有利于最终的检测性能。</p>
<p>与上述利用Transformer增强现代探测器特定模块的方法不同，卡里昂[15]重新设计了物体探测框架，并提出了detection
transformer
(DETR)，这是一种简单且完全端到端的物体探测器。DETR将目标检测任务视为一个直观的集合预测问题，摆脱了传统的手工制作组件像anchor生成和非最大抑制(NMS)后处理。如图6所示，DETR从CNN主干开始，从输入图像中提取特征。为了用位置信息来补充图像特征，在被馈送到编码-解码转换器之前，固定位置编码被添加到展平特征。transformer解码器使用来自编码器的嵌入以及N个学习的位置编码(对象查询)，并产生N个输出嵌入，其中N是图像中预先定义的参数和最大数量的对象。最终的预测是用简单的前馈网络(FFN)计算的，它包括边界框坐标和类别标签，以指示对象的特定类别或没有对象。不像原来的Transformer顺序产生预测，DETR同时并行解码N个对象。DETR采用一种二分匹配算法来分配预测对象和真实对象。如等式所示。(11)中，Hungarian损失被用来计算所有匹配对象对的损失函数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114148519.png" alt="image-20210615114148519"  /></p>
<p>DETR在目标检测方面表现出令人印象深刻的性能，其准确性和速度与COCO基准上流行且公认的更快的R-CNN基线相当。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114200407.png" alt="image-20210615114200407"  /></p>
<p>DETR是一个基于Transformer的物体检测框架的新设计，并启发社区开发完全端到端的检测器。然而，普通的DETR也带来了一些挑战，例如，更长的训练时间表和小物体的不良表现。Zhu等人[155]提出的可变形是解决上述问题的一种常用方法，大大提高了检测性能。变形注意模块不是通过Transformer中的原始多头注意来查看图像特征地图上的所有空间位置，而是关注参考点周围的一小组关键位置。这样，计算复杂度大大降低，也有利于快速收敛。更重要的是，可变形的注意模块可以容易地应用于融合多尺度特征。变形DETR比DETR的性能好，训练成本低10倍，推理速度快1.6倍。一些额外的改进也适用于可变形DETR，包括有效的迭代包围盒细化方法和两阶段方案，这导致进一步的性能增益。</p>
<p>针对计算复杂度较高的问题，Zhang等人[153]提出了一种自适应聚类变换器(ACT)来降低预先训练的的计算开销，无需任何训练过程。ACT使用局部敏感哈希方法自适应地对查询特征进行聚类，并将注意力输出广播给由所选原型表示的查询。通过用建议的ACT代替预先训练的DETR模型的自我注意力模块而无需任何再训练，计算成本可以大大降低，而精度几乎没有下降。此外，通过利用多任务知识提取(MTKD)方法，可以进一步降低性能下降，该方法利用原始transformer提取具有几个微调时期的ACT模块。</p>
<p>Sun等[117]研究了模型的慢收敛问题，揭示了Transformer解码器中的交叉注意模块是其背后的主要原因。为此，提出了DETR的仅编码器版本，并且在检测精度和训练收敛方面实现了相当大的改进。此外，为了更稳定的训练和更快的收敛，设计了一种新的二分匹配方案。提出了两种基于Transformer的集合预测模型，即TSP-FCOS模型和TSPRCNN模型，这两种模型比原DETR模型具有更好的性能。</p>
<p>受自然语言处理中预训练transformer方案的启发，Dai等人[28]提出了一种无监督预训练(UP-DETR)的目标检测方法。具体而言，提出了一种新的无监督前置任务随机查询块检测来预处理DETR模型。通过该方案，IP-DETR在相对较小的数据集上，即PASCAL
VOC，大幅度提高了检测精度。在训练数据充足的COCO基准上，UP-DETR仍优于DETR，证明了无监督预训练方案的有效性。</p>
<h3 id="分割">4.2.2 分割：</h3>
<p>DETR
[15]可以通过在解码器上附加一个遮挡头来自然地扩展全景分割任务，并获得有竞争力的结果。Wang等人[126]提出了Max-DeepLab，利用遮挡变换直接预测全景图像分割结果，不需要盒子检测等代理子任务。与DETR类似，Max-DeepLab以端到端的方式简化了全景分割任务，并直接预测了一组不重叠的遮挡和相应的标签。利用PQ损失来训练模型。此外，与以前在CNN主干上堆叠transformer的方法不同，Max-DeepLab采用了双路框架，以便更好地将CNN与transformer结合起来。</p>
<p>Wang等人[129]提出了一种基于变换的视频实例分割(VisTR)模型，该模型以一系列图像作为输入，并产生相应的实例预测结果。提出了一种实例序列匹配策略，将预测与真值相匹配。为了获得每个实例的遮挡序列，VisTR利用实例序列分割模块来累积来自多个帧的遮挡特征并且用3D
CNN分割遮挡序列。</p>
<p>还有一种尝试是将Transformer用于细胞实例分割[95]，其基于DETR全景分割模型。所提出的Cell-DETR还增加了跳跃连接，以在分割头中桥接CNN主干和CNN解码器的特征，以获得更好的融合特征。Cell-DETR展示了显微图像细胞实例分割的最新性能。</p>
<p>Zhao等[150]设计了一种用于处理点云的新型Transformer架构(Point
Transformer)。所提出的自注意力层对于点集的排列是不变的，因此适用于点集处理任务。Point
Transformer)在三维点云语义分割任务中表现出很强的性能。</p>
<h3 id="车道检测">4.2.3 车道检测：</h3>
<p>在PolyLaneNet
[119]的基础上，Liu等人[81]提出了利用Transformer网络学习全局上下文来提高弯道检测的性能。与PolyLaneNet相似，该方法(LSTR)将车道检测视为用多项式拟合车道的任务，并使用神经网络来预测多项式的参数。为了捕捉车道和全局环境的细长结构，LSTR在体系结构中引入了Transformer网络，以处理由卷积神经网络提取的低级特征。此外，LSTR使用Hungarian损失优化网络参数。如[81]所示，LSTR只有PolyLaneNet网络参数的0.2倍，和PolyLaneNet相比高2.82%的精度和3.65倍的FPS。Transformer网络、卷积神经网络和Hungarian损失的结合实现了一个微小、快速、精确的车道检测框架。</p>
<h2 id="低级视觉">4.3 低级视觉：</h2>
<p>除了高级视觉任务，很少有工作将Transformer应用于低级视觉领域，如图像超分辨率、生成等。与输出为标签或盒子的分类、分割和检测相比，低层任务往往以图像作为输出(如高分辨率或去噪图像)，这更具挑战性。</p>
<p>Parmar等人[92]在概括Transformer模型方面迈出了第一步，以制定图像转换和生成任务，并提出了图像Transformer。图像Transformer由两部分组成:用于提取图像表示的编码器和用于生成像素的解码器。对于值为0-255的每个像素，学习256
×
d维嵌入，用于将每个值编码为d维向量，该向量作为编码器的输入。编码器和解码器的结构与[123]中的相同。解码器中各层的详细结构如下图7所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114259935.png" alt="image-20210615114259935"  /></p>
<p>每个输出像素<span
class="math inline">\(q^{&#39;}\)</span>通过计算输入像素q和先前生成的像素m1、m2，...利用位置嵌入p1、p2，....对于图像条件生成，例如超分辨率和修复，使用编码器-解码器架构，其中编码器的输入是低分辨率图像或损坏的图像。对于无条件和有条件生成(即图像噪声)，只有解码器用于输入噪声矢量。由于解码器的输入是以前生成的像素，在生成高分辨率图像时会带来很大的计算成本，因此提出了一种局部自注意力方案，该方案只使用最近生成的像素作为解码器的输入。结果，图像转换器在图像生成和翻译任务上可以达到与基于CNN的模型的竞争性能，这表明了基于Transformer的模型在低水平视觉任务上的有效性。</p>
<p>与使用每个像素作为Transformer模型的输入相比，最近的工作使用图片快(像素集)作为输入。Yang等人[135]提出了用于图像超分辨率的纹理变换网络(TTSR)。他们在基于参考的图像超分辨率问题中使用Transformer架构，其目的是将相关纹理从参考图像转移到低分辨率图像。以低分辨率图像和参考图像为查询Q和关键K，相关性<span
class="math inline">\(r_{i,j}\)</span>的计算通过每一个图像块Q中的<span
class="math inline">\(q_{i}和K中的K_{i}\)</span>用如下公式计算：</p>
<p><span class="math inline">\(r_{i,j} = \  &lt;
\frac{q_{i}}{||q_{i}||},\frac{k_{i}}{||k_{i}||} &gt;\)</span> (12)</p>
<p>然后提出了一种高分辨率特征选择模块，根据参考图像选择高分辨率特征V，利用相关性匹配低分辨率图像。计算方法是:</p>
<p><span class="math inline">\(h_{i} = \
\underset{i}{\arg\max}r_{i,j}\)</span> (13)</p>
<p>那么最相关的图片块是<span class="math inline">\(t_{i} = \
v_{\text{hi}}\)</span>，其中T中的<span
class="math inline">\(t_{i}\)</span>是转移的特征。之后，软注意力模块用于将V转换为低分辨率特征f。软注意力可以通过以下方式计算：</p>
<p><span class="math inline">\(s_{i} = \ \max_{i}r_{i,j}\)</span>
(14)</p>
<p>因此，将高分辨率纹理图像转换为低分辨率图像的等式可以表述为:</p>
<p><span class="math inline">\(F_{\text{out}} = F +
Conv(Concat(F,T))\bigodot S\)</span> (15)</p>
<p>其中<span
class="math inline">\(F_{\text{out}}\)</span>和F代表低分辨率图像的输出和输入特征，S代表软注意，T代表从高分辨率纹理图像转移的特征。通过引入基于Transformer的架构，TTSR可以成功地将纹理信息从高分辨率参考图像传输到低分辨率图像，以完成超分辨率任务。</p>
<p>上述方法在单个任务上使用Transformer模型，而等人[17]提出图像处理Transformer(IPT)通过使用大规模预训练来充分利用Transformer的优势，并在包括超分辨率、去噪和降额在内的多个图像处理任务中实现最先进的性能。如图8所示。IPT由多头、编码器、解码器和多尾组成。针对不同的图像处理任务，引入了多头多尾结构和任务嵌入。这些特征被分成小块以放入编码器-解码器结构中，然后输出被整形为具有相同大小的特征。由于Transformer模型在大规模预训练方面显示出优势，IPT使用ImageNet数据集进行预训练。具体来说，从ImageNet下载的图像通过手动添加噪声，雨带或者下采样降级为生成的损坏图像，然后将退化图像作为IPT的输入，将干净图像作为输出的优化目标。为了提高IPT模型的泛化能力，引入了自监督方法。然后使用相应的头部、尾部和任务嵌入对每个任务的训练模型进行微调。IPT极大地提高了图像处理任务的性能(例如，图像去噪任务中的2dB)，这证明了基于Transformer的模型在低层视觉领域的巨大潜力。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114317649.png" alt="image-20210615114317649"  /></p>
<h2 id="视频处理">4.4 视频处理：</h2>
<p>Transformer在基于序列的任务上表现惊人，尤其是在NLP任务上。在计算机视觉中，空间和时间维度信息在视频任务中受到青睐。因此，Transformer被应用于许多视频任务，如帧合成[83]，动作识别[41]和视频检索[80]。</p>
<h3 id="高级视频处理">4.4.1 高级视频处理:</h3>
<p><strong>行人行为识别。</strong>视频人体动作任务是指在视频中识别和定位人体动作。背景在识别人类行为中起着至关重要的作用。Rohit等人提出动作转换器[41]来模拟感兴趣的人和周围事物之间的潜在关系。具体来说，I3D被用作提取高级特征映射的主干。通过感兴趣区域池从中间特征图中提取的特征被视为查询()。从中间特征计算键值（K）。自注意力机制由三部分组成，并输出分类和回归预测。Lohit等人[84]提出了一个可解释的可微模块，称为时空Transformer网络，以减少类内方差和增加类间方差。Fayyaz和
Gall提出了一种时空Transformer，用于在弱监督设置下执行动作识别任务。</p>
<p><strong>人脸对齐。</strong>基于视频的人脸对齐任务旨在定位面部标志。时间相关性和空间信息对最终性能很重要。然而，前一种方法不能捕获连续帧上的时间信息和静止帧上的补充空间信息。因此，Liu等人[80]使用双流Transformer网络来分别学习时间和空间特征。两个流以端到端的方式被联合优化，并且特征被加权以获得最终的预测。</p>
<p><strong>视频检索。</strong>基于内容的视频检索的关键是找到视频之间的相似性。要克服这些缺点，只需利用视频级特征。Shao等人[110]建议使用Transformer来建模范围语义相关性。此外，引入监督对比学习策略进行硬否定挖掘。基准数据集上的结果证明了性能和速度优势。Gabeur等人[39]提出了一种多模态Transformer来学习不同的跨模态线索，从而表示视频。</p>
<p><strong>动作识别。</strong>动作识别是指识别一个群体中的一个人的动作。以前解决这个问题的方法是基于单个人的位置。Gavrilyuk等人提出了一个actor-transformer
[40]架构来学习表示。actor-transformer将2D和3D网络生成的静态和动态表示作为输入。Transformer的输出是预测的动作。</p>
<p><strong>视频目标检测。</strong>
为了从视频中检测对象，需要全局和局部信息。Chen等人介绍了内存增强的全局-局部聚合(MEGA)
[19]来捕获更多的内容。代表性特征提高了整体性能，并解决了无效和不足的问题。Yin等人[138]提出了一种时空transformer来聚集空间和时间信息。与另一个空间特征编码组件一起，这两个组件在3D视频对象检测任务中表现良好。</p>
<p><strong>多任务学习。</strong>未剪辑的视频通常包含许多与目标任务无关的帧。因此，挖掘相关信息，去除冗余信息至关重要。为了应对未剪辑视频上的多任务学习，Seong等人采用视频多任务Transformer网络[109]来提取信息。对于CoVieW数据集，任务是场景识别、动作识别和重要性分数预测。ImageNet和Places365上的两个预训练网络提取场景特征和对象特征。在类别转换矩阵(CCM)的帮助下，多任务Transformer被堆叠以融合特征。</p>
<h3 id="低级视频处理">4.4.2 低级视频处理：</h3>
<p><strong>帧/视频合成。</strong>帧合成任务是指在两个连续帧之间或在一个帧序列之后合成帧。视频合成任务旨在合成视频。Liu等人提出了Conv
Transformer
[83]，它包括五个部分:特征嵌入、位置编码、编码器、查询解码器和合成前馈网络。与基于LSTM的作品相比，Conv
Transformer以更具可并行性的架构实现了更好的效果。Schatz等人[108]使用一个递归的Transformer网络从新的角度合成人类行为。</p>
<p><strong>视频修复。</strong>视频修复任务旨在完成帧中缺失的区域.这项具有挑战性的任务需要沿着空间和时间维度合并信息。Zeng等人为此任务提出了一个时空Transformer网络[144]。将所有输入帧作为输入，并并行填充它们。该网络的优化采用时空对抗损失函数。</p>
<h3 id="多模态">4.4.3 多模态：</h3>
<p><strong>视频字幕/摘要。</strong>视频字幕任务的目标是为未剪辑的视频生成文本。事件检测和描述模块是两个主要部分。Zhou等人[154]提出了一种端到端优化的transformer来解决密集视频字幕任务。编码器将视频转换成表示形式。提议解码器从编码中生成事件提议。字幕解码器用建议屏蔽编码并输出描述。Bilkhu等人[9]使用C3D和I3D网络提取特征，并使用transformer生成预测。该算法在单一总结任务和密集总结任务上都表现良好。Li等人[71]利用基于纠缠注意力(ETA)模块的transformer来处理图像字幕任务。Sun等人[29]提出了一个视觉语言框架来学习没有监督的代表。该模型可以应用于许多任务，包括视频字幕、动作分类等。</p>
<h2 id="计算机视觉的自注意力">4.5 计算机视觉的自注意力：</h2>
<p>在上面的章节中，我们已经回顾了将Transformer架构用于可视化任务的方法。自注意力是transformer的关键部分。在这一节中，我们深入研究了基于自注意力的方法在计算机视觉中的挑战性任务，例如，语义分割，实例分割，对象检测，关键点检测和深度估计。我们从第4.5.1节中的自注意力算法开始，并在第4.5.2节中总结了将自注意力用于计算机视觉的现有应用。</p>
<h3 id="自注意力的一般公式">4.5.1 自注意力的一般公式：</h3>
<p>用于机器翻译的自注意力模块[123]通过关注所有位置并在嵌入空间中对它们进行加权求和来计算序列中一个位置处的响应，这可以被视为计算机视觉中可应用的非局部过滤操作[128，11]的一种形式。</p>
<p>我们遵循惯例[128]制定自注意力模块。给定输入信号(例如，图像、序列、视频和特征)<span
class="math inline">\(X \in R^{n \times c}\)</span> 其中n = h ×
w表示特征中的像素数，c表示通道数。输出信号通过如下公式计算：</p>
<p><span class="math inline">\(y_{i} = \ \frac{1}{C(x_{i})}\
\sum_{\forall\ j}^{}{f(x_{i},x_{j})g(x_{j})}\)</span> (16)</p>
<p>其中)<span class="math inline">\(x_{i} \in R^{1 \times
c}\)</span>和<span class="math inline">\(y_{i} \in R^{1 \times
c}\)</span>分别表示输入信号X和输出信号Y的位置(例如，空间、时间和时空)。下标j是枚举所有位置的索引。成对函数f计算表示关系，如i和所有j之间的亲和力。函数g计算位置j处输入信号的表示。响应用因子<span
class="math inline">\(C(x_{i})\)</span>归一化。</p>
<p>请注意，成对函数f有许多选择，例如，高斯函数的简单扩展可用于计算嵌入空间中的相似性,
因此，函数f可以表示为</p>
<p><span class="math inline">\(f\left( x_{i},x_{j} \right) = \
e^{\theta(x_{i}){\phi(x_{j})}^{T}}\)</span> (17)</p>
<p>其中θ和φ可以是任何嵌入层。如果我们考虑线性嵌入形式的θ，φ，g，<span
class="math inline">\(\theta(X) = \ X \bullet W_{\theta}\)</span>，<span
class="math inline">\(\phi(X) = \ X \bullet W_{\phi}\)</span>，<span
class="math inline">\(g(X) = \ X \bullet W_{g}\)</span> 其中<span
class="math inline">\(W_{\theta} \in R^{c \times d_{k}}\)</span></p>
<p><span class="math inline">\(W_{\phi} \in R^{c \times
d_{\phi}}\)</span>，<span class="math inline">\(W_{g} \in R^{c \times
d_{v}}\)</span>.并将归一化因子设置为<span
class="math inline">\(\sum_{\forall\ j}^{}{f\left( x_{i},x_{j}
\right)}\)</span></p>
<p>则公式 16可改写为:</p>
<p><span class="math inline">\(y_{i} = \
\frac{e^{x_{i}w_{\theta,i}w_{\theta,j}^{T}x_{j}^{T}}}{\sum_{j}^{}e^{x_{i}w_{\theta,i}w_{\theta,j}^{T}x_{j}^{T}}}x_{j}w_{g,j}\)</span>
(18)</p>
<p>其中 <span class="math inline">\(W_{\theta,i} \in R^{c \times
1}\)</span>是权值矩阵<span
class="math inline">\(W_{\theta}\)</span>的第i行。对于给定的一个下标i，<span
class="math inline">\(\frac{1}{C(x_{i})}\text{\ f}\left( x_{i},x_{j}
\right)\)</span>成为沿维度j的softmax输出，因此公式可进一步改写为:</p>
<p><span class="math inline">\(Y = \ \text{soft}\max\left(
XW_{\theta}W_{\phi}^{T}X \right)g(X)\)</span> (19)</p>
<p>其中 <span class="math inline">\(Y \in R^{n \times c}\)</span>
是与X大小相同的输出信号，与transformer模型的查询、键和值的表示<span
class="math inline">\(Q = XW_{q}\text{\ \ }K = XW_{k}\text{\ \ }V =
XW_{v}\)</span>相比较，一旦<span class="math inline">\({W_{q} = \text{\
W}}_{\text{θ\ }}\ {W_{k} = \text{\ W}}_{\phi}\ {W_{v} = \text{\
W}}_{g}\)</span> 公式19可以用如下公式表示：</p>
<p><span class="math inline">\(Y = \ \text{soft}\max\left( QK^{T}
\right)V = Attention(Q,K,V)\ \)</span> (20)</p>
<p>自注意力模块为机器翻译提出的与以上为计算机视觉提出的非局部过滤操作完全相同。通常，计算机视觉自注意力模块的最终输出信号将被包装为:</p>
<p><span class="math inline">\(Z = YW_{Z} + X\)</span> (21)</p>
<p>其中Y是通过公式19产生的。如果<span
class="math inline">\(W_{Z}\)</span>初始化为零，这个自注意力模块可以插入到任何现有的模型中，而不会破坏它的初始行为。</p>
<h3 id="视觉任务应用">4.5.2 视觉任务应用：</h3>
<p>自注意力模块被认为是卷积神经网络体系结构的一个构件，它具有与大感受野有关的低标度特性。构建模块总是用在网络的顶部，以捕捉计算机视觉任务的远程交互。接下来，我们回顾了提出的基于自注意力的图像任务方法，如图像分类、语义分割和目标检测。</p>
<p><strong>图像分类。</strong>用于分类的可训练注意力包括两个主流:关于使用图像区域的硬注意力[3，87，134]和生成非刚性特征图的软注意力[125，60，43，102]。Ba等人[3]首先提出了用于图像分类任务的视觉注意项，并利用注意来选择输入图像中的相关区域和位置，这也可以降低所提出模型的计算复杂度，减小输入图像的大小。AG-CNN
[42]建议通过关注热点图从全局图像中裁剪出一个子区域，用于医学图像分类。SENet
[54]提出了软自注意力来重新加权卷积特征的信道响应，而不是使用硬注意力和重新校准特征图的裁剪。Jetley等人[60]使用由相应的估计器生成的注意力图来重新加权深层神经网络中的中间特征。Han等[43]利用属性感知注意力来增强CNN的表征能力。</p>
<p><strong>语义分割。</strong>PSANet [151]、OCNet [139]、DANet
[38]和CFNet
[147]是第一批将自注意力模块引入语义分割任务的作品，它们考虑并增强了上下文像素之间的关系和相似性[146、74、46、89、130]。DANet
[38]同时利用空间和通道维度上的自注意力模块。A2Net
[20]提出将像素分组为一组区域，然后通过将区域表示与生成的注意力权重聚合来增加像素表示。为了减轻自注意力模块中计算像素相似度带来的大量参数，提出了几个工作[140，59，58，75，66]来提高自注意力模块的语义分割效率。例如，CGNL
[140]应用径向基函数核函数的泰勒级数来近似像素相似性。CCNet
[59]通过两个连续的交叉注意来近似原始的自注意力方案。ISSA
[58]建议将密集亲和矩阵分解为两个稀疏亲和矩阵的乘积。还有其他相关的工作使用基于注意力的图形推理模块[76，21，75]来增强局部和全局表示。</p>
<p><strong>目标检测。</strong>Ramachandran等人[102]提出了一个基于注意力的层来建立一个完全注意力模型，它在COCO
[79]基准上优于卷积神经网络。GCNet
[13]发现，对于图像内的不同查询位置，由非局部操作建模的全局上下文几乎是相同的，并提出将简化公式和SENet
[54]统一为全局上下文建模的通用框架[73，52，34，93]。V o
.等人[124]设计了一个双向操作，从一个查询位置收集信息并将其分发到所有可能的位置。Hu等人[53]提出了一种基于自注意力的关系模块，通过一组对象的外观特征之间的相互作用来同时处理一组对象。Chenget
al .提出了RelationNet++
[23]，它带有一个基于注意力的解码器模块，将其他表示桥接成一个基于单一表示格式的典型对象检测器。</p>
<p><strong>其他视觉任务。</strong>Zhang等人[148]提出了分辨率方向的注意力模块，以学习用于精确姿态估计的增强的分辨率方向的特征图。Huang等人[57]提出了一种基于transformer的网络[56]，用于3D手-物体姿态估计。Chang等人[16]借助于基于注意力机制的特征融合块，提高了关键点检测模型的准确性并加速了其收敛。</p>
<h2 id="高效transformer">4.6 高效Transformer：</h2>
<p>尽管Transformer模型在各种任务中取得了成功，但仍然需要高内存和计算资源，这阻碍了在资源有限的设备(例如，移动电话)上的实现。在这一部分中，我们回顾了压缩和加速transformer模型以实现高效的研究，包括网络剪枝、低秩分解、知识提炼、网络量化、压缩架构设计。表4列出了一些压缩基于transformer的模型的代表性工作。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114528349.png" alt="image-20210615114528349"  /></p>
<h3 id="修剪和分解">4.6.1 修剪和分解:</h3>
<p>在基于transformer的预训练模型(如BERT)中，多个注意力操作被并行以独立地模拟不同标记之间的关系[123，29]，尽管所有的头部对于特定的任务都是必需的。Michel等人[85]凭经验观察到，在测试时，大部分注意力可以被转移，而不会显著影响性能。不同层所需的头数量各不相同，对于某些层来说，一个头就足够了。考虑到注意力头上的冗余，在[85]中定义了重要性分数来估计每个头对最终输出的影响，并且可以移除不重要的头以进行有效部署。Dalvi等人[96]从两个角度进一步分析了预训练transformer模型中的冗余，即一般冗余和特定任务冗余。遵循彩票假设等[36]，Prasanna等[96]分析了BERT中的彩票，并表明良好的子网络也存在于基于transformer的模型中。[96]中减少了FFN层和注意力头，以实现高压缩率。</p>
<p>除了transformer模型的宽度，深度，即层数也可以减少，以加快推理过程[32]。不同于transformer模型中不同的注意力头可以并行计算，不同的层必须顺序计算，因为下一层的输入取决于前一层的输出。Fan等人[32]提出了一种分层策略来规范模型的训练，然后在测试阶段将整个层一起移除。考虑到不同设备中的可用资源可能不同，Hou等人[51]提出自适应地减小预定义的transformer模型的宽度和深度，并且同时获得具有不同尺寸的多个模型。重要的注意力头和神经元通过一种重新布线机制在不同的子网中共享。</p>
<p>除了直接丢弃transformer模型中部分模块的剪枝方法之外，矩阵分解旨在基于低秩假设用多个小矩阵来逼近大矩阵。例如，Wang等人[131]在transformer模型中分解标准矩阵乘法，实现更有效的推理。</p>
<h3 id="知识蒸馏">4.6.2 知识蒸馏：</h3>
<p>知识蒸馏旨在通过从巨型教师网络转移知识来训练学生网络[48，12，2]。与教师网络相比，学生网络通常具有更薄、更浅的体系结构，更容易部署在资源有限的资源上。神经网络的输出和中间特征也可以用来将有效的信息从教师传递给学生。专注于transformer模型，Mukherjee等人[88]使用预先训练的BERT
[29]作为老师来指导小模型的训练，在大量未标记数据的帮助下，王等人[127]在预先训练的教师模型中训练学生网络以模仿自注意力层的输出。值与值之间的点积作为一种新的知识形式被引入来指导学生。在[127]中还引入了一名教师助理[86]，这缩小了大型预先训练的transformer模型和紧凑的学生网络之间的差距，使模拟更加容易。考虑到transformer模型中的各种类型的层(即，自注意力层、嵌入层、预测层)，Jiao等人[62]设计了不同的目标函数来将知识从教师转移到学生。例如，学生模型嵌入层的输出是通过均方误差损失来模拟教师模型的输出。一个可学习的线性变换也被用来将不同的特征映射到同一个空间。对于预测层的输出，采用KL散度来度量不同模型之间的差异。</p>
<h3 id="量化">4.6.3 量化：</h3>
<p>量化旨在减少代表网络权重或中间特征的位数[122，137]。通用神经网络的量化方法已经得到了很好的讨论，并获得了与原始网络相当的性能[91，37，6]。最近，如何对transformer模型进行特殊量化备受关注[8，33]。Shridhar等人[112]建议将输入嵌入到二进制高维向量中，然后使用二进制输入表示来训练二进制神经网络。Cheong等人[22]通过低位(例如4位)表示来表示transformer模型中的权重。Zhao等[152]对各种量化方法进行了实证研究，表明k-means量化具有巨大发展潜力。针对机器翻译任务，Prato等人[97]提出了一种完全量化的transformer，这是第一个8位质量模型，没有任何翻译质量损失，如论文所述。</p>
<h3 id="紧凑结构设计">4.6.4 紧凑结构设计：</h3>
<p>除了将预先定义的transformer模型压缩成小模型之外，一些研究试图直接设计紧凑的模型[132，61]。Jiang等[61]提出了一种新的基于跨度的动态卷积模型，将全连通层和卷积层结合起来，简化了自注意力的计算，如图9所示。来自不同标记的表示之间的局部相关性是通过卷积运算来计算的，这比标准transformer中的密集全连接层要有效得多。深度方向的卷积也被用来进一步降低计算成本。文献[1]中提出了有趣的汉堡层，利用矩阵分解来替代原有的自注意力层。矩阵分解可以比标准的自注意力操作更有效地计算，同时很好地反映不同标记之间的依赖性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114410293.png" alt="image-20210615114410293"  /></p>
<p>transformer模型中的自注意力操作计算给定序列(图像识别任务[31]中的块)中不同输入令牌的表示之间的点积，其复杂度为O(N)，其中N是序列的长度。最近，大量的方法集中在降低复杂度到O(N)，使transformer模型可扩展到长序列。例如，Katharopoulos等人[64]将自注意力近似为核特征映射的线性点积，并通过递归神经网络揭示标记之间的关系。Zaheer等人[143]将每个标记视为图中的一个顶点，两个标记之间的内积计算表示为一条边。受图论[113，25]的启发，各种稀疏图被组合在一起以近似transformer模型中的密集图，这也实现了O(N)复杂度。从理论的角度来看，Y
un等人[141]证明了一个具有O(N)复杂度的稀疏变换器足以反映令牌之间的任何一种关系，并且可以进行泛逼近，为进一步研究具有O(N)复杂度的transformer提供了理论上的保障。</p>
<h1 id="结论和未来展望">5. 结论和未来展望:</h1>
<p>与卷积神经网络相比，Transformer因其优越的性能和巨大的潜力成为计算机视觉领域的研究热点。为了发现和利用Transformer的能力，正如调查中所总结的，近年来已经提出了许多解决方案。这些方法在广泛的视觉任务上表现出优异的性能，包括基本图像分类、高级视觉、低级视觉和视频处理。然而，用于计算机视觉的Transformer的潜力还没有被充分开发，还有几个挑战有待解决。</p>
<p>虽然研究人员已经提出了许多基于Transformer的模型来处理计算机视觉任务，但这些工作只是初步的解决方案，还有很大的改进空间。例如，ViT
[31]中的Transformer架构遵循NLP
[123]的标准Transformer。专门针对CV的改进版还有待探索。此外，Transformer在上述任务之外的更多任务上的应用也是必需的。</p>
<p>此外，大多数现有的可视化Transformer模型都是为处理单一任务而设计的。许多NLP模型，如GPT-3
[10]，已经显示了Transformer在一个模型中处理多个任务的能力。CV领域的IPT
[17]还能够处理多个低水平视觉任务，如超分辨率、图像去噪和去雾。我们认为，更多的任务只能在一个模型中涉及。</p>
<p>最后，同样重要的是，为CV开发高效的Transformer模型也是一个公开的问题。Transformer模型通常很大，计算量也很大，例如，基本的ViT模型[31]需要18B的浮点运算来处理图像。相比之下，轻量级CNN模型GhostNet
[44，45]仅用约600M
FLOPs就能实现类似的性能。虽然已经提出了几种压缩Transformer的方法，但是它们的复杂性仍然很大。而这些原本是为NLP设计的方法，可能并不适合CV。因此，高效的Transformer模型是在资源有限的设备上部署可视化Transformer的代理。</p>
<h1 id="参考文献">6. 参考文献：</h1>
<p>[1] Anonymous. Is attention better than matrix decomposition?In
Submitted to International Conference on Learning Representations, 2021.
under review. 15</p>
<p>[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep?
Advances in neural information processing systems,27:2654–2662, 2014.
14</p>
<p>[3] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.Multiple object
recognition with visual attention. In International Conference on
Learning Representations, 2014.13</p>
<p>[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer
normalization. arXiv preprint arXiv:1607.06450,2016. 4</p>
<p>[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine
translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473, 2014. 1</p>
<p>[6] Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant:Quantized
neural networks via proximal operators. arXiv preprint arXiv:1810.00861,
2018. 14</p>
<p>[7] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained
language model for scientific text. arXiv preprint arXiv:1903.10676,
2019. 5</p>
<p>[8] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,Vivek Menon,
Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit
quantization of transformer neural machine language translation model.
arXiv preprint arXiv:1906.00532, 2019. 14</p>
<p>[9] Manjot Bilkhu, Siyang Wang, and Tushar Dobhal. Attention is all
you need for videos: Self-attention based video summarization using
universal transformers. arXiv preprint arXiv:1906.02792, 2019. 12</p>
<p>[10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, et al.Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020. 1, 5, 15</p>
<p>[11] Antoni Buades, Bartomeu Coll, and J-M Morel. A nonlocal
algorithm for image denoising. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 60–65, 2005. 12</p>
<p>[12] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil.
Model compression. In Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 535–541, 2006.
14</p>
<p>[13] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet:
Non-local networks meet squeeze-excitation networks and beyond. In
Proceedings of the IEEE International Conference on Computer Vision
Workshops, 2019.13</p>
<p>[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko.End-to-end object
detection with transformers. arXiv preprint arXiv:2005.12872, 2020.
2</p>
<p>[15] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko.End-to-end object
detection with transformers. In ECCV,2020. 7, 8, 9</p>
<p>[16] Yuan Chang, Zixuan Huang, and Qiwei Shen. The same size dilated
attention network for keypoint detection. In International Conference on
Artificial Neural Networks,pages 471–483, 2019. 13</p>
<p>[17] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng,
Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,and Wen Gao. Pre-trained
image processing transformer.arXiv preprint arXiv:2012.00364, 2020. 2,
10, 15</p>
<p>[18] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun,
David Luan, and Ilya Sutskever. Generative pretraining from pixels. In
International Conference on Machine Learning, pages 1691–1703. PMLR,
2020. 1, 2, 6,8</p>
<p>[19] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Memory enhanced
global-local aggregation for video object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10337–10346,2020. 11</p>
<p>[20] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and
Jiashi Feng. A2-nets: Double attention networks.Advances in neural
information processing systems, pages352–361, 2018. 13</p>
<p>[21] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng,
Jiashi Feng, and Yannis Kalantidis. Graphbased global reasoning
networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 433–442, 2019. 13</p>
<p>[22] Robin Cheong and Robel Daniel. transformers. zip: Compressing
transformers with pruning and quantization. Technical report, tech.
rep., Stanford University, Stanford, California, 2019. 14</p>
<p>[23] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++:Bridging
visual representations for object detection via transformer decoder.
Advances in Neural Information Processing Systems, 2020. 7, 13</p>
<p>[24] Yung-Sung Chuang, Chi-Liang Liu, and Hung-Yi Lee.Speechbert:
Cross-modal pre-trained language model for end-to-end spoken question
answering. arXiv preprint arXiv:1910.11559, 2019. 5</p>
<p>[25] Fan Chung and Linyuan Lu. The average distances in random graphs
with given expected degrees. Proceedings of the National Academy of
Sciences, 99(25):15879–15882,2002. 15</p>
<p>[26] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
Bengio. Empirical evaluation of gated recurrent neural networks on
sequence modeling. arXiv preprint arXiv:1412.3555, 2014. 4</p>
<p>[27] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D
Manning. Electra: Pre-training text encoders as discriminators rather
than generators. arXiv preprint arXiv:2003.10555, 2020. 5</p>
<p>[28] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.UP-DETR:
unsupervised pre-training for object detection with transformers. arXiv
preprint arXiv:2011.09094, 2020.2, 9</p>
<p>[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional transformers for
language understanding. In NAACL-HLT (1),2019. 1, 4, 5, 12, 13, 14</p>
<p>[30] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang,
Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model
pre-training for natural language understanding and generation. In
Advances in Neural Information Processing Systems, pages
13063–13075,2019. 5</p>
<p>[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias
Minderer, Georg Heigold,Sylvain Gelly, et al. An image is worth 16x16
words:Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 1, 2, 6, 8, 15</p>
<p>[32] Angela Fan, Edouard Grave, and Armand Joulin. Reducing
transformer depth on demand with structured dropout.arXiv preprint
arXiv:1909.11556, 2019. 14</p>
<p>[33] Chaofei Fan. Quantized transformer. Technical report,Technical
report, Stanford University, Stanford, California,2019. URL https . . .
. 14</p>
<p>[34] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai.Few-shot
object detection with attention-rpn and multirelation detector. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4013–4022, 2020. 13</p>
<p>[35] Robert B Fisher. Cvonline: The evolving, distributed,
nonproprietary, on-line compendium of computer vision. Re-trieved
January 28, 2006 from http://homepages. inf. ed. ac.uk/rbf/CVonline,
2008. 2</p>
<p>[36] Jonathan Frankle and Michael Carbin. The lottery ticket
hypothesis: Finding sparse, trainable neural networks. arXiv preprint
arXiv:1803.03635, 2018. 14</p>
<p>[37] Joshua Fromm, Meghan Cowan, Matthai Philipose, Luis Ceze, and
Shwetak Patel. Riptide: Fast end-to-end binarized neural networks.
Proceedings of Machine Learning and Systems, 2:379–389, 2020. 14</p>
<p>[38] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei
Fang, and Hanqing Lu. Dual attention network for scene segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3146–3154, 2019. 13</p>
<p>[39] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.
Multi-modal transformer for video retrieval. In European Conference on
Computer Vision (ECCV), pages 214–229, 2020. 11</p>
<p>[40] Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees GM
Snoek. Actor-transformers for group activity recognition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 839–848, 2020. 11</p>
<p>[41] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew
Zisserman. Video action transformer network. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 244–253,
2019. 11</p>
<p>[42] Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, Liang
Zheng, and Yi Yang. Diagnose like a radiologist:Attention guided
convolutional neural network for thorax disease classification. In arXiv
preprint arXiv:1801.09927,2018. 13</p>
<p>[43] Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian
Zhu.Attribute-aware attention model for fine-grained representation
learning. In Proceedings of the 26th ACM international conference on
Multimedia, pages 2040–2048, 2018.13</p>
<p>[44] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and
Chang Xu. Ghostnet: More features from cheap operations. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 1580–1589, 2020. 15</p>
<p>[45] Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and
Tong Zhang. Model rubik’s cube: Twisting resolution, depth and width for
tinynets. Advances in Neural Information Processing Systems, 33, 2020.
15</p>
<p>[46] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao.
Adaptive pyramid context network for semantic segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 7519–7528, 2019. 13</p>
<p>[47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep
residual learning for image recognition. pages 770–778, 2016. 1</p>
<p>[48] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the
knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
14</p>
<p>[49] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997. 1</p>
<p>[50] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997. 4</p>
<p>[51] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun
Liu. Dynabert: Dynamic bert with adaptive width and depth. Advances in
Neural Information Processing Systems, 33, 2020. 14</p>
<p>[52] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and TyngLuh Liu.
One-shot object detection with co-attention and co-excitation. In
Advances in Neural Information Processing Systems, pages 2725–2734,
2019. 13</p>
<p>[53] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei.
Relation networks for object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 3588–3597,
2018. 13</p>
<p>[54] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.
In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 7132–7141, 2018. 13</p>
<p>[55] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert:
Modeling clinical notes and predicting hospital readmission. arXiv
preprint arXiv:1904.05342, 2019. 5</p>
<p>[56] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan.
Handtransformer: Non-autoregressive structured modeling for 3d hand pose
estimation. In European Conference on Computer Vision, pages 17–33,
2020. 13</p>
<p>[57] Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and Junsong
Yuan. Hot-net: Non-autoregressive transformer for 3d hand-object pose
estimation. In Proceedings of the 28th ACM International Conference on
Multimedia, pages 3136–3145, 2020. 13</p>
<p>[58] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang,Xilin Chen, and
Jingdong Wang. Interlaced sparse self-attention for semantic
segmentation. arXiv preprint arXiv:1907.12273, 2019. 13</p>
<p>[59] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao
Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic
segmentation. In Proceedings of the IEEE International Conference on
Computer Vision, pages 603–612, 2019. 13</p>
<p>[60] Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip HS Torr.
Learn to pay attention. In International Conference on Learning
Representations, 2018. 13</p>
<p>[61] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi
Feng, and Shuicheng Yan. Convbert: Improving bert with span-based
dynamic convolution. Advances in Neural Information Processing Systems,
33, 2020. 2, 14, 15</p>
<p>[62] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural
language understanding. In Findings of the Association for Computational
Linguistics: EMNLP 2020, pages 4163–4174, Nov. 2020. 2, 13, 14</p>
<p>[63] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,Luke
Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by
representing and predicting spans. Transactions of the Association for
Computational Linguistics,8:64–77, 2020. 4</p>
<p>[64] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,and
Franc¸ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In International Conference on
Machine Learning, pages 5156–5165.PMLR, 2020. 15</p>
<p>[65] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet
classification with deep convolutional neural networks. In NeurIPS,
pages 1097–1105, 2012. 1</p>
<p>[66] Saumya Kumaar, Ye Lyu, Francesco Nex, and Michael Ying Yang.
Cabinet: Efficient context aggregation network for low-latency semantic
segmentation.arXiv preprint arXiv:2011.00993, 2020. 13</p>
<p>[67] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised
learning of language representations. arXiv preprint arXiv:1909.11942,
2019. 13</p>
<p>[68] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
Gradient-based learning applied to document recognition. Proceedings of
the IEEE, 86(11):2278–2324,1998. 1</p>
<p>[69] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
Kim, Chan Ho So, and Jaewoo Kang. Biobert:a pre-trained biomedical
language representation model for biomedical text mining.
Bioinformatics, 36(4):1234–1240,2020. 5</p>
<p>[70] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
Bart: Denoising sequence-to-sequence pre-training for natural language
generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461, 2019. 5</p>
<p>[71] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. Entangled
transformer for image captioning. In 2019 IEEE/CVF International
Conference on Computer Vision (ICCV), pages 8928–8937, 2019. 12</p>
<p>[72] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,and
Kai-Wei Chang. Visualbert: A simple and performant baseline for vision
and language. arXiv preprint arXiv:1908.03557, 2019. 5</p>
<p>[73] Wei Li, Kai Liu, Lizhe Zhang, and Fei Cheng. Object detection
based on an adaptive attention mechanism. Scientific Reports, pages
1–13, 2020. 13</p>
<p>[74] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang,Zhouchen Lin, and
Hong Liu. Expectation-maximization attention networks for semantic
segmentation. In Proceedings of the IEEE International Conference on
Computer Vision, pages 9167–9176, 2019. 13</p>
<p>[75] Yin Li and Abhinav Gupta. Beyond grids: Learning graph
representations for visual recognition. Advances in Neural Information
Processing Systems, pages 9225–9235, 2018.13</p>
<p>[76] Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and Eric P
Xing. Symbolic graph reasoning meets convolutions.Advances in Neural
Information Processing Systems, pages 1853–1863, 2018. 13</p>
<p>[77] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,Bharath
Hariharan, and Serge Belongie. Feature pyramid networks for object
detection. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2117–2125, 2017. 7</p>
<p>[78] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,and Piotr
Dollar. Focal loss for dense object detection. In ICCV, 2017. 13</p>
<p>[79] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft
coco: Common objects in context. In European conference on computer
vision, pages 740–755, 2014. 13</p>
<p>[80] Hao Liu, Jiwen Lu, Jianjiang Feng, and Jie Zhou. Twostream
transformer networks for video-based face alignment. IEEE transactions
on pattern analysis and machine intelligence, 40(11):2546–2554, 2017.
11</p>
<p>[81] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. Endto-end
lane shape prediction with transformers. In WACV,2021. 7, 9</p>
<p>[82] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692, 2019. 4, 5</p>
<p>[83] Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Chunguo
Li, and Luxi Yang. Convtransformer: A convolutional transformer network
for video frame synthesis. arXiv preprint arXiv:2011.10185, 2020. 11</p>
<p>[84] Suhas Lohit, Qiao Wang, and Pavan Turaga. Temporal transformer
networks: Joint learning of invariant and discriminative time warping.
In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),pages 12426–12435, 2019. 11</p>
<p>[85] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads
really better than one? In Advances in Neural Information Processing
Systems, pages 14014–14024, 2019. 2,14</p>
<p>[86] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine,
Akihiro Matsukawa, and Hassan Ghasemzadeh.Improved knowledge
distillation via teacher assistant. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pages 5191–5198, 2020.
14</p>
<p>[87] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent
models of visual attention. Advances in neural information processing
systems, pages 2204–2212, 2014. 13</p>
<p>[88] Subhabrata Mukherjee and Ahmed Hassan Awadallah.Xtremedistil:
Multi-stage distillation for massive multilingual models. In Proceedings
of the 58th Annual Meeting of the Association for Computational
Linguistics, pages 2221–2234, 2020. 14</p>
<p>[89] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee,Mattias
Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y
Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look
for the pancreas. arXiv preprint arXiv:1804.03999, 2018. 13</p>
<p>[90] Ankur Parikh, Oscar Tackstr om, Dipanjan Das, and Jakob
Uszkoreit. A decomposable attention model for natural language
inference. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, pages 2249–2255, 2016. 1</p>
<p>[91] Eunhyeok Park and Sungjoo Yoo. Profit: A novel training method
for sub-4-bit mobilenet models. In European Conference on Computer
Vision, pages 430–446. Springer,2020. 14</p>
<p>[92] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser,
Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. ICML,
2018. 2, 9, 10</p>
<p>[93] Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier,
and Maguelonne Heritier. Spotnet: Self-attention multi-task network for
object detection. In 2020 17th Conference on Computer and Robot Vision
(CRV), pages 230–237, 2020. 13</p>
<p>[94] Matthew E Peters, Mark Neumann, Robert L Logan IV,Roy Schwartz,
Vidur Joshi, Sameer Singh, and Noah A Smith. Knowledge enhanced
contextual word representations. arXiv preprint arXiv:1909.04164, 2019.
5</p>
<p>[95] Tim Prangemeier, Christoph Reich, and Heinz
Koeppl.Attention-based transformers for instance segmentation of cells
in microstructures. arXiv preprint arXiv:2011.09763,2020. 9</p>
<p>[96] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When bert plays
the lottery, all tickets are winning. arXiv preprint arXiv:2005.00561,
2020. 14</p>
<p>[97] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully
quantized transformer for machine translation. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing:
Findings, pages 1–14, 2020. 2, 14</p>
<p>[98] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and
Xuanjing Huang. Pre-trained models for natural language processing: A
survey. arXiv preprint arXiv:2003.08271, 2020. 5, 13</p>
<p>[99] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative pre-training,
2018. 5</p>
<p>[100] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei,
and Ilya Sutskever. Language models are unsupervised multitask learners.
OpenAI blog, 1(8):9,2019. 5, 6</p>
<p>[101] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
the limits of transfer learning with a unified text-to-text transformer.
arXiv preprint arXiv:1910.10683, 2019. 5</p>
<p>[102] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello,
Anselm Levskaya, and Jonathon Shlens. Standalone self-attention in
vision models. arXiv preprint arXiv:1906.05909, 2019. 13</p>
<p>[103] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster
r-cnn: Towards real-time object detection with region proposal networks.
In Advances in neural information processing systems, pages 91–99, 2015.
1</p>
<p>[104] Frank Rosenblatt. The perceptron, a perceiving and recognizing
automaton Project Para. Cornell Aeronautical Laboratory, 1957. 1</p>
<p>[105] FRANK ROSENBLATT. Principles of neurodynamics.perceptrons and
the theory of brain mechanisms. Technical report, CORNELL AERONAUTICAL
LAB INC BUFFALO NY, 1961. 1</p>
<p>[106] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
Learning internal representations by error propagation. Technical
report, California Univ San Diego La Jolla Inst for Cognitive Science,
1985. 1</p>
<p>[107] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter. arXiv preprint arXiv:1910.01108, 2019. 13</p>
<p>[108] Kara Marie Schatz, Erik Quintanilla, Shruti Vyas, and Yogesh
Singh Rawat. A recurrent transformer network for novel view action
synthesis. In ECCV (27), pages 410–426, 2020. 11</p>
<p>[109] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Video multitask
transformer network. In Proceedings of the IEEE International Conference
on Computer Vision Workshops,pages 0–0, 2019. 11</p>
<p>[110] Jie Shao, Xin Wen, Bingchen Zhao, and Xiangyang Xue. Temporal
context aggregation for video retrieval with contrastive learning.
11</p>
<p>[111] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir
Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based
ultra low precision quantization of bert. In AAAI, pages 8815–8821,
2020. 13</p>
<p>[112] Kumar Shridhar, Harshil Jain, Akshat Agarwal, and Denis Kleyko.
End to end binarized neural networks for text classification. In
Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural
Language Processing, pages 29–34, 2020. 14</p>
<p>[113] Daniel A Spielman and Shang-Hua Teng. Spectral sparsification
of graphs. SIAM Journal on Computing, 40(4):981–1025, 2011. 15</p>
<p>[114] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,Furu Wei, and
Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic
representations. arXiv preprint arXiv:1908.08530, 2019. 5</p>
<p>[115] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,and
Cordelia Schmid. Videobert: A joint model for video and language
representation learning. In Proceedings of the IEEE International
Conference on Computer Vision, pages 7464–7473, 2019. 5</p>
<p>[116] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient
knowledge distillation for bert model compression. arXiv preprint
arXiv:1908.09355, 2019. 13</p>
<p>[117] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.
Rethinking transformer-based set prediction for object detection. arXiv
preprint arXiv:2011.10881, 2020. 2, 9</p>
<p>[118] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang,
and Denny Zhou. Mobilebert: a compact taskagnostic bert for
resource-limited devices. arXiv preprint arXiv:2004.02984, 2020. 13</p>
<p>[119] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine
Badue, Alberto F De Souza, and Thiago OliveiraSantos. Polylanenet: Lane
estimation via deep polynomial regression. arXiv preprint
arXiv:2004.10924, 2020. 9</p>
<p>[120] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
Well-read students learn better: The impact of student initialization on
knowledge distillation. arXiv preprint arXiv:1908.08962, 2019. 13</p>
<p>[121] Shimon Ullman et al. High-level vision: Object recognition and
visual cognition, volume 2. MIT press Cambridge, MA, 1996. 2</p>
<p>[122] Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving the
speed of neural networks on cpus. 2011. 14</p>
<p>[123] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser,and Illia Polosukhin.
Attention is all you need. Advances in neural information processing
systems, 30:5998–6008, 2017. 1, 2, 3, 4, 9, 12, 14, 15</p>
<p>[124] Xuan-Thuy Vo, Lihua Wen, Tien-Dat Tran, and Kang-Hyun Jo.
Bidirectional non-local networks for object detection.In International
Conference on Computational Collective Intelligence, pages 491–501,
2020. 13</p>
<p>[125] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li,
Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.Residual attention
network for image classification. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 3156–3164, 2017.
13</p>
<p>[126] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille,and
Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with
mask transformers. arXiv preprint arXiv:2012.00759, 2020. 2, 7, 9</p>
<p>[127] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming
Zhou. Minilm: Deep self-attention distillation for task-agnostic
compression of pre-trained transformers.arXiv preprint arXiv:2002.10957,
2020. 14</p>
<p>[128] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
Non-local neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 7794–7803, 2018. 12</p>
<p>[129] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan
Cheng, Hao Shen, and Huaxia Xia. End-toend video instance segmentation
with transformers. arXiv preprint arXiv:2011.14503, 2020. 2, 7, 9</p>
<p>[130] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen.
Self-supervised equivariant attention mechanism for weakly supervised
semantic segmentation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 12275–12284, 2020. 13</p>
<p>[131] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
of large language models. arXiv preprint arXiv:1910.04732, 2019. 14</p>
<p>[132] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite
transformer with long-short range attention.arXiv preprint
arXiv:2004.11886, 2020. 15</p>
<p>[133] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
Bert-of-theseus: Compressing bert by progressive module replacing. arXiv
preprint arXiv:2002.02925, 2020. 13</p>
<p>[134] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show,
attend and tell: Neural image caption generation with visual attention.
In International conference on machine learning, pages 2048–2057, 2015.
13</p>
<p>[135] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining
Guo. Learning texture transformer network for image super-resolution. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages 5791–5800, 2020. 2, 10</p>
<p>[136] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,Russ R
Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive
pretraining for language understanding. In Advances in neural
information processing systems, pages 5753–5763, 2019. 5</p>
<p>[137] Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu,
Dacheng Tao, and Chang Xu. Searching for lowbit weights in quantized
neural networks. arXiv preprint arXiv:2009.08695, 2020. 14</p>
<p>[138] Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and Ruigang
Yang. Lidar-based online 3d video object detection with graph-based
message passing and spatiotemporal transformer attention. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pages 11495–11504, 2020. 11</p>
<p>[139] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for
scene parsing. arXiv preprint arXiv:1809.00916,2018. 13</p>
<p>[140] Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, and
Fuxin Xu. Compact generalized non-local network. In Advances in Neural
Information Processing Systems, pages 6510–6519, 2018. 13</p>
<p>[141] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli,Ankit Singh
Rawat, Sashank J Reddi, and Sanjiv Kumar. o(n) connections are
expressive enough: Universal approximability of sparse transformers.
arXiv preprint arXiv:2006.04862, 2020. 15</p>
<p>[142] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.
13</p>
<p>[143] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie,
Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv
preprint arXiv:2007.14062, 2020. 15</p>
<p>[144] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint
spatial-temporal transformations for video inpainting. In European
Conference on Computer Vision, pages 528–543. Springer, 2020. 2, 12</p>
<p>[145] Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang,Xiansheng
Hua, and Qianru Sun. Feature pyramid transformer. In ECCV, 2020. 7</p>
<p>[146] Fan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu,
Feifei Ma, Junyu Han, and Errui Ding. Acfnet: Attentional class feature
network for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 6798–6807, 2019.
13</p>
<p>[147] Hang Zhang, Han Zhang, Chenguang Wang, and Junyuan Xie.
Co-occurrent features in semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages
548–557, 2019. 13</p>
<p>[149] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,Maosong Sun, and
Qun Liu. Ernie: Enhanced language representation with informative
entities. arXiv preprint arXiv:1905.07129, 2019. 5</p>
<p>[150] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen
Koltun. Point transformer. arXiv preprint arXiv:2012.09164, 2020. 9</p>
<p>[151] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change
Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention
network for scene parsing. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 267–283, 2018. 13</p>
<p>[152] Zihan Zhao, Yuncong Liu, Lu Chen, Qi Liu, Rao Ma, and Kai Yu.
An investigation on different underlying quantization schemes for
pre-trained language models. In CCF International Conference on Natural
Language Processing and Chinese Computing, pages 359–371. Springer,
2020.14</p>
<p>[153] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao
Dong. End-to-end object detection with adaptive clustering transformer.
arXiv preprint arXiv:2011.09315, 2020. 2, 8</p>
<p>[154] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and
Caiming Xiong. End-to-end dense video captioning with masked
transformer. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8739–8748, 2018. 2, 12</p>
<p>[155] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and
Jifeng Dai. Deformable detr: Deformable transformers for end-to-end
object detection. arXiv preprint arXiv:2010.04159, 2020. 2, 7, 8</p>
<p>[156] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel
Urtasun, Antonio Torralba, and Sanja Fidler.Aligning books and movies:
Towards story-like visual explanations by watching movies and reading
books. In Proceedings of the IEEE international conference on computer
vision, pages 19–27, 2015. 4</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://iyorei.github.io/post/bee8.html" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Vision-Transformer/" rel="tag">Vision Transformer</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/post/ea5a.html" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            ViT模型训练实验
          
        </div>
      </a>
    
    
      <a href="/post/d6d0.html" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">conda使用</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "BqCVkA9wlDgUTtJHtMbq6rA6-gzGzoHsz",
    app_key: "Yw5QiU6K8SqJsogVotd0C3qv",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2023
        <i class="ri-heart-fill heart_icon"></i> Yore
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="http://www.beian.miit.gov.cn/" target="_black" rel="nofollow">陕ICP备2021003353</a>
        </li>
        
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/baobao.png" alt="Yore&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/blog/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/blog/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
  </div>
</body>

</html>