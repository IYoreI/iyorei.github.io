<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MAC使用指南</title>
      <link href="post/caca.html"/>
      <url>post/caca.html</url>
      
        <content type="html"><![CDATA[<h1 id="mac-初始化配置">MAC 初始化配置</h1><p>初次使用MAC，对于一些使用上的设置还有安装的相关APP进行了记录。</p><h2 id="基本设置">1、基本设置:</h2><p>触控板</p><h2 id="软件工具安装">2、软件工具安装:</h2><h3 id="homebrew">Homebrew:</h3><p>终端执行如下命令,从国内中科大镜像源下载安装:(安装过程中会提示安装git)</p><pre><code>/bin/zsh -c &quot;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;</code></pre><p>安装完成后,终端执行brew -v出现如下信息:</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/homebrew.png" alt="homebrew"  /></p><p>意思是homebrew的根目录没有信任列出的这两项</p><p>解决方案:</p><pre><code># 终端执行如下命令git config --global --add safe.directory /opt/homebrew/Library/Taps/homebrew/homebrew-coregit config --global --add safe.directory /opt/homebrew/Library/Taps/homebrew/homebrew-cask</code></pre><h3 id="xnip">Xnip:</h3><p>一款截屏工具</p><h3 id="iina">IINA：</h3><p>视频播放软件</p><h3 id="clipy">Clipy:</h3><p>剪切板</p><h3 id="typora">Typora：</h3><p>Markdown编辑器，配合PicGo + Github搭建在线图床</p><hr /><p>typora激活小技巧，这么做实在是不应该，下面说教程：</p><p>在应用程序中从终端中打开typora所在位置，</p><p>路径是：/Applications/Typora.app/Contents/Resources/TypeMark/page-dist/static/js</p><p>可以通过如下命令查找e.<strong>hasActivated</strong>="true"==e.<strong>hasActivated</strong>所在位置</p><pre><code># 在typora安装目录下进行关键词内容搜索，找到LicenseIndex.180dd4c7.95238c74.chunk.js 文件所在位置grep -r &quot;e.hasActivated&quot; ./</code></pre><p>将hasActivated="true"==e.hasActivated更改为hasActivated="true"=="true"，重启typora即可。</p><h3 id="picgo">PicGo：</h3><p>图床，下载地址： https://github.com/Molunerfinn/PicGo/releases</p><h3 id="终端工具-iterm2-oh-my-zsh">终端工具： iTerm2 + Oh My Zsh</h3><p>iTerm2安装：https://iterm2.com/downloads.html</p><p>zsh安装：</p><pre class="shell"><code># 查看当前所使用的SHELLecho $SHELL# 查看当前环境下所有支持的 SHELL，检查列表中是否包含zshcat /etc/shells# zsh查看版本号zsh --version# MAC默认自带zsh，可以使用如下命令更新brew install zsh</code></pre><p>将zsh设置为默认shell：</p><p>​可以在系统设置》用户与群组》ctrl+用户头像打开高级选项中确认默认登录shell方式</p><p>安装oh-my-zsh:</p><div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/robbyrussell/oh-my-zsh.git</span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> oh-my-zsh/tools</span><span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sh</span> install.sh</span></code></pre></div><p>oh-my-zsh插件配置：</p><p>主题配置：</p><pre class="shell"><code>vim ~/.zshrc# 先设置了一个主题 afowlerZSH_THEME=&quot;afowler&quot;source ~/.zshrc</code></pre><p>zsh命令自动补全插件（zsh-autosuggestions）：</p><pre class="shell"><code>git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestionsvim ~/.zshrc# 在原有配置基础上添加zsh-autosuggestions 多个插件之间用空格隔开plugins=(git zsh-autosuggestions)source ~/.zshrc</code></pre><p>zsh命令高亮插件（zsh-syntax-highlighting）：</p><pre class="shell"><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlightingvim ~/.zshrc# 在原有配置基础上添加zsh-syntax-highlighting 多个插件之间用空格隔开plugins=(git zsh-autosuggestions zsh-syntax-highlighting)source ~/.zshrc</code></pre><p>在切换zsh命令行解释器后，原先在~/.bash_profile中配置的环境变量失效了，解决方法：</p><pre class="shell"><code># 在~/.zshrc文件最后，增加一行source ~/.bash_profile</code></pre><h3 id="neatreader">NeatReader:</h3><p>电子书阅读器</p><h3 id="右键助手">右键助手：</h3><p>右键新建文档</p><h2 id="开发环境搭建">3、开发环境搭建:</h2><h3 id="jdk安装与卸载">3.1 JDK安装与卸载:</h3><h4 id="卸载jdk">卸载JDK:</h4><p>先在终端执行</p><pre><code>java -version</code></pre><p>如果有输出相关版本信息说,说明MAC自带了1.8.0_202版本的JDK,</p><pre class="shell"><code># 1、执行如下命令查看jdk安装路径/usr/libexec/java_home -V# 2、看到安装目录为: /Library/Java/JavaVirtualMachines# 3、执行rm命令删除sudo rm -rf /Library/Java/JavaVirtualMachines# 1、删除自己安装的JDK/usr/libexec/java_home -V#  看到jdk相关文件的路径如下:#    1.8.202.08 (x86_64) &quot;Oracle Corporation&quot; - &quot;Java&quot; /Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home#/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home# 2、执行rm命令删除sudo rm -rf /Library/Internet\ Plug-Ins/JavaAppletPlugin.plugin/Contents/Home</code></pre><p>检查用户目录下的.bash_profile文件,删除java环境变量相关的配置</p><h4 id="安装jdk">安装JDK:</h4><p>使用homebrew安装jdk</p><pre class="shell"><code># 执行如下命令查看jdk各版本brew search openjdk# 通过如下命令安装指定版本的jdkbrew install openjdk@17</code></pre><p>留意安装完成之后的输出信息,会提示你进行如下操作</p><p>设置软连接:</p><pre class="shell"><code> sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</code></pre><p>设置环境变量:</p><pre class="shell"><code>vim ~/.bash_profile# 输入安装路径export JAVA_HOME=/opt/homebrew/opt/openjdk@17export PATH=$JAVA_HOME/bin:$PATH:.# 保存退出# 动态生效配置文件source ~/.bash_profile</code></pre><p>完成上述步骤之后,在终端执行java-version可以看到jdk相应的版本信息,说明配置完成</p><h3 id="maven安装">3.2 Maven安装：</h3><p>官网下载maven二进制压缩包，解压到本地目录</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/maven-download.png"alt="image-20230607163610982" /><figcaption aria-hidden="true">image-20230607163610982</figcaption></figure><p>设置环境变量:</p><pre class="shell"><code>vim ~/.bash_profile# 输入安装路径export MAVEN_HOME=/Users/yore/env/apache-maven-3.9.2export PATH=$MAVEN_HOME/bin:$PATH:.# 保存退出# 动态生效配置文件source ~/.bash_profile</code></pre><p>验证安装是否成功：</p><pre><code>mvn -v</code></pre><p>配置文件添加镜像源和本地仓库地址，在maven的setting.xml文件中</p><p>设置阿里云镜像源：（定位mirror关键词）</p><pre><code>    &lt;mirror&gt;      &lt;id&gt;alimaven&lt;/id&gt;      &lt;mirrorOf&gt;censtral&lt;/mirrorOf&gt;      &lt;name&gt;alien maven&lt;/name&gt;      &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;    &lt;/mirror&gt;</code></pre><p>设置本地仓库地址：（定位localRepository关键词）</p><pre><code>&lt;localRepository&gt;/Users/yore/repository&lt;/localRepository&gt;</code></pre><h3 id="goland开发环境">3.4 Goland开发环境：</h3><p>通过homebrew安装go</p><pre class="shell"><code># 查看库中go版本brew search go# 安装指定版本gobrew install go@1.18# 环境变量配置到.bash_profileexport GO_HOME=/opt/homebrew/opt/go@1.18export PATH=$GO_HOME/bin:$PATH:.# 配置生效source .bash_profile# 查看版本go version</code></pre><p>以上方法存在在GoLand开发工具中无法自动识别到SDK的情况，如果安装时不指定go版本，默认安装最新版的可以自动识别。</p><p>第二种安装方法，在官网（https://go.dev/dl/）手动下载安装包，下载macoSARM64版，默认安装目录/usr/local/go，配置环境变量</p><pre class="shell"><code># 配置go的几个环境变量# go 工作目录export GOPATH=$HOME/go# go 安装目录export GOROOT=/usr/local/goexport GOBIN=$GOROOT/binexport PATH=$GOBIN:$PATH:.# 配置生效source ～/.bash_profile# 查看相关信息go versiongo env</code></pre><h2 id="其他设置">4、其他设置：</h2><h3 id="关闭microsoft自动更新">关闭microsoft自动更新：</h3><pre class="shell"><code>cd /Library/Application\ Support/Microsoft/MAU2.0# 将此应用程序权限设置为000sudo chmod 000 Microsoft\ AutoUpdate.app</code></pre><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VideoMAE</title>
      <link href="post/9c44.html"/>
      <url>post/9c44.html</url>
      
        <content type="html"><![CDATA[<h1id="videomae-masked-autoencoders-are-data-efficient-learners-for-self-supervised-video-pre-training">VideoMAE:Masked Autoencoders are Data-Efficient Learners for Self-SupervisedVideo Pre-Training</h1><p>收录：NeurIPS-2022</p><p>代码：https://github.com/MCG-NJU/VideoMAE</p><p>参考资料：</p><p>​ https://arxiv.org/abs/2203.12602</p><p>​ https://blog.csdn.net/moxibingdao/article/details/125025960</p><p>​ https://zhuanlan.zhihu.com/p/575573336</p><p>​ https://zhuanlan.zhihu.com/p/446761025</p><h1 id="一背景介绍">一、背景介绍：</h1><p>​Transformer在NLP领域大放异彩，ViT的出现将Transformer带入了视觉领域，又改进了一系列视觉相关的任务，例如：图像分类、目标检测、语义分割、行为识别等等。但是呢，我们一般都需要在超大规模数据集上做预训练，然后在相对较小的数据集上实现最佳性能。在本文中，作者证明了视频掩码自动编码器（VideoMAE）是用于自监督视频预训练（self-supervisedvideo pre-training [SSVP]）的数据高效学习器。</p><p>关于SSVP，作者提出的三个重要发现：</p><p>1、视频数据存在时间冗余性，所以在极高比例的遮挡下(90%-95%)，VideoMAE依然可以产生良好的性能。</p><p>2、VIdeoMAE无需使用额外数据，在比较小的数据集上也能取得非常好的效果。</p><p>3、VideoMAE表明，对于SSVP，数据质量比数据量更重要。</p><p>下图是论文方法和部分方法在UCF101和HMDB51两个数据集上的top1精度的比较</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205162027840.png" alt="image-20221205162027840"  /></p><p>ViT通常依赖大规模监督数据集，比如ImageNet，对于VideoTransformer来说，通常依赖于ViT，也就是依赖大规模图像数据的预训练模型。<strong>如何不依赖任何预训练模型或者额外的数据，能够有效的训练一个VideoTransformer模型是一个挑战。</strong></p><span id="more"></span><h1 id="二理论方法">二、理论方法：</h1><p>​在ViT和MAE之后，作者提出了一种针对视频数据的自监督视频预训练方法（SSVP），称为VideoMasked Autoencoder（VideoMAE）。</p><h2 id="mae">2.1 MAE：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205164856098.png" alt="image-20221205164856098"  /></p><p>MAE是一个非对称的编码器-解码器结构，</p><p><strong>编码器：</strong>ViT网络，将输入图像切分成不重叠的patches，其中部分patches会被遮挡，遮挡率75%，将没有被遮挡的patches作为编码器的输入，加上位置信息，送入transformer网络</p><p><strong>解码器</strong>：ViT网络，将编码器输出的特征和最初输入图像中被遮挡的patch作为解码器的输入，加上位置编码信息，送入transformer网络</p><p>损失函数使用的是MSE，注意损失函数只对masked patches计算</p><h2 id="视频数据的特点分析">2.2 视频数据的特点分析：</h2><p><strong>Temporal redundancy</strong>： 时间冗余性</p><p>​以30FPS来说，每秒钟的视频有30帧图像，这其中连续帧的内容其实是高度冗余的，这个特性有两个问题：</p><p>1、如果我们按照视频原始帧率做预训练，效率很低，同时网络会更多的关注静态信息或慢速运动，所以我们一般训练的时候都是间隔t帧取一帧，t为自定义参数。</p><p>2、时间冗余其实稀释了运动表示，这使得在正常的遮挡比率下（50%-75%）,重建图像没有太大困难，网络无法有效捕获运动表示。</p><p><strong>Temporal correlation</strong>： 时间相关性</p><p>​视频可以认为是静态帧的时间扩展，因此相邻帧之间存在固有的对应关系，这种相关性会帮助网络重建那些被遮挡的信息。在这种情况下，它可能会引导VideoMAE学习低级的时间对应关系，而不是高级信息。所以需要有针对性的设计掩码策略。</p><h2 id="videomae">2.3 VideoMAE：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205172420444.png" alt="image-20221205172420444"  /></p><p><strong>采样策略：</strong></p><p>​ 针对时间冗余性，采取跨步采样策略，随机采样一个视频片段，视频帧采样间隔 t,</p><p><strong>掩码策略：</strong></p><p>​管道式掩码策略<img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205191932070.png" alt="image-20221205191932070"  /></p><p><strong>Cube embedding：</strong> 时空立方体embedding</p><p>​在输入到编码器中之前，对于采样得到的视频片段，采用时空联合的形式进行像素块嵌入。将大小为T×H×W 视频片段中大小为 2×16×16的视觉像素视为一个视觉像素块。因此，采样得到的视频片段经过时空块嵌入(cubeembedding)层后可以得到 T/2 × H/16 × W/16个视觉像素块。在这个过程中，同时会将视觉像素块的通道维度映射为 D 。</p><p>​这种设计可以减少输入数据的时空维度大小，一定程度上也有助于缓解视频数据的时空冗余性。</p><p><strong>joint space-time attention：</strong> 时空联合注意力机制</p><p>​VideoMAE采用了极高的掩码率，只保留了极少的token作为编码器的输入。为了更好地提取这部分未被遮蔽的token的时空特征，VideoMAE选择使用原始的ViT作为Backbone，同时在注意力层中采用时空联合自注意力（即不改变原始ViT的模型结构）。因此所有未被遮蔽的token都可以在自注意层中相互交互。时空联合自注意力机制的O(n2)级别的计算复杂度是网络的计算瓶颈，而前文中针对VideoMAE使用了极高掩码比率策略，仅将未被遮蔽的token（例如10%）输入到编码器中。这种设计一定程度上可以有效地缓解计算复杂度的问题。</p><h2 id="网络实现细节">2.4 网络实现细节：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205192615771.png" alt="image-20221205192615771"  /></p><h1 id="三实验结果">三、实验结果：</h1><h2 id="ava数据集">3.1 AVA数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205195332623.png" alt="image-20221205195332623"  /></p><h2 id="ssv2数据集">3.2 SSV2数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205195354889.png" alt="image-20221205195354889"  /></p><h2 id="k400数据集">3.3 K400数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205195410052.png" alt="image-20221205195410052"  /></p><h2 id="消融实验">3.4 消融实验：</h2><p>作者在SSV2和K400两个数据集上做了如下消融实验：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20221205192924879.png" alt="image-20221205192924879"  /></p><p>（a）对VideoMAE种的解码器组件，做了不同深度的实验</p><p>（b）不同的掩码策略，如图b普通随机掩蔽和帧掩蔽实现的性能低于tube掩蔽。以SSV2为例，当掩蔽率增加到90%时，性能从68.0%提高到69.6%。</p><p>（c）如何设计重建的目标？以SSV2为例，如果只选取视频片段的中心帧作为目标，效果为63.0%,连续采样T帧的效果不如间隔t帧的效果，如果用T帧视频来重建2T帧，效果略差。</p><p>（d）从头开始训练的VideoTransformer、在ImageNet-21k上预训练、在ImageNet-21k+k400预训练以及采用VideoMAE的四种方式在两个数据集上的精度对比</p><p>（e）在三个数据集上预训练 然后在SSV2和K400实验的结果</p><p>（f）不同损失函数的对比</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> action recognition </tag>
            
            <tag> computer vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SlowFast Networks for Video Recognition</title>
      <link href="post/88eb.html"/>
      <url>post/88eb.html</url>
      
        <content type="html"><![CDATA[<h1id="slowfast-networks-for-video-recognition-iccv-2019"><strong>SlowFastNetworks for Video Recognition</strong> [ICCV-2019]</h1><h1 id="一背景介绍">一、背景介绍：</h1><p>​ 首先作者提出的想法：我们一般处理2D图像的时候，很自然地会对称的处理两个维度x,y，但如果是加了时间维度的视频呢？运动是方向的时空对应物，但所有时空方向的可能性都不相等<strong>慢动作比快动作更有可能发生</strong>，这个有生物学依据：作者提到论文中的方法是受灵长类生物细胞生物学研究的启发，存在两类细胞，P细胞和M细胞，M细胞在高时间频率下工作，对快速的时间变化做出反应，而P细胞在低时间频率下提供精细的空间细节和颜色。</p><p>举例：例如，在挥舞动作的过程中，挥手不会改变他们的“手”身份，一个人总是属于“人”类别，即使他/她可以从走路过渡到跑步。因此，分类语义的识别(以及它们的颜色、纹理、光照等)可以相对缓慢地刷新。另一方面，被执行的动作可以比他们的主体身份进化得更快，比如拍手、挥手、摇晃、行走或跳跃。可以期望使用快速刷新帧(高时间分辨率)来有效地模拟潜在的快速变化的运动。</p><p>如果时间和空间方向上的可能性不同，那么我们就没有理由对称的处理空间和时间维度，基于这种直觉，作者提出了一种用于视频识别的双路径SlowFast模型(图1)。其中一<strong>种路径旨在捕捉图像或少量稀疏帧提供的语义信息</strong>，其运行速度较低，刷新速度较慢。相比之下，<strong>另一种路径负责捕捉快速变化的运动</strong>，刷新速度快，时间分辨率高。</p><p>这种方法和双流算法的区别在哪里？</p><p>1、 双流算法的两个流采用相同的主干结构</p><p>2、双流算法需要计算光流信息、论文中提出的网络模型是从原始数据中端到端学习</p><span id="more"></span><h1 id="二理论方法">二、理论方法：</h1><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202201111501686.png" alt="image-20220111150137514" style="zoom:80%;" /></p><p>整体网络框架包含了一个低帧率的Slow路径和一个高帧率的Fast路径</p><p><span class="math inline">\(\tau\)</span> : slow路径采样频率</p><p><span class="math inline">\(T\)</span> : slow路径采样的总帧数</p><p><span class="math inline">\(\alpha\)</span> ：fast路径采样频率为<spanclass="math inline">\(\tau/\alpha\)</span></p><p><strong>Slow路径：</strong></p><p>​ 输入的是低帧率的采样数据，使用一个较大的时序跨度，由参数<spanclass="math inline">\(\tau\)</span>设置</p><p><strong>Fast路径：</strong></p><p>采样速率为慢路径的<span class="math inline">\(\alpha =8\)</span>倍，每隔2帧采样一次</p><ul><li><p>追求高分辨率特征，所以不使用时间下采样（时间池化和时间跨度卷积），只在分类前的一个全局池化，所以特征张量在时间维度上总有<spanclass="math inline">\(\alpha t\)</span>帧</p></li><li><p>低信道容量，使它很轻,轻量级的设计由</p></li><li><p>类似于slow路径的卷积网络，但是通道数是它的<spanclass="math inline">\(\beta = 1/8\)</span></p></li></ul><p>Fast路径没有对空间维度进行特殊处理，因为通道较少，所以它的空间建模能力较弱，结果表明：<strong>Fast路径在弱化其空间建模能力的同时增强了其时间建模的能力</strong></p><p><strong>Lateral Connection:</strong></p><p>连接点：</p><p>​ ResNet: pool1, res2, res3, and res4</p><p>连接的几种方式：</p><p>​ slow pathway: <span class="math inline">\(\{T,S^2,C\}\)</span></p><p>​ fast pathway: <span class="math inline">\(\{\alpha T,S^2,\betaC\}\)</span></p><ol type="1"><li><strong>Time-to-channel</strong></li></ol><p>​ 对fast路径的特征张量进行转换，由<span class="math inline">\(\{\alphaT,S^2,\beta C\}\)</span> 转换为<spanclass="math inline">\(\{T,S^2,\alpha\beta C\}\)</span></p><ol start="2" type="1"><li><strong>Time-strided sampling</strong></li></ol><p>​ 从fast路径的特征张量中随机抽取一个样本<spanclass="math inline">\(\{T,S^2,\beta C\}\)</span></p><ol start="3" type="1"><li><strong>Time-strided convolution</strong></li></ol><p>​ 直接进行3D卷积操作：<span class="math inline">\(5*1*1\)</span>kernal size、<span class="math inline">\(2\beta C\)</span> outputchannels、<span class="math inline">\(stride = \alpha\)</span></p><p>网络实例化参数：以resnet为例</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202201111527460.png" alt="image-20220111152716366" style="zoom:80%;" /></p><h1 id="三实验分析">三、实验分析：</h1><p><strong>实验数据集：</strong></p><p>​ <strong>Kinetics-400</strong>： 400 个人类动作类别中的 240k训练视频和 20k 验证视频组成</p><p>​ <strong>Kinetics-600</strong>：600 个类中有 392k 训练视频和 30k验证视频</p><p>​ <strong>Charades</strong>：157 个类中有 9.8k 训练视频和 1.8k验证视频</p><p>​ <strong>AVA</strong> ：数据取自 437 部电影。时空标签每秒提供一帧，每个人都用边界框和（可能是多个）动作进行注释</p><p>首先看作者在几个数据集上测试的网络性能结果：</p><p><strong>Kinetics-400：</strong></p><p>​效果没什么好说的，都达到sota水平，作者在此基础上还分别测试了不同超参下的网络表现，其中NL代表的是使用nonlocal网络</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322161913268.png" alt="image-20220322161913268"  /></p><p><strong>Kinetics-600：</strong></p><p>​ Kinetics-600数据集也是一样，可以看到超参数<spanclass="math inline">\(\tau = 16\)</span> <spanclass="math inline">\(\beta = 8\)</span>效果是最好的</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322161953660.png" alt="image-20220322161953660"  /></p><p><strong>Charades ：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322162022581.png" alt="image-20220322162022581"  /></p><p><strong>AVA:</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322162133270.png" alt="image-20220322162133270"  /></p><p><strong>消融实验：</strong></p><p>1、作者对比了只是用slow网络和使用slowfast网络的效果，可以看到加了fast网络之后，整体计算量提升的并不大，但是准确率提升很明显</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322162731495.png" alt="image-20220322162731495"  /></p><p>2、关于超参数<span class="math inline">\(\beta\)</span>的实验</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322162910120.png"alt="image-20220322162910120" /><figcaption aria-hidden="true">image-20220322162910120</figcaption></figure><p>3、采用不同的输入 测试效果</p><p>可以看到采用灰度图的准确率和RGB相差不大，减少了运算量</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20220322162946910.png"alt="image-20220322162946910" /><figcaption aria-hidden="true">image-20220322162946910</figcaption></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> action recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL note</title>
      <link href="post/45dd.html"/>
      <url>post/45dd.html</url>
      
        <content type="html"><![CDATA[<h1 id="一基础知识">一、基础知识：</h1><h2 id="sql分类">1.1 SQL分类：</h2><ul><li><p><strong>数据查询语言</strong>(DQL-Data Query Language)</p><p>带有select关键字的都是查询语句</p></li><li><p><strong>数据操纵语言</strong>(DML-Data Manipulation Language)</p><p>DML操作的是表中的数据。比如：insert、delete、update</p></li><li><p><strong>数据定义语言</strong>(DDL-Data Definition Language)</p><p>DDL主要操作的是表的结构。比如： create、drop、alter</p></li><li><p><strong>事务控制语言</strong>(TCL-Transactional ControlLanguage)</p><p>事务提交：commit 事务回滚：rollback</p></li><li><p><strong>数据控制语言</strong>(DCL-Data Control Language)</p><p>比如： 授权grant、撤销权限revoke</p></li></ul><span id="more"></span><h2 id="条件查询">1.2 条件查询：</h2><table><colgroup><col style="width: 18%" /><col style="width: 81%" /></colgroup><thead><tr class="header"><th>运算符</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>=</td><td>等于</td></tr><tr class="even"><td>&lt;&gt;或!=</td><td>不等于</td></tr><tr class="odd"><td>&lt;</td><td>小于</td></tr><tr class="even"><td>&lt;=</td><td>小于等于</td></tr><tr class="odd"><td>&gt;</td><td>大于</td></tr><tr class="even"><td>&gt;=</td><td>大于等于</td></tr><tr class="odd"><td>between…and ….</td><td>两个值之间,等同于 &gt;= and &lt;= 闭区间</td></tr><tr class="even"><td>is null</td><td>为 null（ is not null 不为空）</td></tr><tr class="odd"><td>and</td><td>并且</td></tr><tr class="even"><td>or</td><td>或者</td></tr><tr class="odd"><td>in</td><td>包含，相当于多个 or （ not in 不在这个范围中）</td></tr><tr class="even"><td>not</td><td>not 可以取非，主要用在 is 或 in 中</td></tr><tr class="odd"><td>like</td><td>like 称为模糊查询，支持%或下划线匹配 %匹配任意个字符下划线，一个下划线只匹配一个字符</td></tr></tbody></table><p><strong>特别注意：</strong></p><ol type="1"><li>在数据库当中null不能使用等号进行衡量。需要使用is null</li><li>and和or同时出现，and优先级较高。如果想让or先执行，需要加“小括号”</li></ol><h2 id="排序">1.3 排序：</h2><p>排序采用 order by 子句， order by后面跟上排序字段，排序字段可以放多个，多个采用逗号间隔， order by默认采用升 序，如果存在 where 子句那么 order by 必须放到 where语句的后面</p><p>升序： asc</p><p>降序： desc</p><h2 id="单行处理函数">1.4 单行处理函数:</h2><table><thead><tr class="header"><th>Lower</th><th>转换小写</th></tr></thead><tbody><tr class="odd"><td>upper</td><td>转换大写</td></tr><tr class="even"><td>substr</td><td>取子串（ substr(被截取的字符串,起始下标, 截取的长度)）</td></tr><tr class="odd"><td>length</td><td>取长度</td></tr><tr class="even"><td>trim</td><td>去空格</td></tr><tr class="odd"><td>str_to_date</td><td>将字符串转换成日期</td></tr><tr class="even"><td>date_format</td><td>格式化日期</td></tr><tr class="odd"><td>format</td><td>设置千分位</td></tr><tr class="even"><td>round</td><td>四舍五入</td></tr><tr class="odd"><td>rand()</td><td>生成随机数</td></tr><tr class="even"><td>Ifnull</td><td>可以将 null 转换成一个具体值</td></tr></tbody></table><h2 id="聚合函数分组函数">1.5 聚合函数/分组函数：</h2><table><thead><tr class="header"><th>count</th><th>取得记录数</th></tr></thead><tbody><tr class="odd"><td>sum</td><td>求和</td></tr><tr class="even"><td>avg</td><td>取平均</td></tr><tr class="odd"><td>max</td><td>取最大的数</td></tr><tr class="even"><td>min</td><td>取最小的数</td></tr></tbody></table><p><strong>特别注意：</strong></p><ol type="1"><li>分组函数自动忽略NULL，不需要提前对NULL进行处理。</li><li>count(具体字段)：表示统计该字段下所有不为NULL的元素的总数。count(*)：统计表当中的总行数。（只要有一行数据count则++）</li><li>分组函数不能够直接使用在where子句中。</li></ol><h2 id="分组查询">1.6 *分组查询：</h2><div class="sourceCode" id="cb1"><preclass="sourceCode sql"><code class="sourceCode sql"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">select</span> </span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">from</span></span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">where</span></span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">group</span> <span class="kw">by</span></span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">having</span></span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">order</span> <span class="kw">by</span></span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">..</span>.</span></code></pre></div><p>执行顺序：</p><ol type="1"><li>from</li><li>where</li><li>group by</li><li>having</li><li>select</li><li>order by</li></ol><p><strong>关于having的说明：</strong>:</p><p>使用having可以对分完组之后的数据进一步过滤。having不能单独使用，having不能代替where，having必须和groupby联合使用。</p><p>where和having的区别：一个是在分组前执行，一个是在分组后执行，where和having，优先选择where，where实在完成不了了，再选择having。</p><h2 id="连接查询">1.7 连接查询：</h2><p>根据表连接的方式分类：</p><ul><li><strong>内连接：</strong><ul><li>等值连接 join</li><li>非等值连接</li><li>自连接</li></ul></li><li><strong>外连接：</strong><ul><li>左外连接 (left join)</li><li>右外连接 (right join)</li></ul></li></ul><p>当两张表进行连接查询，没有任何条件限制的时候，最终查询结果条数，是两张表条数的乘积，这种现象被称为：笛卡尔积现象。（笛卡尔发现的，这是一个数学现象。）</p><h2 id="子查询">1.8 子查询:</h2><ul><li>where后面的子查询</li><li>from后面的子查询</li><li>select后面的子查询 [了解]</li></ul><h2 id="union合并">1.9 union合并：</h2><p>union的效率要高一些。</p><h2 id="limit分页">1.10 limit分页：</h2><p>limit startIndex,length</p><p>第一个参数代表取数据的起始下标，默认从0开始</p><p>第二个参数代表取多少个数据</p><h2 id="日期">1.11 *日期：</h2><p>mysql的日期格式： %Y 年 %m 月 %d 日 %h 时 %i 分 %s 秒</p><p>str_to_date函数可以把字符串varchar转换成日期date类型数据</p><ul><li>str_to_date('字符串日期', '日期格式')</li></ul><p>date_format函数通常使用在查询日期方面。设置展示的日期格式。</p><ul><li>date_format(日期类型数据, '日期格式')</li></ul><p><strong>date是短日期：只包括年月日信息。</strong><strong>datetime是长日期：包括年月日时分秒信息。</strong></p><p>mysql短日期默认格式：%Y-%m-%d mysql长日期默认格式：%Y-%m-%d%h:%i:%s</p><h1 id="二表相关">二、表相关：</h1><h2 id="添加修改删除表结构">2.1 添加、修改、删除表结构：</h2><h2 id="约束">2.2 *约束:</h2><ul><li><strong>非空约束</strong>：not null</li><li><strong>唯一性约束</strong>: unique</li><li><strong>主键约束</strong>: primary key （简称PK）</li><li><strong>外键约束</strong>：foreign key（简称FK）</li><li>检查约束：check（mysql不支持，oracle支持）</li></ul><h3 id="非空约束">2.2.1 非空约束：</h3><p>​ <font color=red>非空约束notnull约束的字段不能为NULL</font>。只有列级约束，没有表级约束！</p><div class="sourceCode" id="cb2"><preclass="sourceCode sql"><code class="sourceCode sql"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>        <span class="kw">id</span> <span class="dt">int</span>,</span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        name <span class="dt">varchar</span>(<span class="dv">255</span>) <span class="kw">not</span> <span class="kw">null</span>  <span class="op">//</span> <span class="kw">not</span> null只有列级约束，没有表级约束！</span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>);</span></code></pre></div><h3 id="唯一性约束">2.2.2 唯一性约束：</h3><p>​<font color=red>唯一性约束unique约束的字段不能重复，但是可以为NULL。</font></p><div class="sourceCode" id="cb3"><preclass="sourceCode sql"><code class="sourceCode sql"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>        <span class="kw">id</span> <span class="dt">int</span>,</span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        name <span class="dt">varchar</span>(<span class="dv">255</span>) <span class="kw">unique</span>,   #列级约束</span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        email <span class="dt">varchar</span>(<span class="dv">255</span>)</span><span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>);</span><span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span><span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="kw">id</span> <span class="dt">int</span>,</span><span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        name <span class="dt">varchar</span>(<span class="dv">255</span>),</span><span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        email <span class="dt">varchar</span>(<span class="dv">255</span>),</span><span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>      <span class="kw">unique</span>(name,email)  #表级约束。</span><span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>);</span></code></pre></div><div class="sourceCode" id="cb4"><preclass="sourceCode sql"><code class="sourceCode sql"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>#unique 和not null可以联合</span><span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>            <span class="kw">id</span> <span class="dt">int</span>,</span><span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>            name <span class="dt">varchar</span>(<span class="dv">255</span>) <span class="kw">not</span> <span class="kw">null</span> <span class="kw">unique</span>  #一个字段同时被not null和unique约束的话,该字段自动变成主键字段</span><span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>);</span></code></pre></div><h3 id="主键约束">2.2.3 主键约束：</h3><p>主键值是每一行记录的唯一标识。</p><div class="sourceCode" id="cb5"><preclass="sourceCode sql"><code class="sourceCode sql"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>            <span class="kw">id</span> <span class="dt">int</span> <span class="kw">primary</span> <span class="kw">key</span>,  #列级约束  单一主键</span><span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>            name <span class="dt">varchar</span>(<span class="dv">255</span>)</span><span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>);</span><span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span><span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span><span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="kw">create</span> <span class="kw">table</span> t(</span><span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            <span class="kw">id</span> <span class="dt">int</span>,</span><span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            name <span class="dt">varchar</span>(<span class="dv">255</span>),</span><span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            email <span class="dt">varchar</span>(<span class="dv">255</span>),</span><span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            <span class="kw">primary</span> <span class="kw">key</span>(<span class="kw">id</span>,name)   # 使用表级约束添加复合主键</span><span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>);</span></code></pre></div><p><strong>实际开发中一般只使用单一主键。</strong></p><p><strong>一张表，主键约束只能添加1个（主键只能有1个）</strong></p><h3 id="外键约束">2.2.4 外键约束：</h3><p>外键主要是维护表之间的关系的，主要是为了保证参照完整性，如果表中的某个字段为外键字段，那么该字段的值必须来源于参照的表的字段。</p><h1 id="三存储引擎">三、存储引擎：</h1><h2 id="myisam存储引擎">3.1 <strong>MyISAM存储引擎</strong>：</h2><p><strong>MyISAM索引文件和数据文件是分离的（非聚集）。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109100912965.png" alt="image-20210910091152820" style="zoom: 67%;" /></p><p>使用三个文件表示每个表： ​ 1. 格式文件 —存储<strong>表结构</strong>的定义（mytable.frm） ​ 2. 数据文件 —存储<strong>表数据</strong>（mytable.MYD） ​ 3. 索引文件 —存储<strong>表索引</strong>（mytable.MYI）：索引是一本书的目录，缩小扫描范围，提高查询效率的一种机制。​<br />​对于一张表来说，只要是主键，或者加有unique约束的字段上会自动创建索引。</p><p>MyISAM存储引擎特点：<strong>可被转换为压缩</strong>、只读表来节省空间这是这种存储引擎的优势！！！！<strong>MyISAM不支持事务机制，安全性低。</strong></p><h2 id="innodb存储引擎">3.2 <strong>InnoDB存储引擎</strong>：</h2><p>聚集</p><p>​ 表数据文件本身就是按B+树组织的一个索引结构文件</p><p>​ 聚集索引--叶子节点包含了完整的数据记录</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109100927834.png" alt="image-20210910092704666" style="zoom: 67%;" /></p><p>​ 这是mysql默认的存储引擎，同时也是一个重量级的存储引擎。 ​InnoDB支持事务，支持数据库崩溃后自动恢复机制。 ​InnoDB存储引擎最主要的特点是：非常安全。</p><p>它管理的表具有下列主要特征： – 每个 InnoDB 表在数据库目录中以.frm格式文件表示 – InnoDB 表空间 tablespace被用于存储表的内容（表空间是一个逻辑名称。表空间存储数据+索引。） –提供一组用来记录事务性活动的日志文件 – 用 COMMIT(提交)、SAVEPOINT及ROLLBACK(回滚)支持事务处理 – 提供全 ACID 兼容 – 在 MySQL服务器崩溃后提供自动恢复 – 多版本（MVCC）和行级锁定 –支持外键及引用的完整性，包括级联删除和更新</p><p>InnoDB最大的特点就是支持事务：以保证数据的<strong>安全</strong>。效率不是很高，并且也不能压缩，不能转换为只读，不能很好的节省存储空间。</p><h3 id="innodb页结构">3.2.1 InnoDB页结构：</h3><p>Innodb基本单位页结构：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109121033647.png" alt="image-20210912103339528" style="zoom: 67%;" /></p><p>Buffer Pool 原理：</p><p>​ BufferPool用来缓存客户端可能要频繁访问的数据，基本单位是页，下图中绿色的页表示当前页缓存了磁盘中的某一页数据，白色的页表示BufferPool的空闲区，黄色的页是代表了该缓存的页被客户端修改了，此时的数据状态和磁盘对应的文件不一致。</p><p>三个链表：</p><p>​ <strong>Free链表</strong>是记录了BufferPool中当前空闲的区域，当有查询请求触发了从磁盘加载某一页数据时，会同时将该数据缓存起来。</p><p>​<strong>Flush链表</strong>记录了被修改过的页，后台会有线程定时将数据刷新到磁盘，保证数据一致性。</p><p>​<strong>LRU链表</strong>是用来记录和更新频繁访问的页数据，从而实现高效的数据访问性能。</p><p>​对于像全表查询这种请求，会严重影响LRU的性能，所以在MySQL中的LRU链表分为了两部分，热数据区域和冷数据区域，这也就涉及到一个问题，冷热数据的变更，在MySQL规定了一个阈值：1秒，只有在冷数据区域的数据，临近两次访问的时间间隔大于1秒，数据才会被提到热数据区域。而对于全表查询这种请求，会在短时间内频繁访问某一页，所以它并不会被提取到热数据区域，从而避免了全表查询引起LRU大面积缓存失效。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109171041577.png" alt="InnoDB页" style="zoom:80%;" /></p><h2 id="memory存储引擎">3.3 <strong>MEMORY存储引擎</strong></h2><p>​ 使用 MEMORY 存储引擎的表，其数据存储在内存中，且行的长度固定， ​这两个特点使得 MEMORY 存储引擎非常快。</p><p>MEMORY 存储引擎管理的表具有下列特征： –在数据库目录内，每个表均以.frm 格式的文件表示。 –表数据及索引被存储在内存中。（目的就是快，查询快！） – 表级锁机制。 –不能包含 TEXT 或 BLOB 字段。</p><p>MEMORY 存储引擎以前被称为HEAP 引擎。</p><p>MEMORY引擎优点：查询效率是最高的。不需要和硬盘交互。MEMORY引擎缺点：不安全，关机之后数据消失。因为数据和索引都是在内存当中。</p><h1 id="四事务">四、事务：</h1><p>查看当前事务级别：</p><pre class="mysql"><code>select @@tx_isolation;</code></pre><p>一个事务其实就是一个完整的业务逻辑,是一个最小的工作单元。不可再分。</p><p>与事务相关的操作是DML语句，也就是insert、update、delete</p><h2 id="事务的四大特性">4.1 事务的四大特性;</h2><pre><code>A：原子性    说明事务是最小的工作单元。不可再分。C：一致性    所有事务要求，在同一个事务当中，所有操作必须同时成功，或者同时失败，    以保证数据的一致性。I：隔离性    A事务和B事务之间具有一定的隔离。    教室A和教室B之间有一道墙，这道墙就是隔离性。    A事务在操作一张表的时候，另一个事务B也操作这张表会那样？？？D：持久性    事务最终结束的一个保障。事务提交，就相当于将没有保存到硬盘上的数据    保存到硬盘上！</code></pre><h2 id="隔离性的四个级别">4.2 隔离性的四个级别：</h2><p>假设现在有两个事务，事务A和事务B。</p><ul><li><p><font color=red><strong>读未提交 readuncommitted（最低的隔离级别）</strong></font></p><p>A事务执行过程中，B事务读取了A事务的修改。但是由于某些原因，A事务可能没有完成提交，发生RollBack了操作，则B事务所读取的数据就会是不正确的。这个未提交数据就是脏读。</p><p><strong>存在问题：</strong></p><p>​ <strong>脏读现象</strong></p></li><li><p><font color=red><strong>读已提交：read committed（ORACLE默认级别）</strong></font></p><p>在读已提交这个级别下，事务A只能读取到事务B提交之后的数据。</p><p><strong>解决了脏读现象的问题</strong></p><p><strong>存在问题：</strong></p><p>​<strong>不可重复读现象</strong>：在当前事务开启后，对于同一条查询语句，在当前事务中的不同时间点执行后出现了结果不一致的现象，称为不可重复读，因为在该隔离级别下，两次查询的间隔中有可能存在其他事务对该表进行了修改操作，并且事务完成提交了，所以会出现这种情况。</p></li><li><p><font color=red><strong>可重复读：repeatable read（MySQL默认级别）</strong></font></p><p>在事务A开启后，在事务中的任何时间点，读取到的数据都是一致的。即使中途有其他事务对表做了修改操作并且提交了，事务A读取到的数据还是没有发生改变，这就是可重复读。</p><p><strong>解决了不可重复读问题</strong></p><p><strong>存在问题：</strong></p><p>​<strong>幻读现象</strong>：B事务读取了两次数据，在这两次的读取过程中A事务添加了数据，B事务的这两次读取出来的集合不一样。</p></li><li><p><font color=red><strong>序列化/串行化：serializable（最高的隔离级别）</strong></font></p><p>这种隔离级别最高，效率最低，解决了上面的所有问题。它可以理解为事务排队执行，不能并发</p><p>通常数据库不会用这个隔离级别，我们需要其他的机制来解决这些问题:<strong>乐观锁和悲观锁</strong>。</p></li></ul><h2 id="mvcc机制">4.3 MVCC机制：</h2><p>MySQL中 InnoDB实现了MVCC的事务并发处理机制</p><p>多版本并发控制：</p><p>MVCC提供了时间一致性的处理思路，在MVCC下读事务时，通常使用一个时间戳或者事务ID来确定访问哪个状态的数据库及哪些版本的数据。</p><ul><li><p><strong>ReadView</strong>：一致性视图</p></li><li><p><strong>DB_TRX_ID</strong>: 6ByteInnoDB中每个事务有一个唯一的事务ID叫做 transactionid。在事务开始时向InnoDB事务系统申请得到，是按申请顺序严格递增的</p></li><li><p><strong>Rollpoint</strong>：回滚指针，7Byte，指向当前记录的ROLLBACK SEGMENT的<strong>undolog</strong>记录，通过这个指针获得之前版本的数据。该行记录上所有旧版本在undo log 中都通过链表的形式组织。</p></li><li><p><strong>DB_ROW_ID</strong>：如果声明了主键，InnoDB以用户指定的主键构建B+Tree，如果未声明主键，InnoDB会自动生成一个隐藏主键，说的就是DB_ROW_ID</p><p>另外，每条记录的头信息（recordheader）里都有一个专门的<code>bit</code>（<strong>deleted_flag</strong>）来表示当前记录是否已经被删除</p></li></ul><p>要实现read committed在另一个事务提交之后其他事务可见和repeatableread在一个事务中SELECT操作一致，就是依靠ReadView，对于readuncommitted，直接读取最新值即可，而serializable采用加锁的策略通过牺牲并发能力而保证数据安全，<strong>因此只有RC和RR这两个级别需要在MVCC机制下通过ReadView来实现</strong>。</p><p><strong>在readcommitted级别下，readview会在事务中的每一个SELECT语句查询发送前生成</strong>（也可以在声明事务时显式声明STARTTRANSACTION WITH CONSISTENTSNAPSHOT），因此每次SELECT都可以获取到当前已提交事务和自己修改的最新版本。</p><p><strong>而在repeatableread级别下，每个事务只会在第一个SELECT语句查询发送前或显式声明处生成</strong>，其他查询操作都会基于这个ReadView，这样就保证了一个事务中的多次查询结果都是相同的，因为他们都是基于同一个ReadView下进行MVCC机制的查询操作。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109111900551.png"alt="image-20210911185932181" /><figcaption aria-hidden="true">image-20210911185932181</figcaption></figure><p>InnoDB为每一个事务构造了一个数组m_ids用于保存一致性视图生成瞬间当前所有活跃事务(开始但未提交事务)的ID，将数组中事务ID最小值记为低水位min_id，当前系统中已创建事务ID最大值+1记为高水位max_id</p><p>一致性视图下查询操作的流程如下:</p><ol type="1"><li><p>当查询发生时根据以上条件生成ReadView，该查询操作遍历<strong>Undolog链</strong>，根据当前被访问版本(可以理解为Undolog链中每一个记录即一个版本，遍历都是从最新版本向老版本遍历)的DB_TRX_ID，如果DB_TRX_ID小于min_id,则该版本在ReadView生成前就已经完成提交，该版本可以被当前事务访问。<strong>DB_TRX_ID在绿色范围内的可以被访问</strong></p></li><li><p>若被访问版本的DB_TRX_ID大于max_id，说明该版本在ReadView生成之后才生成，因此该版本不能被访问，根据当前版本指向上一版本的指针DB_ROLL_PT访问上一个版本，继续判断。<strong>DB_TRX_ID在蓝色范围内的都不允许被访问</strong></p></li><li><p>若被访问版本的DB_TRX_ID在[min_id,max_id)区间内，则判断DB_TRX_ID是否等于当前事务ID，等于则证明是当前事务做的修改，可以被访问，否则不可被访问,继续向上寻找。<strong>只有DB_TRX_ID等于当前事务ID才允许访问橙色范围内的版本</strong></p></li><li><p>最后，还要确保满足以上要求的可访问版本的数据的delete_flag不为true，否则查询到的就会是删除的数据。</p></li></ol><p><strong>一致性读和当前读</strong>:</p><p>当前读会触发视图更新</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109111934622.png" alt="image-20210911193402088" style="zoom:67%;" /></p><h2 id="事务底层原理">4.4 事务底层原理:</h2><h3 id="事务执行过程">4.4.1事务执行过程：</h3><ol type="1"><li>首先执行查询语句，如果数据不在bufferpool中则需要从磁盘中加载数据到buffer pool</li><li>修改bufferpool里面的页数据-------（这时磁盘和缓冲区的数据是不一致的）</li><li>根据相应的DML语句生成redo log --&gt; log bufer</li><li>redo log持久化（有三种方案：<strong>0</strong>：不立即进行持久化，由后台线程去做；<strong>1</strong>：写到操作系统缓存区，然后立即持久化到磁盘；<strong>2</strong>：写到操作系统缓存区，然后定时刷新到磁盘）</li><li>bin log 持久化</li><li>undo log 持久化</li><li>操作成功，事务提交。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109181407465.png" alt="image-20210918140634691" style="zoom:67%;" /></p><h3 id="三种log的区别">4.4.2 三种log的区别：</h3><ul><li><strong>Redo log:</strong>InnoDB引擎中的概念，记录了页中的具体行被修改的数据信息</li><li><strong>Bin log:</strong> MySQL中的概念，记录的是sql语句，比如在主从复制的时候，只能依赖于Bin log文件</li><li><strong>Undo log:</strong>记录事务执行过程前的数据状态信息，在roolback操作时 使用Undo log文件</li></ul><h2 id="锁机制">4.5 锁机制：</h2><h1 id="五索引">五、索引：</h1><h2 id="索引操作">5.1 索引操作：</h2><p>创建索引：</p><pre class="mysql"><code>create index index_name on table_name(column_name)</code></pre><p>删除索引：</p><pre class="mysql"><code>drop index index_name on table_name</code></pre><h2 id="索引失效">5.2 索引失效：</h2><ul><li><p>查询条件使用了百分号在第一位的模糊查询</p><pre class="mysql"><code>select * from user where user_name like &#39;%k&#39;;</code></pre></li><li><p>使用or的时候会失效，如果使用or那么要求or两边的条件字段都要有索引，才会走索引，如果其中一边有一个字段没有索引，那么另一个字段上的索引也会失效。</p></li><li><p>使用复合索引的时候，没有使用左侧的列查找，索引失效。</p></li><li><p>在where当中索引列参加了运算，索引失效。</p><pre class="mysql"><code>select * from score where score+1 = 61;</code></pre></li><li><p>在where当中索引列使用了函数。</p><pre class="mysql"><code>select * from user where lower(user_name)=&#39;jack&#39;;</code></pre></li></ul><h2 id="索引数据结构">5.3 索引数据结构:</h2><h3 id="二叉树搜索二叉树">5.3.1 二叉树(搜索二叉树)：</h3><p>对于单边增长的数据来说，用搜索二叉树和全表扫描效率一样，所以mysql没有选择这种数据结构</p><h3 id="红黑树一种平衡二叉树">5.3.2 红黑树(一种平衡二叉树)：</h3><p>数据量大的时候，树的高度不可控</p><h3 id="hash">5.3.3 Hash：</h3><p>优点：</p><pre><code>1. 键值对唯一，速度快</code></pre><p>不足：</p><pre><code>1. 无法支持范围查询2. 存在hash冲突的问题：  解决方法有： 开放定址法、链地址法3. 不支持队列联合索引4. 无法利用索引完成排序，以及like这样的部分模糊查询</code></pre><h3 id="b树">5.3.4 B树：</h3><p>不支持范围查询，非叶子节点存储data‘数据占用空间</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109100958675.png" alt="image-20210910095846537" style="zoom:67%;" /></p><h3 id="mysql中的b树">5.3.5 MySQL中的B+树：</h3><p>B+树在B树的基础上做了改变。<strong>多叉平衡二叉树</strong></p><ul><li><strong>叶子节点存储所有索引字段</strong></li><li>叶子节点多了一个双向指针，可以支持范围查询。</li><li><strong>非叶子节点不存储data</strong>，只存储索引</li><li>叶子节点用指针连接，提高区间访问的性能</li></ul><p><img src="../../../Users/Yore/AppData/Roaming/Typora/typora-user-images/image-20210910090250826.png" alt="image-20210910090250826" style="zoom:80%;" /></p><ol type="1"><li>为什么推荐必须建主键？</li></ol><p>如果没有主键的话，mysql会自动帮表建一个隐式的字段，还要帮忙维护，增加数据库压力</p><ol start="2" type="1"><li><p>为什么推荐使用自增主键？</p><p>B+树叶子节点本身是有序的排列，如果是自增的话，只会一直在末尾加节点，不是自增的话，可能会引起树的结构发生很大的变化。</p></li></ol><h2 id="联合索引">5.4 联合索引：</h2><p><strong>联合索引的底层数据结构？</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202109101449826.png" alt="image-20210910144956680" style="zoom:67%;" /></p><p><strong>联合索引的排序是怎么排的？</strong></p><p>​ 按照<strong>最左匹配原则</strong>，逐个字段比较。</p><h3 id="一张表建立多个索引">5.5 一张表建立多个索引：</h3><p>在Innodb中，我们在数据库中会根据主键索引建立一棵B+树，data字段是包含了表中的所有数据（<strong>聚集索引</strong>）。如果还有其他字段的索引或者是联合索引的话，对于每一个索引都会建立一个B+树，但是其他树的data并不是包含表中的所有数据（<strong>非聚集索引</strong>），因为这样的话相当于是将一张表的数据拷贝的多份，既浪费资源同时在做更新操作时又会增加维护成本，所以在MySQL中，其他的索引的data字段其实是绑定了主键，通过索引找到主键，然后再基于主键索引查找完整的记录。这就是<strong>回表查询</strong>。</p><h1 id="六.-数据库三范式">六. 数据库三范式：</h1><h2 id="第一范式">6.1 第一范式：</h2><p><strong>要求任何一张表必须有主键，每一个字段原子性不可再分。</strong></p><h2 id="第二范式">6.2 第二范式：</h2><p>建立在第一范式的基础之上，要求所有非主键字段<strong>完全依赖</strong>主键，不要产生<strong>部分依赖</strong>。</p><h2 id="第三范式">6.3 第三范式：</h2><p>建立在第二范式的基础之上，要求所有非主键字段<strong>直接依赖</strong>主键，不要产生<strong>传递依赖</strong>。</p><h1 id="七.-mysql优化">七. MySQL优化：</h1>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka note</title>
      <link href="post/65c9.html"/>
      <url>post/65c9.html</url>
      
        <content type="html"><![CDATA[<h1 id="一基础知识">一、基础知识：</h1><h2 id="kafka介绍">1.1 kafka介绍：</h2><p>ApacheKafka是一个<strong>快速、可扩展的、高吞吐、可容错</strong>的分布式发布订阅消息系统。Kafka具有高吞吐量、内置分区、支持数据副本和容错的特性，适合在大规模消息处理场景中使用。</p><p><strong>Kafka提供了什么功能：</strong></p><ul><li>要<strong>发布</strong>（写）和<strong>订阅</strong>（读）流事件</li><li><strong>存储</strong>持久和可靠的事件流</li><li>支持<strong>实时</strong>和<strong>离线</strong>的事件流处理</li></ul><p><strong>Kafka有什么特性：</strong></p><ul><li>分布式、高吞吐、高可用、可扩展</li><li>数据持久化</li><li>支持多消费者</li></ul><span id="more"></span><h2 id="消息中间件作用">1.2 消息中间件作用：</h2><ul><li><p><strong>解耦</strong></p><p>消息队列在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p></li><li><p><strong>冗余</strong></p><p>有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。</p></li><li><p><strong>扩展性</strong></p><p>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的；只要另外增加处理过程即可。不需要改变代码、不需要调节参数。</p></li><li><p><strong>灵活性 &amp;</strong> <strong>峰值处理能力</strong></p><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p></li><li><p><strong>可恢复性</strong></p><p>当体系的一部分组件失效，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。而这种<strong>允许重试或者延后处理请求</strong>的能力通常是造就一个略感不便的用户和一个沮丧透顶的用户之间的区别。</p></li><li><p><strong>顺序保证</strong></p><p>在大多使用场景下，数据处理的顺序都很重要。消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。部分消息系统保证消息通过FIFO（先进先出）的顺序来处理，因此消息在队列中的位置就是从队列中检索他们的位置。</p></li><li><p><strong>异步处理机制</strong></p><p>有时候，你不想也不需要立即处理消息。消息队列提供了异步处理机制，允许你把一个消息放入队列，但并不立即处理它。你想向队列中放入多少消息就放多少，然后在你乐意的时候再去处理它们。</p></li></ul><h2 id="主要概念">1.3 主要概念：</h2><ol type="1"><li><strong>事件</strong>【event】</li></ol><p>事件可以说是在kafka中流动的最小数据单元，代表了一条消息或者记录。从概念上讲，事件具有键、值、时间戳和可选的元数据标头等数据信息。</p><ol start="2" type="1"><li><p><strong>主题</strong>【topic】</p><p>事件被组织并持久地存储在<strong>主题中</strong>。非常简单，主题类似于文件系统中的文件夹，事件就是该文件夹中的文件，通过主题可以起到一个归类的作用，对不同的事件存储在不同的主题中。</p></li><li><p><strong>分区</strong>【partition】</p><p>主题是可以分区的，这意味着不同的事件数据可以存放在不同的分区中，而不同的分区可以在不同的broker上，而分区的选择可以是根据事件的键来完成。数据的这种分布式放置对于可伸缩性非常重要，因为它允许客户端应用程序同时从/向多个broker读取和写入数据，当一个新事件发布到一个主题时，它实际上被附加到该主题的分区之一。并且Kafka保证给定主题分区的任何消费者将始终以与写入事件完全相同的顺序读取该分区的事件。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112131806922.png" alt="image-20211213180551133"  /></p></li><li><p><strong>副本</strong>【replication】</p><p>为了使数据具有容错性和高可用性，每个主题都可以<strong>复制</strong>，甚至可以跨地理区域或数据中心进行<strong>复制</strong>，以便始终有多个代理拥有数据副本，以防万一出现问题，你想要对broker进行维护等。常见的生产设置是复制因子为3，即你的数据将始终存在三个副本。此复制在主题分区级别执行。</p></li><li><p><strong>生产者</strong>【producer】</p><p>生产者是那些向 Kafka 发布（写入）事件的客户端应用程序</p></li><li><p><strong>消费者</strong>【consumer】</p><p>消费者是订阅（读取和处理）这些事件的客户端应用程序</p></li><li><p><strong>代理</strong>【broker】</p><p>已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker).消费者可以订阅一个或多个主题，并从broker拉数据，从而消费这些已发布的消息(事件)。</p></li></ol><h1 id="二原理剖析">二、原理剖析：</h1><h2 id="零拷贝">2.1 零拷贝：</h2><p>在写一个服务端程序时（WebServer或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：将服务端主机磁盘中的文件不做修改地从已连接的socket发出去，我们通常用下面的代码完成：</p><div class="sourceCode" id="cb1"><preclass="sourceCode java"><code class="sourceCode java"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span><span class="op">((</span>n <span class="op">=</span> <span class="fu">read</span><span class="op">(</span>diskfd<span class="op">,</span> buf<span class="op">,</span> BUF_SIZE<span class="op">))</span> <span class="op">&gt;</span> <span class="dv">0</span><span class="op">)</span></span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">write</span><span class="op">(</span>sockfd<span class="op">,</span> buf <span class="op">,</span> n<span class="op">);</span></span></code></pre></div><p>基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于Linux的I/O操作默认是缓冲I/O。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上I/O操作中，发生了多次的数据拷贝。</p><p>当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112211059109.png" alt="image-20211220170048310" style="zoom: 80%;" /></p><p>从上图中可以看出，共产生了四次数据拷贝，即使使用了DMA来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。</p><p><font color=red><strong>零拷贝主要的任务就是避免CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。</strong></font></p><p>我们继续回到引文中的例子，我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型：</p><p>让数据传输不需要经过user space，实现方式有： MMAP</p><p>应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝到内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112211059969.png" alt="image-20211220170235871"  /></p><p>使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时,write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设计模式</title>
      <link href="post/a2d2.html"/>
      <url>post/a2d2.html</url>
      
        <content type="html"><![CDATA[<h1 id="一6大设计原则">一、6大设计原则：</h1><h2 id="单一职责原则">1.1 单一职责原则:</h2><p>单一职责原则 (Single Responsibility Principle) SRP的定义：<font color =red><strong>应该有且仅有一个原因引起类的变更</strong>。</font></p><p>单一职责原则适用于接口、类，同样适用于方法，一个方法尽可能只做一件事情，但是该原则最难划分的就是<strong>职责</strong>。一般建议来看，接口的设计<strong>一定</strong>要做到单一职责，类的设计<strong>尽量</strong>做到只有一个原因引起变化。</p><h2 id="里氏替换原则">1.2 里氏替换原则：</h2><p>(Liskov Substitution Principle) LSP</p><p>4层含义：</p><pre><code>1. 子类必须完全实现父类的方法2. 子类可以有自己的个性3. 覆盖或者实现父类的方法时输入参数可以被**放大**4. 覆写或者实现父类的方法时输出结果可以被**缩小**</code></pre><span id="more"></span><h2 id="依赖倒置原则">1.3 依赖倒置原则：</h2><p>(Dependence Inversion Principle) DIP</p><p>3层含义：</p><pre><code>1. 高层模块不应该依赖底层模块，两者都应该依赖其抽象2. 抽象不应该依赖细节3. 细节应该依赖抽象</code></pre><p>在java语言中，<strong>抽象</strong>就是指接口或者抽象类，两者时不能直接被实例化的。<strong>细节</strong>就是实现类，实现接口或者抽象类而产生的类就是细节。</p><p>依赖倒置原则在java中就是： 面向接口编程（OOD）</p><pre><code>1. 模块间的依赖通过抽象发生，实现类之间不发生直接的依赖关系，其依赖关系是通过接口或者抽象类产生的2. 接口或者抽象类不依赖于实现类3. 实现类依赖接口或抽象类</code></pre><p><strong>采用依赖倒置原则可以减少类间的耦合性，提高系统的稳定性，降低并行开发引起的风险，提高代码的可读性和可维护性。</strong></p><h2 id="接口隔离原则">1.4 接口隔离原则：</h2><p>建立单一的接口，不要建立臃肿庞大的接口。换句话说，就是<strong>接口尽量细化，同时接口中的方法尽量少</strong>。</p><p><font color=red>接口隔离原则</font>和<font color=red>单一职责</font>的<strong>区别</strong>在于<strong>两者的角度是不同的</strong>，单一职责是从业务逻辑划分的角度去看待，要求类和接口的职责要单一。而接口隔离原则是要求接口的方法尽量少，尽量提供专用的接口，有几个模块就有几个接口。</p><h2 id="迪米特法则">1.5 迪米特法则：</h2><p>(Law of Demeter,) LoD</p><p>只与直接的朋友通信。朋友类的定义为：出现在成员变量、方法的输入输出参数中的类称为成员朋友类，而出现在方法体内部的类不属于朋友类。</p><h2 id="开闭原则">1.6 开闭原则：</h2><h1 id="二23种设计模式">二、23种设计模式：</h1><h2 id="单例模式">2.1 *单例模式：</h2><p><strong>定义：</strong>某一个类只有一个实例，而且自行实例化并向整个系统提供该实例。</p><p><strong>特点：</strong></p><pre><code>1. 单例类只能有一个实例2. 单例类必须自己创建自己的实例类</code></pre><p><strong>应用：</strong>单例模式保证了全局对象的唯一性，比如系统启动读取配置文件就需要单例保证配置的一致性。spring中对bean的管理默认都是单例模式，需要注意多线程使用的问题，避免对非静态变量使用不当导致数据不一致的情况发生，比如java中的日期类。</p><p><strong>注意：</strong>多线程使用的时候，注意实例变量是否会因为多线程而产生状态不一致问题。</p><p><strong>实现：</strong> <strong>饿汉式、懒汉式、枚举类</strong></p><h2 id="工厂方法模式">2.2 *工厂方法模式：</h2><p><strong>定义：</strong>定义一个用于创建对象的接口，让子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。</p><p><strong>优点：</strong></p><pre><code>1. 良好的封装性，代码结构清晰，降低模块间的耦合性。2. 可扩展性好，在增加新的产品类时，只需要添加新的子类，不用改动工厂类。3. 高层模块只依赖产品的抽象，符合迪米特法则、依赖倒置原则和里氏替换原则。</code></pre><p><strong>应用场景：</strong>需要灵活、可扩展框架的时候，可以考虑采用工厂方法模式。</p><p><strong>扩展：</strong></p><pre><code>1. 升级为多工厂模式，，对不同的产品定义不同的创造者2. 替换单例模式， 采用反射的方式创建对象3. 延迟初始化，用map容器保存生产的对象</code></pre><h2 id="抽象工厂模式">2.3 *抽象工厂模式：</h2><p><strong>定义：</strong>为创建<strong>一组</strong>相关或相互依赖的对象提供一个接口，而且无须指定它们的具体类。</p><p>工厂方法模式提供了针对一类产品在创建时的低耦合的解决方案，抽象工厂模式是工厂方法模式的升级版，在有多个业务分类时，是非常好的解决方式。</p><p><strong>优点：</strong></p><pre><code>1. 封装性，高层模块并不关心每个产品类是如何创建出来的，这是由具体的某一个工厂类关心的，高层模块只需要知道工厂类是谁就可以创建出需要的对象。2. 产品内部的约束为非公开状态。</code></pre><p><strong>缺点：</strong></p><pre><code>1. 可扩展性不好，如果要增加一个新的产品，涉及到的改动很大。</code></pre><h2 id="模板方法模式">2.4 模板方法模式：</h2><p><strong>定义：</strong>定义一个操作中的算法的框架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构即可重新定义该算法的某些特定步骤。<strong>仅仅使用java的继承机制来实现。</strong></p><p>抽象模板的方法分为两类：</p><ol type="1"><li><p>基本方法：</p><p>也叫基本操作，是由子类实现的方法，并且在模板方法中被调用。</p></li><li><p>模板方法：</p><p>可以有一个或者多个，一般有一个固定的实现逻辑，完成对基本方法的调用。</p></li></ol><p><strong>注：</strong>为了防止恶意操作，一般模板方法都加上final关键字，不允许被覆写。</p><p><strong>优点：</strong></p><pre><code>1. 封装不变部分，扩展可变部分。    不变部分由父类实现，可变部分通过继承由子类来扩展2. 提取公共部分代码，便于维护。3. 行为由父类控制，子类实现。</code></pre><p><strong>缺点：</strong></p><pre><code>高层模块并不是和具体实现完全解耦合。在模板方法中，子类的执行结果会影响父类的结果。</code></pre><h2 id="建造者模式">2.5 建造者模式：</h2><p><strong>定义：</strong>也叫生成器模式，将一个复杂对象的构建和它的表示分离，使得同样的构建过程可以创建不同的表示。</p><p><strong>优点：</strong></p><pre><code>1. 封装性。客户端不必知道产品内部的组成细节。2. 建造者独立，易扩展。</code></pre><p><strong>应用场景：</strong>相同的方法，不同的执行顺序，产生不同的事件结果时，可以考虑使用建造者模式。</p><p><strong>注：</strong></p><p>​建造者模式最主要的功能就是基本方法的调用顺序的安排，通俗的说就是零件的装配，顺序不同产生的对象也不同；而工厂方法的重点是创建。</p><h2 id="代理模式">2.6 *代理模式：</h2><p><strong>定义：</strong>为其他对象提供一种代理以控制对这个对象的访问。</p><p>代理模式也叫委托模式，在日常应用中，代理模式可以提供非常好的访问控制，它有三种角色定义：</p><ul><li>抽象主题角色：可以是抽象类或者接口，是一个普通的业务类型的定义</li><li>具体主题角色：也叫被委托角色、被代理角色，是业务逻辑的具体执行者</li><li>代理主题角色：也叫委托类、代理类，负责对具体角色的应用，把抽象主题角色定义的方法委托给具体主题角色实现。</li></ul><p><strong>优点:</strong></p><pre><code>1. 职责清晰2. 高扩展性</code></pre><p><strong>扩展：</strong></p><ul><li><p><strong>普通代理：</strong>调用者需要知道代理的存在，通过代理去调用真实角色的具体实现</p></li><li><p><strong>强制代理：</strong>调用者直接调用真实角色，不用关心代理是否存在，<strong>代理的产生是由真实角色决定的</strong></p></li><li><p><strong>动态代理：</strong>动态代理是在实现阶段不用关心代理是谁，而在运行阶段才指定代理哪一个对象。相对来说，自己写代理类的方式就是静态代理。</p></li></ul><h2 id="原型模式">2.7 原型模式：</h2><p><strong>定义：</strong>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p><p><strong>实现：</strong>java中实现Cloneable接口来标识这个对象是可拷贝的，这个接口只是一个标记作用，在JVM中具有这个标记的对象才有可能被拷贝。那怎么才能从”有可能被拷贝“转换为”可以被拷贝“呢？通过重写clone方法，这个方法是在Object类中。</p><p><strong>优点：</strong></p><pre><code>1. 性能优良： 原型模式是在内存二进制流中拷贝，比直接new一个对象性能好很多2. 逃避构造函数的约束，直接在内存中拷贝，不会执行构造函数。相应的也是一个缺点，就是减少了约束</code></pre><p><strong>应用：</strong></p><pre><code>1. 资源优化场景，类初始化需要消耗非常多的资源2. 性能和安全场景，通过new一个对象会需要非常多的数据准备或者权限访问，则可以使用原型模式。</code></pre><p><strong>注意事项：</strong></p><ul><li><p>构造函数不会被执行</p></li><li><p>深拷贝和浅拷贝：</p><p>Object提供的方法clone只能拷贝本对象，对象内的数组、引用对象都不拷贝，还是执行原对象的内部元素地址，这种拷贝就属于浅拷贝。</p><p><strong>如果要实现深拷贝的话，可以通过手动对私有的类变量进行独立的拷贝。</strong></p></li><li><p>clone和final冲突，要使用clone方法，类的成员变量上不要增加final关键字。</p></li></ul><h2 id="中介者模式">2.8 中介者模式：</h2><p><strong>定义：</strong>用一个中介对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使其耦合松散，而且可以独立地改变它们之间的交互。</p><p><strong>优点：</strong></p><pre><code>1.减少了类间依赖，原有的一对多的依赖变成了一对一的依赖，同事类只依赖中介者。</code></pre><p><strong>缺点：</strong></p><pre><code>1. 中介者会变的很臃肿，而且逻辑复杂。</code></pre><h2 id="命令模式">2.9 命令模式：</h2><p><strong>定义：</strong>命令模式是一个高内聚模式，将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请求排队或者记录请求日志，可以提供命令的撤销和恢复功能。</p><p><strong>优点：</strong></p><pre><code>1. 类间解耦：调用者与接收者之间没有任何依赖关系2. 可扩展性：Command子类非常容易扩展，而高层模块Invoker和Client不会产生严重的代码耦合3.与其他模式组合，更优秀</code></pre><p><strong>缺点：</strong></p><pre><code>1. 关于Command子类扩展的问题，如果有很多个命令，那么Command子类会膨胀的非常大。(结合模板方法，可以减少Command子类膨胀的问题)</code></pre><h2 id="责任链模式">2.10 责任链模式：</h2><p><strong>定义：</strong>使多个对象都有机会处理请求，从而避免了请求的发送者和接收者之间的耦合关系，将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。</p><p><strong>优点：</strong></p><pre><code>1. 解耦： 请求和处理分开，请求者可以不用知道是谁处理的，处理者可以不用直到请求的全貌</code></pre><p><strong>缺点：</strong></p><pre><code>1. 性能问题：每个请求都是从头到尾遍历链表2. 调试不方便： 链条较长时，类似递归，逻辑比较复杂</code></pre><h2 id="装饰模式">2.11 装饰模式：</h2><p><strong>定义：</strong>动态的给一个对象添加一些额外的职责，就增加功能来说，装饰模式相比生成子更为灵活。</p><p><strong>优点：</strong></p><pre><code>1. 装饰类和被装饰类独立，不会相互耦合2. 装饰模式是继承关系的一个替代方案3. 可以动态扩展一个实现类的功能</code></pre><p><strong>缺点：</strong></p><pre><code>1. 多层装饰比较复杂，定位问题麻烦</code></pre><p><strong>应用：</strong></p><pre><code>1. 需要扩展一个类的功能2. 动态地给一个对象增加功能</code></pre><h2 id="策略模式">2.12 策略模式：</h2><p><strong>定义：</strong>定义一组算法，将每个算法封装起来，并且使它们之间可以互换。</p><p>策略模式使用地是面向对象地继承和多态机制。</p><p>策略模式的重点就是封装角色，借用了代理模式的思路，它和代理模式的差别就是策略模式的封装角色和被封装的策略类不用时同一个接口，如果是同一个接口就成为了代理模式。</p><p><strong>优点：</strong></p><pre><code>1. 策略自定义，自由切换2. 避免使用多重条件判断3. 扩展性良好</code></pre><p><strong>缺点：</strong></p><pre><code>1. 每一个策略都是一个策略类，复用可能性小，所以类数量会增多2. 所有策略类都需要对外暴露</code></pre><p><strong>应用场景：</strong></p><pre><code>1. 多个类只有再算法或行为上稍有不同的场景2. 算法需要自由切换的场景3. 需要屏蔽算法规则</code></pre><p><strong>注：</strong></p><p>如果系统中一个策略家族的具体策略数量超过4个，则需要考虑使用混合模式，解决策略类膨胀和对外暴露的问题。</p><h2 id="适配器模式">2.13 适配器模式：</h2><p><strong>定义：</strong>将一个类的接口变成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。</p><p><strong>优点：</strong></p><pre><code>1. 适配器可以使原本不想关的两个类一起工作2. 增加了类的透明性，3. 灵活性好</code></pre><p><strong>注：</strong><font color=red>适配器模式最好在详细设计阶段不要考虑它，它不是为了解决还处在开发阶段的问题，而是解决正在服役的项目问题。</font></p><ul><li><p>通过对象层次的关联关系进行委托的，而不是继承关系，这种适配器叫对象适配器。</p></li><li><p>通过继承进行的适配，称为类适配器。</p></li></ul><h2 id="迭代器模式">2.14 迭代器模式：</h2><p><strong>定义：</strong>提供一种方法访问一个容器对象中各个元素，而又不用暴露该对象内部的细节。</p><p>迭代器类似一个数据库中的游标，可以在一个容器内上下移动，遍历所有它需要查看的元素。</p><p>从JDK 1.5开始，在各种容器中都已经实现了这种模式的访问，所以对我们遍历某一个容器来说，并不需要重新实现这个过程。</p><h2 id="组合模式">2.15 组合模式：</h2><p><strong>定义：</strong>将对象组合成树状结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。</p><p><strong>优点：</strong></p><pre><code>1. 高层模块调用简单2. 节点自由增加</code></pre><p><strong>缺点：</strong></p><pre><code>1. 组合模式与依赖倒置原则冲突</code></pre><h2 id="观察者模式">2.16 *观察者模式：</h2><p><strong>定义：</strong>观察者模式也叫发布订阅模式。它定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到通知并被自动更新。</p><p><strong>优点：</strong></p><pre><code>1. 观察者被被观察者之间是抽象耦合2. 建立一套触发机制</code></pre><p><strong>缺点：</strong></p><pre><code>1. 存在开发效率和运行效率的问题</code></pre><p><strong>注：</strong></p><ul><li>广播连的问题：它和责任链模式最大的区别在于观察者广播连在传播的过程中消息是随时更改的，而责任链模式在消息传递过程中基本保持消息不变。</li><li>异步处理问题：如果观察者比较多，而且处理时间比较长，就需要考虑采用异步处理，但是异步处理就要靠v了线程安全和队列的问题。(消息队列的作用)</li></ul><h2 id="门面模式">2.17 门面模式：</h2><p>门面模式也叫外观模式，是一种比较常用的封装模式。</p><p><strong>定义：</strong>要求一个子系统的外部与其内部的通信必须通过一个统一的对象进行。</p><p><strong>优点：</strong></p><pre><code>1. 减少系统的相互依赖2. 提高了灵活性3. 提高安全性</code></pre><p><strong>缺点：</strong></p><pre><code>1. 不符合开闭原则</code></pre><p>门面模式的核心就是<strong>封装</strong>。对于和外界系统交互来说，门面模式可以帮助我们提供给外界系统唯一的访问入口，同时它是稳定的，并不参与内部子系统的业务逻辑的变化。</p><h2 id="备忘录模式">2.18 *备忘录模式：</h2><p><strong>定义：</strong>在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可以将该对象恢复到原先保存的状态。</p><p><strong>应用场景：</strong></p><pre><code>1. 需要保存和恢复数据相关状态场景2. 提供一个可回滚的操作3. 数据库连接的事务管理就是采用的备忘录模式</code></pre><p><strong>注意事项：</strong></p><ol type="1"><li>备忘录的生命周期：备忘录创建出来就要在最近的代码中使用，要主动管理它的生命周期，建立就要使用，不用就要立即删除其引用。</li><li>备忘录的性能：不要在频繁建立备份的场景中使用备忘录模式。</li></ol><p><strong>扩展：</strong></p><ol type="1"><li><strong>clone方式的备忘录</strong>：类似于原型模式，考虑到深拷贝和浅拷贝的问题，在复杂场景下，会让你的程序逻辑异常混乱，所以适用于比较简单的场景。</li><li><strong>多状态的备忘录模式：</strong></li><li><strong>多备份的备忘录模式：</strong></li></ol><h2 id="访问者模式">2.19 访问者模式：</h2><p><strong>定义：</strong>封装一些作用于某中数据结构中的各元素的操作，它可以在不改变数据结构的前提下定义作用于这些元素的新的操作。</p><p><strong>优点：</strong></p><pre><code>1. 符合单一职责原则2. 优秀的扩展性3. 灵活性非常高</code></pre><p><strong>缺点：</strong></p><pre><code>1.具体元素对访问者公布细节2.具体元素变更比较困难3.违背了依赖倒置原则</code></pre><h2 id="状态模式">2.20 状态模式：</h2><p><strong>定义：</strong>当一个对象内在状态改变时允许其改变行为，这个对象看起来像改变了其类。</p><p>状态模式的核心是封装，状态的变更引起了行为的变更。对外来说，我们只看到行为的改变，而不用知道是状态变化引起的。</p><p><strong>优点：</strong></p><pre><code>1. 结构清晰：避免了过多的swtich...case或者if...else语句的使用2. 遵循设计原则：很好的体现了开闭原则和单一职责原则3. 良好的封装性</code></pre><p><strong>缺点：</strong></p><pre><code>1. 子类膨胀问题</code></pre><p><strong>应用场景：</strong></p><pre><code>1. 行为随状态改变而改变的场景2. 条件、分支判断语句的替代者：通过扩展子类实现了条件的判断处理</code></pre><h2 id="解释器模式">2.21 解释器模式：</h2><p><strong>定义：</strong>给定一门语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子。</p><p>一个简单的语法分析工具，扩展性好。</p><h2 id="享元模式">2.22 享元模式：</h2><p><strong>定义：</strong>使用共享对象可以有效的支持大量的细粒度的对象。<strong>共享</strong></p><p><strong>优缺点：</strong>享元模式可以大大减少应用程序创建的对象(通过对象池缓存)，降低程序内存的占用。但它也提供了系统复杂性，需要分离出内部和外部状态。</p><p><strong>扩展问题：</strong></p><ul><li>线程安全问题</li></ul><h2 id="桥梁模式">2.23 桥梁模式：</h2><p><strong>定义：</strong>也叫桥接模式，将抽象和实现解耦，使得两者可以独立地变化</p><p><strong>优点：</strong></p><pre><code>1. 抽象和实现分离2. 优秀的扩充能力3. 实现细节对客户透明</code></pre><p><strong>注意事项：</strong></p><ul><li>考虑如何拆分抽线和实现</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LooC (ICLR2021)</title>
      <link href="post/9f60.html"/>
      <url>post/9f60.html</url>
      
        <content type="html"><![CDATA[<p><strong>WHAT SHOULD NOT BE CONTRASTIVE IN CONTRASTIVELEARNING</strong></p><h1 id="一背景介绍">一、背景介绍：</h1><p>​现在的自监督对比学习方法，大多通过学习对不同的数据增强保持不变，从而学习到一个好的视觉表征。这里就隐含地假设了<strong>一组特定地表示不变性</strong>，(比如颜色不变性)，但是如果下游任务不满足这种假设条件时，(比如区分黄色轿车和红色轿车)，这种方法可能就会对任务起到反作用。</p><p>​<strong>对于数据增强来说，我们在网络中引入的每一种增强都是鼓励网络对该种变换保持不变性。</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031019112.png" alt="image-20211203095927992" style="zoom:80%;" /></p><p>​如上图所示，自监督对比学习依赖于（a）中描述的数据扩充来学习视觉表征。然而，目前的方法通过鼓励神经网络对信息不太敏感来引入感应偏差，可能有帮助也可能有伤害。如（b）所示，旋转不变嵌入对某些花卉类别有帮助，但可能会损害动物识别性能；相反，颜色不变性通常似乎有助于粗粒度的动物分类，但会损害许多花卉类别和鸟类类别。</p><p>​所以本文提出一种对比学习框架，它<strong>不需要特定地、任务相关地不变性的先验知识</strong>，通过构造独立的嵌入空间来学习捕获视觉表示的变化和不变因子，每个嵌入空间对除<strong>一个增广</strong>外的所有增广都是不变的。<strong>目的就是想在对比学习的框架中能够捕获到个体变换的因素。而无需假定下游不变性的先验知识。</strong></p><span id="more"></span><h1 id="二理论方法">二、理论方法：</h1><p>​<strong>对比学习通过最大化数据样本的相似性和相异性来学习表示，这些数据样本分别被组织成相似和相异对。</strong>也就是我们说的正样本对和负样本对。</p><p>​我们一般使用的数据增强策略是通过一个数据增强模块，其中包含了几个数据增强的原子操作，比如随机裁剪，颜色、翻转等。对于一个样本图像<spanclass="math inline">\(I\)</span>，经过数据增强后得到随机视图<spanclass="math inline">\(\widetilde{I}\)</span>,正样本对是对同一模板图进行数据增强得到的，负样本对可以是对不同样本图进行数据增强得到的。然后经过特征提取网络，再经过projectionhead 提取表示 进一步映射到一个特征空间中进行度量。比如InfoNCE损失函数:</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031124733.png" alt="image-20211203112412650" style="zoom:80%;" /></p><p>​该论文研究不同的任务可能适用不同的增强策略，比如，颜色在鸟类的细粒度分类中起着重要作用，再比如，旋转增强对于花和汽车的分类是有差异的，花旋转后可能会比较相似，但是汽车可能就完全不相似了。</p><p>​ 本文提出了一个框架 Leave-one-out Contrastive Learning(<strong>LOOC</strong>)，称为多元强化对比学习框架。<strong>该框架可以有选择地防止因扩充而导致的信息丢失。</strong></p><ul><li>多个 embedding space</li><li>每个embeddingspace对某一个增强<strong>不是</strong>不变的，<strong>而是专注于单个增强</strong></li></ul><p>​</p><p>​ 每个embeddingspace专用于单个增强，共享层将包含增强变化和不变信息，我们通过几个嵌入空间共同学习一个共享表示。我们要么单独将共享表示，要么将所有空间的连接转移到下游任务。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031036688.png"alt="image-20211203103650563" /><figcaption aria-hidden="true">image-20211203103650563</figcaption></figure><p>​ 以翻转和颜色的数据增强来看，首先参考图像<spanclass="math inline">\(I\)</span>被扩充为两个视图q、k。<strong>它们是经过了不同但有规律的数据增强得到的</strong>。视图<spanclass="math inline">\(q\)</span>和<spanclass="math inline">\(K_0\)</span>分别经过两组独立的数据增强操作，或者可以理解为互斥。就是<strong>不同角度的旋转和不同颜色的增强</strong>。<strong>其他的几个视图分别是相对于视图<spanclass="math inline">\(q\)</span>来说只有一处增强不同，其他数据增强操作保持一致</strong>。<spanclass="math inline">\(I_q\)</span>和<spanclass="math inline">\(I_{k_1}\)</span>​经过了相同的旋转角度，但是不同的颜色来增强，<spanclass="math inline">\(I_q\)</span>和<spanclass="math inline">\(I_{k_2}\)</span>经过了相同颜色，但是不同的旋转角度来增强。</p><p>​ 经过数据增强的视图，首先通过特征提取网络<spanclass="math inline">\(f\)</span>转化为特征空间<spanclass="math inline">\(V\)</span>的特征向量，之后经过projection映射到不同的子空间中，称为归一化的embeddingspace <span class="math inline">\(Z\)</span>。在这些子空间中，<spanclass="math inline">\(Z_0\)</span>这个子空间中，一个样本图像经过增强后所有的特征<spanclass="math inline">\(v\)</span>应该被映射到相近的同一区域。而在其他子空间<spanclass="math inline">\(Z_i\)</span>中，只有<spanclass="math inline">\(v^q\)</span>和<spanclass="math inline">\(v^{k_i}\)</span>应该被映射到相近的同一区域。</p><p>​</p><p>​有了上面的方法，我们还需要定义监督网络学习的损失函数，其实和InfoNCE相似：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031550140.png" alt="image-20211203155056018" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031551951.png" alt="image-20211203155126896" style="zoom:80%;" /></p><p>说明：</p><ul><li><span class="math inline">\(E_{i,j}^{\{+,-\}}\)</span> = <spanclass="math inline">\(exp(Z_i^q \cdot Z_i^{k_j^{\{+,-\}}} /\tau)\)</span></li><li>{<span class="math inline">\({z_i}^{k_j^+}\)</span> | <spanclass="math inline">\(\forall j\)</span> <spanclass="math inline">\(\in\)</span> {0,1,2,...,n} and <spanclass="math inline">\(j\)</span> <spanclass="math inline">\(\neq\)</span> <spanclass="math inline">\(i\)</span> }</li><li>对于任一子空间的<spanclass="math inline">\(Z^q\)</span>，我们定义<spanclass="math inline">\(Z^{k^+}\)</span>表示的是<strong>同一图像</strong>经过数据增强得到的对应图，<spanclass="math inline">\(Z^{k^-}\)</span>表示<strong>其他图像</strong>经过数据增强的图。</li><li>总体包括两部分，前面一部分是对所有特征保持不变的子空间<spanclass="math inline">\(Z_0\)</span>, 后面是各个子空间<spanclass="math inline">\(Z_i\)</span></li></ul><h1 id="三实验分析">三、实验分析：</h1><p><strong>实验比较对象：</strong></p><ul><li>MoCo、LooC、LooC++</li><li>LooC： 只是用通过共享学习得到的特征空间V</li><li>LooC++: 将RestNet conv5包括在projection中，并且concatenated了特征空间Z</li></ul><p><strong>实验数据集：</strong></p><ul><li><p><strong>ImageNet-100(IN-100)：</strong> ImageNet 子集100类别</p></li><li><p><strong>iNat-1k：</strong> 一个大规模分类数据集1010种类别</p></li><li><p><strong>CUB-200：</strong>加州理工大学鸟类数据集，200种鸟类的细粒度分类数据集</p></li><li><p><strong>Flowers-102：</strong> VGG Flowers数据集102种类别</p></li></ul><p>测试数据集：</p><ul><li><strong>ObjectNet dataset （ON-13）：</strong>一个收集的测试集，我们只使用与IN-100重叠的13个类别，称为ON-13</li><li><strong>IN-C-100：</strong> ImageNet 100类别</li></ul><p><strong>实验结果：</strong></p><p>​测试在不同旋转角度下的分类准确性，对于MoCo来说，加了旋转增强，降低了它的分类准确性，而对于LooC来说，更好的利用了增强带来的信息增益。<imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031634972.png"alt="image-20211203163423881" /></p><p>​ 测试连接到下游任务之后的性能，LooC++ 得到很好的实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031637798.png" alt="image-20211203163742709"  /></p><p>该组实验表明，旋转增强有利于ON-13，而纹理增强有利于IN-C-100</p><p>在训练时使用了不同的数据增强，在测试时测试不同的增强的各种排列组合的性能，从而间接说明，不同的任务会受不同的数据增强的影响。</p><p><strong>就是说相当于需要做排列组合的测试：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031642436.png" alt="image-20211203164224338" style="zoom:80%;" /></p><p>使用全部数据增强，实验结果;</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031654365.png" alt="image-20211203165454241"  /></p><p>不同的下游任务对依赖于增强的或不变的表示显示出不同的偏好。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031657580.png" alt="image-20211203165705491"  /></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> self-supervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typora数学符号</title>
      <link href="post/9452.html"/>
      <url>post/9452.html</url>
      
        <content type="html"><![CDATA[<h1 id="typora数学符号">Typora数学符号：</h1><p>转载自：https://blog.csdn.net/wait_for_eva/article/details/84307306</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031056420.png" style="zoom:80%;" /></p><span id="more"></span><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031057196.png" alt="" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031057394.png" alt="" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202112031058360.png" alt="" style="zoom:80%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSPU-Net</title>
      <link href="post/7c64.html"/>
      <url>post/7c64.html</url>
      
        <content type="html"><![CDATA[<h1 id="sspu-net">SSPU-Net</h1><p>Accepted by ACM MM 2021</p><h1 id="研究动机">1. 研究动机：</h1><p>​ 传统的点云数据通过基于GroundTruth的有监督的方式实现数据上采样的工作，而本文提出了一种可以基于自监督的方式实现点云数据上采样。</p><p>​为什么要上采样？因为一般采集到的点云数据通常是稀疏的，对于一些局部的几何结构表达不够好，可能会影响下游的处理任务。所以，有必要对稀疏点云进行上采样，以生成稠密完整的点云，从而方便后续的点云处理任务。</p><p><strong>创新点：</strong>、</p><ul><li>NEU(neighbor expansion unit)模块 — 领域扩展单元</li><li>DRU(differentiable point cloud rendering unit)模块 —可微渲染单元</li></ul><span id="more"></span><h1 id="网络结构">2. 网络结构：</h1><h2 id="整体网络结构">2.1 整体网络结构:</h2><p>​首先通过一种邻域扩展单元（NEU）来将稀疏点云向上采样为稠密点云，利用每个点的局部几何结构自适应地学习权重来插值新点的特征。然后通过一个可微渲染单元（DRU），将输入的稀疏点云和生成的密集点云渲染成多视点图像。为了使稀疏点云和稠密点云具有相似的几何结构，我们在点云上构造了<strong>形状一致性损失</strong>，在渲染图像上构造了<strong>图像一致性损失</strong>来训练点云上采样网络。</p><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012153916885.png" alt="image-20211012153916885" style="zoom:67%;" /></p><h2 id="neu单元">2.2 NEU单元：</h2><p>变量说明：</p><p>r： 上采样率</p><p>c： 特征维度，一般点云数据的坐标是三维坐标</p><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012153941074.png" alt="image-20211012153941074" style="zoom:67%;" /></p><p>​ <spanclass="math inline">\(X_i\)</span>代表给定的输入点云数据中的一个点，首先我们选取该点临近的r个点的特征构建出特征图A，同时我们重复时使用该点特征r次构建出特征图B，然后对两个特征图进行concat得到特征图F，然后通过MLP自适应学习权重用于插值。<spanclass="math inline">\(\alpha\)</span>和<spanclass="math inline">\(\beta\)</span>分别表示临近点和中心点的权重，<spanclass="math inline">\(\gamma\)</span>代表激活函数然后通过如下公式计算插值点的坐标信息：</p><p><img src="https://i.loli.net/2021/10/31/XlOyH8Aq4aL7JKb.png" alt="image-20211031163046783"  /></p><p>​ 为了更好的融合特征，还采用了2Dgrid和自注意力机制。最终基于一个点坐标，构建出了r个点的坐标信息。实现稀疏图像生成密集图像的过程。</p><h2 id="dru">2.3 DRU:</h2><p>​通过NEU单元，我们可以获得了在原始输入稀疏数据S的基础上生成的密集图像D，DRU模块的作用就是将稀疏和密集的点云渲染成多视图图像，以计算后续图像的一致性损失。</p><p>​具体做法，首先将虚拟摄像机放置在输入点云周围的不同视点，以重建点云的投影曲面。然后，我们使用软网格渲染方法从投影曲面渲染多视图图像</p><p><img src="https://i.loli.net/2021/10/31/ejIuMHTgyWO4KRd.png" alt="image-20211031184825532" style="zoom:80%;" /></p><p>如何生成不同视角的投影曲面？这里用到了一些数学计算公式</p><p>​ <span class="math inline">\(x = p_i - o\)</span></p><p>​ <span class="math inline">\(s = [1,0,0]\)</span></p><p>​ <span class="math inline">\(v_1 = x * s\)</span></p><p>为了计算<span class="math inline">\(v_2\)</span>，<spanclass="math inline">\(v_3\)</span>，需要先计算一个辅助向量<spanclass="math inline">\(\widehat{v}\)</span> 垂直于<spanclass="math inline">\(x\)</span>和<spanclass="math inline">\(s\)</span>: <spanclass="math inline">\(\widehat{v} = x * s\)</span></p><p>然后通过下面一组公式即可算出<spanclass="math inline">\(v_2\)</span>,<spanclass="math inline">\(v_3\)</span>同理</p><p><img src="https://i.loli.net/2021/10/31/haYmZgi4UtPOwzs.png" alt="image-20211031165454467" style="zoom:80%;" /></p><p>通过向量计算，得到当前点的局部切线三角形平面，再通过网格渲染得到最终的图像</p><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012155116286.png" alt="image-20211012155116286" style="zoom:80%;" /></p><h2 id="loss">2.4 Loss：</h2><p><strong>定义：</strong></p><p>输入的稀疏点云数据为Ｓ</p><p>生成的密集点云数据为Ｄ</p><p>密集点云数据下采样得到的数据为Ｄ‘</p><h3 id="shape-consistent-loss">2.4.1 Shape-consistent loss：</h3><p>​ 该loss处理的对象是原输入的稀疏点云数据S和经过NEU单元构建的密集点云数据D，具体的设计思想是，选择一种下采样方案（例如论文提到的FPS），对NEU构建的密集点云数据进行下采样，得到新的稀疏点云数据D',然后通过最小化D和D'来训练学习过程。</p><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012160433951.png" alt="image-20211012160433951" style="zoom:80%;" /></p><h3 id="image-consistent-loss">2.4.2 Image-consistent loss：</h3><p>​对于稀疏的点云数据Ｓ和密集的点云数据Ｄ分别经过ＤＲＵ单元可生成不同的渲染图像,显然D的数据密集,会有更多的点,因此也会生成更多的三角形曲面.但是论文提到经过实验验证,该差异对上采样几乎没有影响,所以损失的定义还是鼓励两个稀疏和密集的点云数据尽可能一致.</p><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012161459634.png" alt="image-20211012161459634" style="zoom:80%;" /></p><h1 id="实验部分">3. 实验部分：</h1><h2 id="数据集">3.1 数据集:</h2><p>​基于PU-Net、MPU、PU-GAN和FAUST中的部分人体模型来构建一个大型数据集，称为SSPU数据集，包括227个不同类别的3D模型,随机选择191个模型作为训练集，其余的模型作为测试集。</p><p>epoch: 30</p><p>batch_size: 28</p><p>k[NEU] : 4</p><h2 id="实验结果">3.2 实验结果:</h2><ol type="1"><li><p>与目前一些常用的基于有监督的网络结构进行比较，输入数据有两类，一类是2048个点数据，另一类是4096个点数据。可以看出SSPU-Net与目前的一些自监督的方法取得了差不多的效果。这里的CDHD P2F等是不同的距离度量方式。</p><p>不过和PU-Net相比的话，整体都是由于PU-Net的</p></li></ol><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012162801253.png" alt="image-20211012162801253"  /></p><ol start="2" type="1"><li>Table2是在PU-GAN数据集上进行的测试,整体结果还是优于PU</li></ol><p><img src="https://gcore.jsdelivr.nett/gh/IYoreI/PicGo@main/img/image-20211012162828067.png" alt="image-20211012162828067"  /></p><ol start="3" type="1"><li>下面这幅图是一个可视化的结果,展示了不同网络之间上采样的效果.</li></ol><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012163238185.png" alt="image-20211012163238185" style="zoom:67%;" /></p><ol start="4" type="1"><li>消融实验:</li></ol><p><img src="https://gcore.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20211012163506011.png" alt="image-20211012163506011"  /></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> self-supervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Siam-RPN</title>
      <link href="post/1e0d.html"/>
      <url>post/1e0d.html</url>
      
        <content type="html"><![CDATA[<h1 id="siam-rpn">Siam-RPN</h1><h1 id="研究动机">1. 研究动机：</h1><p>在Siam-FC中，作者提出的算法首次将孪生网络引入了目标跟踪的领域，实现了端到端的训练，它是一个兼容了速度和精度的算法，在3个尺度变换和5个尺度变换的条件下，跟踪的速率分别达到了86fps和58 fps。</p><p>Siam-FC需要多尺度测试，在跟踪阶段分别生成不同尺度的搜索框，进行跟踪，选取得分结果最好的作为跟踪结果。</p><p>Siam-FC没有做回归去调整候选框的位置。</p><p>本篇论文提出的Siam-RPN可以说是对Siam-FC的改进，在速度和精度上都有提升。速度上达到了160FPS，而RPN子网络进一步提升了对目标框预测的准确度。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108271557609.png" alt="image-20210827155728388" style="zoom:80%;" /></p><span id="more"></span><h1 id="网络结构">2. 网络结构：</h1><h2 id="rpn网络">2.1 RPN网络：</h2><p>首先看一下RPN网络，RPN网络是在Faster-RCNN中提出的代替了ss算法用来<strong>提取目标候选框</strong>的。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108281103259.png" alt="image-20210828110330143" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108281629070.png" alt="image-20210828162943959" style="zoom:80%;" /></p><p>上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条分支，上面一条通过softmax分类anchors获得positive和negative分类，下面一条分支用于计算对于anchors的boundingboxregression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positiveanchors和对应bounding boxregression偏移量获取proposals，同时剔除太小和超出边界的proposals。</p><p>具体的实现： 对于前面一个步骤通过特征提取网络得到的特征图，采用3 *3大小的卷积核，同时padding=1来进行卷积操作。</p><p><strong>如何选定Anchor？</strong></p><p>对于特征图上的每一个点，我们可以计算出对应于原图的位置，然后生成9种anchor（这些anchor分别代表不同的大小和比例，大小有：[128* 128,256 * 256, 512 * 512],比例有：[1 : 1、1 : 2、2 :1]）。然后我们可以根据anchor和ground_truth的IoU值来给anchor打上正负样本的标签。注意，这里生成的anchor特别多，在实际训练的时候一般是随机选取指定数量的anchor，比如：选取256个anchor，其中正负样本比例为1: 1。</p><p><strong>如何计算特征图中某一点和原图的位置对应关系？</strong></p><p>特征图中的某一个点，对应的就是原图中某一个区域，也就是我们说的感受野大小，如果我们使用3*3的卷积核，同时padding=1来进行卷积操作的话，那么得到的特征图和原图是一样大小的，也就是说卷积操作不改变输入和输出的大小，只有在polling层会成倍减小特征图的输出。所以我们可以根据当前点在特征图中的位置，扩大相应的倍数来还原在原图中的位置。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108281445406.png" alt="image-20210828144354316" style="zoom:80%;" /></p><p><strong>如何定义正负样本？</strong></p><p>正样本：</p><ul><li>与某一真实框IoU最大的Anchor</li><li>与任意真实框IoU&gt;0.7的Anchor</li></ul><p>负样本：</p><ul><li>与所有真实框的IoU&lt;0.3的Anchor</li></ul><p><strong>分类分支和回归分支的计算过程？</strong></p><p>上面的介绍中没有提到我们特征图的深度，这里的特征图是通过前一步骤的特征提取网络得到的，所以采用不同的网络会得到不同深度的特征图，在Faster-RCNN中，这里的维度是256维。我们在特征图上做3*3的卷积时保持通道数不变，所以每一次卷积会得到一个256维的向量，这里每一个特征256-d都对应于原图中的9个预测的anchor。然后在两个分支上分别做1*1的卷积得到预测结果。对于每一个Anchor，分类分支给出两个结果（获得2个score，正类是物体，负类是背景）、回归分支给出四个结果（获得4的值x,y,w,h）</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108281509947.png" alt="image-20210828150942859" style="zoom:80%;" /></p><h2 id="siam-rpn-1">2.2 Siam-RPN:</h2><p>Siam-RPN整体的网络结构分为两部分，一部分是SiameseNetwork,另一部分是Region Proposal Network,前者用来提取特征，而RPN子网络又由两个分支组成，一个是用来区分目标和背景的<strong>分类分支</strong>，另外一个是微调候选区域的<strong>回归分支</strong>:</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108271605211.png" alt="image-20210827160538091" style="zoom:80%;" /></p><p>对于Siam-RPN网络来说，首先第一步，经过Siam-FC孪生网络完成模板图像和搜索图像的特征提取工作，然后，对于模板图像z的特征，分别经过3*3大小的卷积操作，同时需要提升通道数。对于检测分支，也是相同的卷积操作，但是保持通道数不变，然后借鉴Siam-FC，采用相似的互相关运算得到分类分支和回归分支的预测结果。</p><p>【参考： https://zhuanlan.zhihu.com/p/31426458】</p><p>分类分支，每一个Anchor有两个输出，分别代表预测的结果是前景和背景的概率。</p><p>回归分支，每一个Anchor有四个输出，分别代表预测的Anchor与GroundTruth之间的偏移量。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108301102846.jpeg"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>对于窗口一般使用四维向量 (x,y,w,h)表示，分别表示窗口的中心点坐标和宽高。红色的框A代表原始的positiveAnchors，绿色的框G代表目标的GT，<strong>我们的目标是寻找一种关系，使得输入原始的anchorA经过映射得到一个跟真实窗口G更接近的回归窗口G'</strong></p><p>如何寻找这个关系呢？ 简单的思路就是通过平移和缩放来实现：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108301119837.png" alt="image-20210830111909752" style="zoom:80%;" /></p><p>所以回归分支的目的就是学习这四种变换的参数，让预测框更好的逼近真实值。</p><h2 id="loss">2.3 Loss：</h2><p>Siam-RPN的loss定义包括两部分，类似于Faster-RCNN： <spanclass="math display">\[loss = L_{cls} + \lambda L_{reg}\]</span> <strong>分类分支</strong>采用交叉熵损失函数</p><ul><li><p>二分类</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108301133652.png" alt="image-20210830113336587" style="zoom:80%;" /><span class="math display">\[-y_i 表示样本i的label，正类为1，父类为 \\-p_i 表示样本i预测为正类的概率\]</span></p></li><li><p>多分类</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108301133433.png" alt="image-20210830113353387" style="zoom:80%;" /><span class="math display">\[- M 类别的数量 \\- y_{ic} 符号函数(0或1)，如果样本i的真实类别等于c取1，否则取0 \\- P_{ic} 观测样本i属于类别c的预测概率\]</span></p></li></ul><p><strong>回归分支</strong>采用Smooth L1 损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108301125820.png" alt="image-20210830112508723" style="zoom:80%;" /></p><h1 id="训练阶段">3. 训练阶段：</h1><h2 id="anchor选定">3.1 Anchor选定：</h2><p>选定5种不同比例的anchor：[0.33,0.5,1,2,3]</p><h2 id="iou阈值设置">3.2 IoU阈值设置：</h2><p>阈值上限： 0.6</p><p>阈值下限： 0.3</p><h2 id="正负样本选定">3.3 正负样本选定：</h2><p>在Siam-RPN中，一个训练队包含64个样本，其中最多16个正样本。</p><h1 id="实验结果">4. 实验结果：</h1><h2 id="vot-2015数据集">4.1 VOT 2015数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108272125426.png" alt="image-20210827212531333" style="zoom: 67%;" /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108272125462.png" alt="image-20210827212544391" style="zoom: 80%;" /></p><h2 id="vot-2016数据集">4.2 VOT 2016数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202108272126870.png" alt="image-20210827212653781" style="zoom:67%;" /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202110072124500.png" alt="image-20210827212703707" style="zoom: 80%;" /></p><h2 id="vot-2017数据集">4.3 VOT 2017数据集：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/202110072124247.png" alt="image-20210827212749852" style="zoom:80%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SiamFC</title>
      <link href="post/8fcc.html"/>
      <url>post/8fcc.html</url>
      
        <content type="html"><![CDATA[<h1 id="一背景介绍">一、背景介绍：</h1><p><strong>[参考资料]</strong>:</p><ol type="1"><li>https://zhuanlan.zhihu.com/p/148516834</li><li>https://www.jiqizhixin.com/articles/2017-05-14</li></ol><p>目标跟踪的方法一般分为两种模式：<strong>生成式模型</strong>和<strong>鉴别式模型</strong></p><p><font color=red><strong>生成式模型：</strong></font></p><p>​ 早期的一些目标跟踪算法都属于是生成式模型跟踪算法的研究，比如：Meanshift、ParticleFilter(粒子滤波)、基于特征点的光流法等等。<strong>此类方法首先建立目标模型或者提取目标特征,在后续帧中进行相似特征搜索，逐步迭代实现目标定位。</strong>比如Meanshift方法是一种基于概率密度分布的跟踪方法，首先会对目标建模，例如利用目标的颜色分布来描述目标，然后计算目标在下一帧图像上的概率分布，从而迭代得到局部最密集的区域。Meanshift适用于目标的色彩模型和背景差异比较大的情形。</p><p>​ 但是这类方法也存在明显的缺点,就是图像的背景信息没有得到全面的利用.且目标本身的外观变化有随机性和多样性特点,因此,<strong>通过单一的数学模型描述待跟踪目标具有很大的局限性</strong>.具体表现为在光照变化,运动模糊, 分辨率低, 目标旋转形变等情况下, 模型的建立会受到巨大的影响,从而影响跟踪的准确性; 模型的建立没有有效地预测机制,当出现目标遮挡情况时, 不能够很好地解决。</p><p><font color=red><strong>鉴别式模型：</strong></font></p><p>​ 鉴别式模型是指<strong>将目标模型和背景信息同时考虑在内,通过对比目标模型和背景信息的差异, 将目标模型提取出来,从而得到当前帧中的目标位置。</strong>也有文中指出该模式是利用分类来做跟踪的方法。即把跟踪的目标作为前景，利用在线学习或离线训练的检测器来区分前景目标和背景，从而得到前景目标的位置。</p><p>​ <strong>相关滤波</strong>的跟踪算法始于 2012 年 P.Martins 提出的 CSK方法，作者提出了一种基于循环矩阵的核跟踪方法，并且从数学上完美解决了<strong>密集采样</strong>（DenseSampling）的问题，<strong>利用傅立叶变换快速实现了检测的过程</strong>。在训练分类器时，一般认为离目标位置较近的是正样本，而离目标较远的认为是负样本。相关滤波系列的方法发展很快，比如CSK 作者引用的 MOSSE 方法，后续他又提出了基于 HOG 特征的 KCF方法。后续还有考虑多尺度或颜色特征（Color Name表）的方法以及用深度学习提取的特征结合 KCF 的方法（比如 DeepSRDCF方法）等。从它的发展过程来看，考虑的尺度越来越多，特征信息也更加丰富，当然计算时间也会相应增加，但总体上说，相关滤波系列的跟踪方法在实时性上优势明显，采用哪种改进版本的方法视具体的应用而定。相关滤波的方法也有一些<strong>缺陷</strong>，比如目标的快速移动，形状变化大导致更多背景被学习进来等都会对CF系列方法造成影响。虽然后续的研究也有一些针对性的改进，比如改进边界效应，改善背景更新策略或提高峰值响应图的置信度等，<strong>但普适性还需要进一步研究，特别是对不同的应用针对性地调整</strong>。</p><p>​基于深度学习的方法，在大数据背景下，利用深度学习训练网络模型，<strong>得到的卷积特征输出表达能力更强</strong>；深度学习还有一大优势就是<strong>端到端的输出</strong>。</p><p><strong>【总结】</strong>：目标跟踪的方法经历了从<strong>经典跟踪算法</strong>-&gt; <strong>基于核相关滤波的跟踪算法</strong> -&gt;<strong>基于深度学习的跟踪算法</strong></p><span id="more"></span><p>​传统目标跟踪的问题是通过学习对象的外观模型来解决的，只使用视频本身作为的训练数据。效果不错，但它们只用在线的方法本质上限制了它们所能学习的模型的丰富性。随着深度学习的在计算机视觉中的应用，大家也想尝试用深度卷积网络来解决目标跟踪问题，但是我们一般会通过一个大型监督数据集来训练一个网络，所以缺乏这种大规模的数据集和另一个实时性的要求就成为了一个限制。这对上面提到的限制，在这篇论文提出的时候已经有下面的一些相关的工作了：</p><ul><li>把深度网络当作一个特征提取器，然后应用到相关滤波的跟踪框架中。</li><li>针对特定目标，采用在线SGD微调网络，用于当前目标跟踪任务。</li></ul><p>但是这两种方法都有不足，第一种没有充分利用端到端的学习优势，第二种速度慢。作者在本篇论文中提出了全卷积孪生网络，在初始离线阶段，深度卷积网络被训练于应对更加一般的相似性学习问题，然后在跟踪期间简单地在线评估该函数。</p><h1 id="二网络结构">二、网络结构:</h1><p>如何实现目标跟踪？我们可以用相似性学习来解决这个问题。相似性的度量有很多类，比如距离、相关系数、角度(余弦相似度)等等。这篇论文使用的卷积运算，后面会介绍。我们学习一个函数<spanclass="math display">\[f(z,x)\]</span>​，该函数将样本图像z和候选图像x进行比较，如果两个图像描述的内容很相近，就获得高分，否则获得低分。</p><p>具体到这篇论文是如何实现的呢？在训练阶段，我们会有两个不同的输入，输入z代表样本图像，输入x代表搜索图像，这两个输入的尺寸是不一样的，是从一个视频段的每一帧图像中随机选取一帧作为样本图像，然后在距离此帧图片不超过100帧的范围内随机选取另一帧是搜索图，然后对这两张图片进行裁剪以及平均颜色填充，等比例缩放等操作得到不同尺度的图片对。在这个过程中，<strong>目标所在位置始终都是图像的中心位置</strong>。经过卷积神经网络进行特征提取，也就是这里的<spanclass="math display">\[\varphi\]</span>所代表的函数的作用。对于所提取到的特征图进行相似性计算，相似性计算有很多种方式，这里的相似性计算采用的是卷积的操作，最终得到一个相似性得分。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718104120152.png" alt="image-20210718104120152" style="zoom:80%;" /></p><p>论文提出的全卷积孪生网络结构，它可以实现端到端的训练，上面提到的<spanclass="math display">\[\varphi\]</span>就是一个神经网络，卷积神经网络的具体构造是在AlexNet的基础上做的调整，比如说没有了padding操作，它采用了五层的卷积操作，前两层有最大池化，激活函数采用RELU等等。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718104138856.png" alt="image-20210718104138856" style="zoom:80%;" /></p><p>卷积的理解：（参考：https://www.cnblogs.com/shine-lee/p/9932226.html）</p><p>​ 卷积和相关：</p><p>​相关是将滤波器在图像上滑动，对应位置相乘求和；卷积则先将滤波器旋转180度（行列均对称翻转），然后使用旋转后的滤波器进行相关运算。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718150141669.png" alt="image-20210718150141669" style="zoom: 80%;" /></p><ul><li><p><strong>从函数拟合的角度：</strong></p><p>​ <span class="math display">\[y = \delta(wx+b)\]</span></p><p>​从这个公式来理解，每一次卷积操作，我们都是在对函数的自变量做映射关系(可以是线性的或者是非线性的，取决于激活函数的使用)，不断嵌套，不断构建一个复杂函数。我们的目的就是尽可能的找到一个复杂的函数，能够拟合所有的训练数据和真实值之间的映射关系。</p></li><li><p><strong>从模板匹配的角度：</strong></p><p>​我们说卷积和提取特征，卷积核定义了某种模式，卷积（相关）运算是在计算每个位置与该模式的相似程度，或者说每个位置具有该模式的分量有多少，当前位置与该模式越像，响应越强。也就代表了两者之间的相似程度。</p></li></ul><p>在训练过程中如何计算LOSS呢？</p><p>由于我们的图片对中的目标都是在中心位置，所以 如果我们得到的scoremap中得分高的点大多落在以中心位置为圆心，固定长度为半径的圆内，则认为是正样本，否则为负样本。损失函数采用二分类交叉熵损失函数计算。下图分别是真实值和权重值的示例图。</p><p>损失函数公式： <span class="math display">\[Loss(X_i,y_i) = -w_i(y_ilogx_i+(1-y_i)log(1-x_i))\]</span></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718135724529.png" alt="image-20210718135724529" style="zoom: 80%;" /></p><h1 id="三数据处理">三、数据处理：</h1><h2 id="训练数据">3.1 训练数据：</h2><p>​首先我们的训练数据集是很多段视频中包含的一帧一帧的图片，每一张图片都有对应的标签数据，标签文件中包含了该帧图像中所有的目标的位置信息，例如下图展示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210722163148438.png" alt="image-20210722163148438" style="zoom:80%;" /></p><h2 id="数据预处理">3.2 数据预处理：</h2><p>​接下来我们需要根据上面每一帧的图像生成图像对，作为模型的输入数据，分别为样本图像z，尺度为127* 127，搜索样本x，尺度为225 * 225。下图就是实际使用的图片对：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718135155121.png" alt="image-20210718135155121"  /></p><p>​生成过程是首先原始图像的目标框的中心点，计算目标框的w,h。然后通过计算(w+h)/2获得扩展的每个方向(w和h)的扩展量，将两者相乘，再开方，大致就可以得到图像最终的缩放大小，然后基于原图进行裁剪，不够的地方采用平均RGB像素进行填充，最后将图像缩放到指定的尺度。例如下面是获取搜索图像的整体过程，主要涉及了一些数学变换：</p><div class="sourceCode" id="cb1"><preclass="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_instance_image(img, bbox, size_z, size_x, context_amount, img_mean<span class="op">=</span><span class="va">None</span>):</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    cx, cy, w, h <span class="op">=</span> xyxy2cxcywh(bbox)</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    wc_z <span class="op">=</span> w <span class="op">+</span> context_amount <span class="op">*</span> (w<span class="op">+</span>h)</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    hc_z <span class="op">=</span> h <span class="op">+</span> context_amount <span class="op">*</span> (w<span class="op">+</span>h)</span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    s_z <span class="op">=</span> np.sqrt(wc_z <span class="op">*</span> hc_z)</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    scale_z <span class="op">=</span> size_z <span class="op">/</span> s_z</span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    d_search <span class="op">=</span> (size_x <span class="op">-</span> size_z) <span class="op">/</span> <span class="dv">2</span></span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    pad <span class="op">=</span> d_search <span class="op">/</span> scale_z</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    s_x <span class="op">=</span> s_z <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> pad</span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    scale_x <span class="op">=</span> size_x <span class="op">/</span> s_x</span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    instance_img <span class="op">=</span> crop_and_pad(img, cx, cy, size_x, s_x, img_mean)</span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> instance_img, scale_x, s_x</span></code></pre></div><div class="sourceCode" id="cb2"><preclass="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> crop_and_pad(img, cx, cy, model_sz, original_sz, img_mean<span class="op">=</span><span class="va">None</span>):</span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    xmin <span class="op">=</span> cx <span class="op">-</span> original_sz <span class="op">//</span> <span class="dv">2</span></span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    xmax <span class="op">=</span> cx <span class="op">+</span> original_sz <span class="op">//</span> <span class="dv">2</span></span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    ymin <span class="op">=</span> cy <span class="op">-</span> original_sz <span class="op">//</span> <span class="dv">2</span></span><span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ymax <span class="op">=</span> cy <span class="op">+</span> original_sz <span class="op">//</span> <span class="dv">2</span></span><span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    im_h, im_w, _ <span class="op">=</span> img.shape</span><span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span><span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    left <span class="op">=</span> right <span class="op">=</span> top <span class="op">=</span> bottom <span class="op">=</span> <span class="dv">0</span></span><span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> xmin <span class="op">&lt;</span> <span class="dv">0</span>:</span><span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        left <span class="op">=</span> <span class="bu">int</span>(<span class="bu">abs</span>(xmin))</span><span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> xmax <span class="op">&gt;</span> im_w:</span><span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        right <span class="op">=</span> <span class="bu">int</span>(xmax <span class="op">-</span> im_w)</span><span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ymin <span class="op">&lt;</span> <span class="dv">0</span>:</span><span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        top <span class="op">=</span> <span class="bu">int</span>(<span class="bu">abs</span>(ymin))</span><span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ymax <span class="op">&gt;</span> im_h:</span><span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        bottom <span class="op">=</span> <span class="bu">int</span>(ymax <span class="op">-</span> im_h)</span><span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span><span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    xmin <span class="op">=</span> <span class="bu">int</span>(<span class="bu">max</span>(<span class="dv">0</span>, xmin))</span><span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    xmax <span class="op">=</span> <span class="bu">int</span>(<span class="bu">min</span>(im_w, xmax))</span><span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    ymin <span class="op">=</span> <span class="bu">int</span>(<span class="bu">max</span>(<span class="dv">0</span>, ymin))</span><span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    ymax <span class="op">=</span> <span class="bu">int</span>(<span class="bu">min</span>(im_h, ymax))</span><span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    im_patch <span class="op">=</span> img[ymin:ymax, xmin:xmax]</span><span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> left <span class="op">!=</span> <span class="dv">0</span> <span class="kw">or</span> right <span class="op">!=</span><span class="dv">0</span> <span class="kw">or</span> top<span class="op">!=</span><span class="dv">0</span> <span class="kw">or</span> bottom<span class="op">!=</span><span class="dv">0</span>:</span><span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> img_mean <span class="kw">is</span> <span class="va">None</span>:</span><span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            img_mean <span class="op">=</span> <span class="bu">tuple</span>(<span class="bu">map</span>(<span class="bu">int</span>, img.mean(axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))))</span><span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        im_patch <span class="op">=</span> cv2.copyMakeBorder(im_patch, top, bottom, left, right,</span><span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                cv2.BORDER_CONSTANT, value<span class="op">=</span>img_mean)</span><span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model_sz <span class="op">!=</span> original_sz:</span><span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        im_patch <span class="op">=</span> cv2.resize(im_patch, (model_sz, model_sz))</span><span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> im_patch</span></code></pre></div><h2 id="跟踪过程">3.3 跟踪过程：</h2><p>​有了训练数据，有了loss的定义，我们就可以完成模型的训练了，那下一步我们如何实现跟踪的过程呢？因为按照网络的输出来看是一个scoremap，我们需要根据它计算目标在每一帧图像中的位置。</p><p>首先，我们知道第一帧图象是给定了目标框的，之后的每一帧跟踪的过程都依赖于上一帧的位置。跟踪阶段对于每一帧图片，都进行了三次大小不同的裁剪，分别计算scoremap ,取得分值最大的</p><p>​<img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210722195551679.png" alt="image-20210722195551679" style="zoom:80%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210722195910683.png" alt="image-20210722195910683" style="zoom:80%;" /></p><h1 id="四实验结果">四、实验结果：</h1><p>下图是OTB-13上，不同模型在OPE(一次性通过评估)、TRE(时间鲁棒性评估)和SRE(空间鲁棒性评估)三个指标上的比较。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718140950365.png" alt="image-20210718140950365" style="zoom:80%;" /></p><p>下图是在VOT-14上，各个模型在鲁棒性和准确度的上比较：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718141001859.png" alt="image-20210718141001859" style="zoom:80%;" /></p><p>下图是VOT-15上，最好的40个结果和该论文的模型的基于平均重叠的排名</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210718141009593.png" alt="image-20210718141009593" style="zoom:80%;" /></p><p>性能评估：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/%E6%8D%95%E8%8E%B7.PNG" alt="捕获" style="zoom:80%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ViT模型训练实验</title>
      <link href="post/ea5a.html"/>
      <url>post/ea5a.html</url>
      
        <content type="html"><![CDATA[<h1 id="一背景介绍">一、背景介绍：</h1><p>自2017年Google提出的Transformer结构以来，迅速引发一波热潮，最初《Attentionis all youneed》这篇论文的提出是针对于NLP领域的，通过自注意力机制代替传统处理序列数据时采用的循环神经网络结构，不仅实现了并行训练，提升了训练的效率，同时也在应用中取得很好的结果。之后的一段时间中，各种基于Transformer改进的网络结构涌现出来，在不同领域中都达到SOTA的效果。</p><p>2020年Google又提出了《AN IMAGE IS WORTH 16X16 WORDS : TRANSFORMERSFOR IMAGE RECOGNITION AT SCALE》这篇论文，该文章已经被收录于ICLR2021。首次提出VisionTransformer(ViT)将Transformer结构应用在了CV领域图像分类中，论文中表明，与当前效果最好的卷积神经网络结构相比，ViT仍然取得很好的成绩，同时需要更少的计算资源。</p><p>本次实验内容是复现ViT模型，并将该模型应用在CIFAR10数据集上进行实验，与原论文中的实验结果做比较和分析。</p><span id="more"></span><h1 id="二基本思想">二、基本思想：</h1><h2 id="整体结构">2.1 整体结构：</h2><p>借鉴于2017年Google提出Transformer的思想，ViT的作者希望能够把最纯净的Transformer结构应用在图像分类中，但是Transformer最初提出是针对NLP领域中的机器翻译任务，所以作者对Transformer结构做了一些细小的改动，让它完成图像分类任务。</p><p>改动的地方有：</p><p>（1）传统Transformer结构是由Encoder-Decoder框架组成，而对于ViT来说，只使用了Encoder部分。</p><p>（2）标准Transformer的输入是一维序列数据，所以需要将图像转换为序列数据，论文作者提出的思路就是将一张图片无重叠切分成固定大小的patches，然后将每一个patch通过拉伸操作转换为一维向量，最后通过一个线性变换层将输入的patches转换为一个固定长度的向量，称为patch_embedding。</p><p>（3）因为对于分类任务，最后的输出应该是一个标签，所以作者对TransformerEncoder的输入做了调整, 在输入序列的最开始位置添加了一个CLS Token。</p><p>整体网络结构如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715153850276.png" alt="image-20210715153850276" style="zoom:150%;" /></p><h2 id="模块分析">2.2 模块分析：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715162318118.png" alt="image-20210715162318118"  /></p><h3 id="patch-embedding">2.2.1 Patch Embedding:</h3><p>Transformer在NLP领域应用时，处理的都是词向量序列，所以在处理二维图像时，我们需要对图像做一些特殊处理，在原论文中，作者提出的方式是将一张图片切分成大小相同的图像块，比如16*16，并将每一个patch映射到固定维度的向量embed_dim=768，该向量的维度在整个计算过程中保持不变。这一块的操作是通过一个二维的卷积来完成的，卷积核大小设置为16*16，步长为16。</p><p>将图片进行切分之后，我们还需要对每个patches添加不同的位置信息PosEmbedding，这里的位置信息如何获得，在《Attention is all youneed》这篇论文中，作者采用sin cos的方式计算得来。</p><p>将Patch Embedding和PosEmbedding进行相加就可以得到了最终可以送入Transformer结构的输入信息，也就是把数据传给了多头自注意力层。</p><p>通过jupyter notebook对patchEmbedding的操作进行模拟，查看张量在处理过程中的维度变化：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715153923915.png" alt="image-20210715153923915"  /></p><h3 id="multi-head-attention">2.2.2 Multi-Head Attention:</h3><p>关于注意力机制这一模块，论文中采用与《Attention is all youneed》中完全一样的结构。需要注意的是，在ViT中不同规模的模型，它们在网络结构设计上存在着一些微小的差别，比如对于本次实验采用的vit_base_patch16_224预训练模型来说，它的整个网络结构堆叠了12层，并且多头注意力的head数设置为12。其他模型如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715153935876.png" alt="image-20210715153935876"  /></p><p>在这一个模块中，计算过程与Self-Attention中的计算过程一样，对于输入的数据，我们得到QKV三个矩阵，然后通过scaledDot-Product和softmax计算出不同序列（这里也就是不同patch）之间的相关性得分，最后得到输出序列，输出序列与输入保持同一个维度，由于这里的layers设置为12，所以输入序列数据需要经过12个Attention之后才会得到输出结果。</p><h3 id="mlp">2.2.3: MLP:</h3><p>在TransformerEncoder层中，MLP是包含一个隐藏层的全连接网络，隐藏层的节点数由参数mlp_ratio来控制。而MLP的输出和输入的维度保持一致，均为768。同时在Encoder层中的MLP采用的激活函数是GELU。</p><p>在整个网络的最后还有一个MLPHead模块，该模块是用来实现分类的，这个分类头在预训练阶段是带有一个隐藏层的MLP，在微调阶段改为由单个线性层实现。由于对于数据集CIFAR10来说，类别数是10，所以这里MLPHead的作用就是完成维度由768到10的线性变换，实现分类，注意这里计算时采用了log_softmax。</p><h1 id="三实验过程">三、实验过程：</h1><h2 id="实验说明">3.1. 实验说明：</h2><p>论文中指出，在中小规模的数据集上训练时，Transformer的表现并没有预期的好，因为相比于卷积神经网络来说，Transformer并没有体现出局部性和平移不变性，所以在数据量不足的情况下很难有很好的泛化能力。</p><p>基于上述问题，作者在论文中将ViT模型分别在ImageNet、ImageNet-21k和未开源的Google内部的数据集JFT-300M这三个大型数据集上做了预训练之后，ViT模型已经接近或者超过许多图像识别的基准水平了。</p><p>本次实验是采用的vit_base_patch16_224预训练模型，然后在CIFAR10数据集上进行微调。实验过程中参考了以下两个开源实现库：</p><ol type="a"><li>Google官方公布的代码：、</li></ol><blockquote><p><a href="https://github.com/google-research/vision_transformer"class="uri">https://github.com/google-research/vision_transformer</a></p></blockquote><ol start="2" type="a"><li>第三方开源实现：</li></ol><blockquote><p><a href="https://github.com/rwightman/pytorch-image-models"class="uri">https://github.com/rwightman/pytorch-image-models</a></p></blockquote><h2 id="实验环境">3.2. 实验环境：</h2><p>CUDA: 11.3、cudnn: 8.2、Miniconda 3、python: 3.9.5、pytorch:1.8.1</p><p>GPU： NVIDIA Quadro RTX 4000</p><h2 id="实验步骤">3.3. 实验步骤：</h2><h3 id="网络结构实现">3.3.1 网络结构实现：</h3><p>论文中的模型共包含以下几个结构模块：</p><ol type="1"><li><p>Patch_Embeding: 实现将图片转换为1D token (dim = 768)</p></li><li><p>Attention: 注意力机制模块</p></li><li><p>MLP：多层感知器模块</p></li><li><p>MLP Head：分类</p></li></ol><p>由于论文官方公布的代码是基于TensorFlow实现的，而且需要安装Jax库，但是在windows下对Jax的支持不太友好，所以本次实验采用的是pytorch实现的，关于各个模块的实现参考了timm开源库的代码实现。</p><h3 id="数据集预处理">3.3.2 数据集预处理：</h3><p>CIFAR10 数据集下载地址：<ahref="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"class="uri">https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a></p><p>由于下载的数据集是二进制文件，一共包含了五个训练数据文件和一个验证数据文件，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715153957235.png" alt="image-20210715153957235"  /></p><p>所以需要手动对数据集进行解压，转换为32*32标准像素的图片，并且分别存储为train和val文件夹中，以备使用。数据集共包含50000张训练图片，10000张测试图片，共10个类别。解压后的图片如下：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154003632.png" alt="image-20210715154003632"  /></p><p>这里的文件名由类别和文件标号组成，由下划线分割，在加载数据集的时候也是通过文件名来获取到图片的标签信息。</p><h3 id="预训练模型加载">3.3.3 预训练模型加载：</h3><p>本次实验使用的预训练模型为ViT-B_Patch16_224，该模型指定patchsize为16，输入的图片大小为226*224，并且在ImageNet上完成了模型的预训练过程。在实验中，是通过timm库提供的预训练模型加载接口实现了模型的加载：</p><p>model = timm.create_model('vit_base_patch16_224',pretrained=True)</p><h3 id="训练过程">3.3.4 训练过程：</h3><p>Timm库提供了通用的训练示例代码，但是对于ViT的训练来说，无法直接使用，还需要对其中的部分过程进行修改，以满足ViT的需求，所以本次实验的训练过程，是通过修改Timm库提供的训练代码完成对ViT模型的微调过程。整个训练的代码实现包含以下几个方面：</p><ol type="1"><li><strong>通过配置文件或者命令行的方式定义各种超参数</strong>。</li></ol><p>经过对Timm库的源码的阅读，发现Timm库对于超参数的解析提供了便捷的、完备的配置入口，所以对于本次实验的需要定义的超参数，基于了Timm的提供的参数解析方法，通过一个config.yml文件统一配置了模型需要的一些超参数。如下图所示：</p><p>（比如，一些基本的配置：数据集地址，预训练模型、分类任务的类别数，学习率等等）。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154016861.png" alt="image-20210715154016861"  /></p><ol start="2" type="1"><li><strong>加载数据集：</strong></li></ol><p>关于数据集的加载，一开始采用Timm库提供的data_loader接口并未发现问题，但是在训练完一个epoch后，发现验证数据集的Top1准确率竟然达到了100%，完全超出了论文中的最好效果，带着怀疑的态度阅读了data_loader的具体实现源码，并且调试了加载数据集的过程才发现它并不能支持本次实验的数据集正常加载，读取到的标签均为同一个类别。</p><p>所以在实验过程中，对data_loader进行了修改，弃用原有的标签读取方式，重写提取标签的过程，通过文件名来获取标签信息，并且和对应的图片绑定起来。</p><ol start="3" type="1"><li><strong>加载模型：</strong></li></ol><p>由于Timm库本身对ViT的各种预训练模型都进行了收集和封装，所以模型的加载可以直接通过不同的模型名称，调用不同的url地址即可加载到本地。</p><ol start="4" type="1"><li><strong>定义优化器和损失函数：</strong></li></ol><p>优化器基于pytorch提供的SGD优化器，按原论文中提出的参数设置为0.9。</p><p>对于损失函数来说，在训练集上的采用的是标签平滑交叉熵损失函数，在验证集上的损失函数采用的是交叉熵损失函数。</p><h1 id="四实验结果及分析">四、实验结果及分析：</h1><p>GPU：NVIDIA Quadro RTX 4000，显存8G</p><p>在自己的电脑上训练一个epoch大概需要25分钟，并且batch_size最大可设置为16。</p><h2 id="结果展示">4.1 结果展示：</h2><p>（1）首先看一下经过30个epoch的训练以后得到的实验结果：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154028264.png" alt="image-20210715154028264"  /></p><p>可视化结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154042760.png" alt="image-20210715154042760"  /></p><p>（2）添加clip_grad参数，将并将学习率设置为0.03，不做warmup</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154049529.png" alt="image-20210715154049529"  /></p><p>经过30个epoch后，得到的结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154059275.png" alt="image-20210715154059275"  /></p><p>原论文的实验结果：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154110029.png" alt="image-20210715154110029"  /></p><p>实际验证效果：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154121041.png" alt="image-20210715154121041"  /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154125743.png" alt="image-20210715154125743"  /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154138023.png" alt="image-20210715154138023"  /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154220865.png" alt="image-20210715154220865"  /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154153288.png" alt="image-20210715154153288"  /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154220865.png" alt="image-20210715154204208"  /></p><h2 id="结果分析">4.2 结果分析：</h2><p>从原论文的实验结果的说明来看，作者分别在ImageNet、ImageNet-21k、JFT-300M三个数据集上对不同规模的模型进行了预训练，然后再将这些模型应用在CIFAR10、CIFAR100等数据集上做验证得到的Top1准确率的统计结果。比如，对于ViT-B/16来说，在ImageNet数据集上做预训练，然后在CIFAR10上做微调最终得到的Top1准确率为98.13%。而在自己的实验中得到的准确率在96.9%左右。</p><p>对于原论文中指出的一些超参数的设置：论文中作者指出，在<strong>训练过程</strong>中使用Adam优化器，平滑参数设置为(0.9,0.999)，权重衰减率设置为0.1，batch_size=4096。这些参数有比较好的表现。在<strong>微调阶段</strong>采用SGD优化器，momentum设为0.9，batch_size=512.</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154238989.png" alt="image-20210715154238989"  /></p><p>按照原论文512的batch_size来说，这一点由于本地资源有限无法完全和原论文保持一致，在本地的环境下batch_size最大可以设置为16。还有关于学习率的设置和衰减方案在原论文中提到好几种，有提到采用余弦衰减、也有采用分段常数衰减的方式，本次实验采用的是余弦衰减，初始值设置为0.03。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210715154247987.png" alt="image-20210715154247987"  /></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vision Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vision Transformer译文</title>
      <link href="post/bee8.html"/>
      <url>post/bee8.html</url>
      
        <content type="html"><![CDATA[<h1 id="视觉transformer综述"><strong>视觉Transformer综述</strong></h1><h1 id="摘要">摘要</h1><p>​Transformer是一种基于最初应用在自然语言处理领域上的自注意力机制的深度神经网络。受Transformer强表征能力的启发，研究人员将它扩展到计算机视觉任务上。与卷积网络和循环网络等其他网络类型相比，基于Transformer的模型在各种视觉基准上表现出竞争性甚至更好的性能。在这篇文献综述，我们通过将这些可视化Transformer模型应用在不同类别的任务中，并且对它们的优缺点进行了分析。特别地，主要类别包括基本图像分类、高级视觉、低级视觉和视频处理。也会简要回顾计算机视觉中的自注意力机制，因为自注意力机制是Transformer的基本组成部分。有效的Transformer模型推动了Transformer进入实际应用。最后，讨论了可视化Transformer的进一步研究方向。</p><span id="more"></span><h1 id="引言">1. 引言：</h1><p>深层神经网络已经成为现代人工智能系统的基础设施。已经提出了各种网络类型来处理不同的任务。由线性层和非线性激活组成的多层感知机或者说全连接层是经典的神经网络。卷积神经网络[CNN]引入了卷积层和池化层，用于处理像图像这样移位不变的数据。循环神经网络[RNN]利用循环单元来处理序列数据或时间序列数据。Transformer是一种新提出的网络，主要运用自注意力机制提取内在特征。在这些网络中，Transformer是最近发明的神经网络，但却在广泛的人工智能应用领域显示出了的巨大潜力。</p><p>Transformer最初应用于自然语言处理(NLP)任务，并带来了显著的改进。例如，Vaswani等人首先提出了仅基于注意力机制的Transformer模型用于机器翻译和英语选区解析任务。Devlin等人介绍了一种新的语言表示模型，称为BERT，它通过对左右两个上下文的联合调节，从未标记的文本中预先训练一个转换器。BERT获得了当时11个自然语言处理任务的最好结果。Brown等人在45TB压缩明文数据上预训练了基于GPT-3模型的具有1750亿个参数的巨大Transformer模型，并且在不同类型的下游自然语言任务上实现了强性能而无需微调。这些基于Transformer的模型表现出很强的表现能力，并在自然语言处理领域取得了突破。</p><p>受自然语言处理中Transformer能力的启发，最近研究人员将Transformer扩展到计算机视觉任务。CNN过去是视觉应用的基本组成部分，但是Transformer正在展示它作为CNN之外的另一种选择的能力。Chen等人训练一个序列Transformer对像素进行自回归预测，并与CNN相比在图像分类任务上取得有竞争力的结果。ViT是Dosovitskiy等人最近提出的视觉Transformer模型。它将单纯的Transformer模型直接应用于小块图像序列，并在多个图像识别基准上获得最先进的性能。除了基本的图像分类之外，Transformer还被用来解决更多的计算机视觉问题，如对象检测、语义分割、图像处理和视频理解。由于其优异的性能，越来越多的基于Transformer的模型被提出用于改进各种视觉任务。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113030319.png" alt="image-20210615113030319"  /></p><p>基于Transformer的视觉模型如雨后春笋般涌现，这导致我们难以跟上新进展的速度。因此，对现有的视觉模型进行调查是很有必要的，并且也对整个社区是有益的。在这篇文章中，我们重点提供了视觉Transformer的最新进展的综合概述，并讨论了进一步改进的潜在方向。为了更好地存档和方便不同主题的研究人员，我们按照应用场景对Transformer模型进行分类，如表1所示。特别地，主要场景包括基本图像分类、高级视觉、低级视觉和视频处理。高级视觉处理图像[121]中所见内容的解释和使用，例如对象检测、分割和车道检测。有许多Transformer模型解决了这些高层次的视觉任务，如DETR[14]，用于对象检测的可变形DETR [155]和用于分割的Max-DeepLab [126]。</p><p>低级图像处理主要涉及从图像(通常表示为图像本身)中提取描述[35]，其典型应用包括超分辨率、图像去噪和风格转换。低级视觉中很少有作品[17，92]使用Transformer，需要更多的研究。视频处理是计算机视觉中除了基于图像的任务之外的重要组成部分。由于视频的顺序特性，Transformer可以自然地应用于视频[154，144]。与传统神经网络CNNs或者RNNs相比，Transformer在这些任务上开始显示出竞争优势。在这里，我们对这些基于Transformer的可视化模型的工作进行了调查，以跟上这一领域的进展。视觉Transformer的发展时间表如图1所示，我们相信越来越多的优秀作品将被镌刻在里程碑上。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113127167.png" alt="image-20210615113127167"  /></p><p>论文的其余部分组织如下。第二节首先介绍自注意力机制和标准Transformer。我们在第3节中描述了自然语言处理中的Transformer方法，因为研究经验可能对视觉任务有益。接下来，第四部分是论文的主要部分，总结了图像分类、高级视觉、低级视觉和视频任务的视觉转换模型。我们还简要回顾了自注意力机制的CV和有效的Transformer方法，因为它们与我们的主题密切相关。最后，我们给出了结论并讨论了几个研究方向和挑战。</p><h1 id="transformer概述">2. Transformer概述：</h1><p>Transformer[123]首次应用于自然语言处理中的机器翻译任务。如图2所示，它由一个编码器模块和一个解码器模块组成，具有几个相同结构的编码器/解码器，每个编码器由自注意力层和前馈神经网络组成，而每个解码器由自注意力层、编码解码器注意力层和前馈神经网络组成。在用Transformer翻译句子之前，句子中的每个单词都会被嵌入到一个512维的向量中</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113136453.png" alt="image-20210615113136453"  /></p><h2 id="自注意力层">2.1 自注意力层：</h2><p>在自注意力层，输入向量首先被转换成三个不同的向量，即查询向量q、关键向量k和值向量v，向量的维数均为512维。从不同输入得到的向量然后被打包成三个不同的矩阵Q、K、V。之后，通过以下步骤计算不同输入向量之间的注意函数(如图3左侧所示):</p><ol type="1"><li><p>通过公式<span class="math inline">\(S = Q \bulletK^{T}\)</span>计算不同输入向量之间的得分。</p></li><li><p>通过公式<span class="math inline">\(S_{n} =\frac{S}{\sqrt{d_{k}}}\)</span>将梯度稳定性的分数标准化.</p></li><li><p>使用softmax函数将分数转换为概率: <span class="math inline">\(P =softmax(S_{n})\)</span></p></li><li><p>通过公式<span class="math inline">\(Z = V \bulletP\)</span>得到加权矩阵Z，整个过程可以统一为单个函数：<spanclass="math inline">\(\text{Attention}(Q,\ K,\ V\ ) = \\text{softmax}\left( \frac{Q \bullet K^{T}}{\sqrt{d_{k}}} \right)\bullet V\)</span> （1）</p></li></ol><p>直觉上公式(1)比较简单，第一步计算两个不同向量之间的得分，得分是为了确定当在当前位置对单词进行编码时，我们对其他单词的关注程度。第二步，为了更好的训练，将分数标准化，使其具有更稳定的梯度。第三步，将分数转换成概率。最后，每个值向量乘以加总概率，具有较大概率的向量将被随后的层更多地关注。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113151684.png" alt="image-20210615113151684"  /></p><p>解码器模块中的编解码注意力层与编码模块中的自注意力层几乎相同，只是键矩阵K和值矩阵V是从编码器模块中导出的，查询矩阵Q是从前一层中导出的。</p><p>注意，上述过程与每个单词的位置无关，因此自注意力层缺乏捕捉一个句子中单词位置信息的能力。为了解决这个问题，在原始输入中添加了一个带有维度<spanclass="math inline">\(d_{\text{model}}\)</span>的位置编码，以获得单词的最终输入向量。具体而言，该位置用以下等式编码:</p><p><span class="math inline">\({PE}(pos,2i) =sin(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}})\)</span>（2）</p><p><span class="math inline">\(PE(pos,2i + 1) =cos(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}})\)</span>（3）</p><p>其中pos表示单词在句子中的位置，i表示位置编码的当前维度。</p><h2 id="多头注意力">2.2 多头注意力：</h2><p>自注意力层通过添加一种称为多头注意力的机制来进一步改进，以提高普通自注意力的性能。注意，对于一个给定的参考词，我们在通读句子的时候，往往要重点关注其他几个词。因此，尽管不影响其他位置的注意力也是同等重要的，但是单头自注意力层限制了集中在特定位置(或几个特定位置)的能力。这是通过给注意力层不同的表示子空间来实现的。具体来说，不同的头使用不同的查询、键和值矩阵，并且由于随机初始化，它们可以在训练后将输入向量投影到不同的表示子空间中。</p><p>具体来说，给定一个输入向量和头数h，输入向量首先被转换成三组不同的向量，即查询组、关键字组和值组。每组有h个向量，向量维度是<spanclass="math inline">\(d_{q^{&#39;} } = d_{k^{&#39;} } = d_{v^{&#39;} } =\frac{d_{\text{model} } }{h} =64\)</span>。然后，从不同输入得到的向量被打包成三组不同的矩阵 <spanclass="math inline">\({ \{ Q_{i}\} }_{i = 1}^{h}、{ \{ K_{i}\} }_{i =1}^{h}、{ \{ V_{i}\} }_{i =1}^{h}\)</span>，然后多头注意力的处理过程如下：</p><p><span class="math inline">\(\text{Multi}\text{Head}\left(Q^{&#39;},K^{&#39;},V^{&#39;} \right) = Concat\left(\text{head}_{1},\ldots,\text{head}_{h} \right)W^{0},\)</span> (4)</p><p><span class="math display">\[\text{where }\text{head}_{i} =Attention(Q_{i},K_{i},V_{i})\]</span></p><p>where<span class="math inline">\(Q^{&#39;}是{ \{ Q_{i}\}}_{i =1}^{h}的连接,K^{&#39;},V^{&#39;}同理\)</span>。<spanclass="math inline">\(w^{0} \in \R^{d_{\text{model}}*d_{\text{model}}}\)</span></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113218314.png" alt="image-20210615113218314"  /></p><h2 id="transformer其他部分">2.3 Transformer其他部分：</h2><p><strong>编码器和解码器中的其他部分。</strong>如图4所示，为了加强信息流并获得更好的性能，在编码器和解码器的每个子层中添加了残差连接。随后是层规范化[4]。上述操作的输出可以描述为:</p><p><span class="math inline">\(\text{LayerNorm}\left( X\  + \Attention(X) \right)\)</span>（5）</p><p>需要注意的是，这里使用X作为自注意力层的输入，因为查询、键和值矩阵Q、K和V都来自于同一个输入矩阵X</p><p><strong>前馈神经网络。</strong>在每个编码器和解码器的自注意力层之后采用前馈神经网络。具体来说，前馈神经网络由两个线性变换层和其中一个ReLU激活函数组成，可以表示为:</p><p><span class="math inline">\(\text{FFNN}(X) = \ w_{2}\sigma\left(W_{1}X \right)\)</span> (6)</p><p>其中，w1、w2为两个线性变换层的两个参数矩阵，σ为ReLU激活函数。隐层的维数是2048。</p><p><strong>解码器的最后一层。</strong>解码器的最后一层的目的是将向量的堆栈转换回单词。这是通过一个线性层和一个softmax层来实现的。线性层将向量投射到具有<spanclass="math inline">\(d_{\text{word}}\)</span>维的向量中，其中<spanclass="math inline">\(d_{\text{word}}\)</span>是词汇表中的单词数。然后，利用softmax层将该向量转换为概率。</p><p>大多数用于计算机视觉任务的Transformer都利用了原Transformer的编码器模块。简而言之，它是一种新的特征选择器，不同于卷积神经网络(CNNs)和循环神经网络(RNNs)。相比CNN只关注局部特征，变压器能够捕捉到长距离特征，这意味着Transformer可以很容易地获取全局信息。与RNN的隐状态顺序计算相比，Transformer的自注意力层和全连接层的输出可以并行计算，且易于加速，因此效率更高。因此，进一步研究Transformer在自然语言处理和计算机视觉领域的应用具有重要意义。</p><h1 id="再看nlp中的transformer">3. 再看NLP中的Transformer:</h1><p>在Transformer出现之前，具有附加功能的循环神经网络(例如GRU[26]和LSTM[50])增强了大多数最先进的语言模型。然而，在RNN中，信息流需要从前一个隐藏状态到下一个隐藏状态的顺序处理，这就妨碍了训练过程中的加速和并行化，从而阻碍了RNN处理更长的序列或构建更大的模型的潜力。2017年，Vaswani等人[123]提出了Transformer，这是一种全新的编码器-解码器架构，完全建立在多头自注意力机制和前馈神经网络上，旨在解决序列到序列的自然语言任务(如机器翻译)，轻松获取全局依赖关系。Transformer的成功证明，仅利用注意力机制就可以获得与专注的RNN相当的性能。此外，Transformer的体系结构支持大规模并行计算，允许在更大的数据集上进行训练，从而导致大量用于自然语言处理的大型预训练模型(PTM)。</p><p>BERT[29]及其变体(如SpanBERT [63]，RoBERTa[82])是一系列基于多层Transformer编码器架构的PTM。在BERT训练前阶段，分别对图书语料库[156]和英文维基百科数据集进行两项任务:1)遮挡语言建模(MLM)，首先随机遮挡输入中的一些标记，然后训练模型进行预测;2)下一个句子预测使用成对句子作为输入，并预测第二句是否为文档中的原始句子。经过预训练后，BERT可以通过在较宽的下游任务范围内单独添加一个输出层来进行微调。更具体地说，在执行序列级任务(例如，情感分析)时，BERT使用第一个标记的表示进行分类;而对于令牌级任务(例如，名称实体识别)，所有令牌都被送入softmax层进行分类。在发布时，BERT在11个自然语言处理任务上取得了最先进的结果，在预先训练的语言模型中建立了一个里程碑。生成式预训练Transformer系列(例如，GPT[99]，GPT-2[100])是另一种基于Transformer解码器架构的预训练模型，它使用遮挡的自注意力机制。GPT系列与BERT系列的主要区别在于训练前的方式。与BERT不同，GPT系列是经过从左到右(LTR)语言建模预处理的单向语言模型。此外，句子分隔符(SEP)和分类符号(CLS)只参与GPT的微调阶段，BERT在训练前学习了这些嵌入。由于GPT的单向预训练策略，它在许多自然语言生成任务中表现出了优越性。最近，一个巨大的基于Transformer的模型，GPT-3，具有令人难以置信的1750亿个参数被引入[10]。通过对45TB压缩明文数据进行预训练，GPT-3能够直接处理不同类型的下游自然语言任务，无需微调，在许多自然语言数据集上实现了强大的性能，包括自然语言理解和生成。除了上述基于Transformer的PTMs，自Transformer引入以来，许多其他模型也被提出。因为这不是我们调查的主要主题，我们只是在表2中为感兴趣的读者列出了一些有代表性的模型。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615113502666.png" alt="image-20210615113502666"  /></p><p>除了在大型语料库上训练用于一般自然语言处理任务的PTMs之外，基于Transformer的模型已经应用于许多其他自然语言处理相关领域或多模态任务。BioNLP域。基于Transformer的模型已经超过了许多传统的生物医学方法。BioBERT[69]使用Transformer架构进行生物医学文本挖掘任务;SciBERT[7]是由训练Transformer针对114M涵盖生物医学和计算机科学领域的科学文章开发的，旨在更精确地执行与科学领域相关的自然语言处理任务;Huang等人[55]提出ClinicalBERT利用Transformer开发和评估临床记录的连续表示，作为一个副作用，ClinicalBERT的注意力图可以用来解释预测，从而发现不同医疗内容之间的高质量联系。Multi-ModalTasks。由于Transformer在基于文本的NLP任务上的成功，许多研究致力于探索基于Transformer处理多模式的任务(例如，视频文本，图像文本和音频文本)的可能性。VideoBERT[115]使用一个基于CNN的模块对视频进行预处理以获得表示标记，基于此，Transformer编码器被训练学习用于下游任务的视频-文本表示，如视频标题。VisualBERT[72]和VL-BERT[114]提出了单流统一Transformer来捕获视觉元素和图像-文本关系，用于诸如视觉问答(VQA)和视觉共性推理(VCR)等下游任务。此外，一些研究(如Speechbert[24])探索了使用Transformer编码器编码音频和文本对以处理语音问答(SQA)等自动文本任务的可能性。</p><p>基于Transformer的模型在各种自然语言处理和相关任务上的快速发展显示了其结构优势和通用性。这使得Transformer成为自然语言处理之外的许多其他AI领域的通用模块。接下来的部分将集中讨论Transformer在过去两年中出现的各种计算机视觉任务中的应用。</p><h1 id="视觉transformer">4. 视觉Transformer：</h1><p>在本节中，我们将全面回顾计算机视觉中基于Transformer的模型，包括在图像分类、高级视觉、低级视觉和视频处理中的应用。简要介绍了自注意力机制和模型压缩方法在高效Transformer中的应用。</p><h2 id="图像分类">4.1 图像分类：</h2><p>受Transformer在自然语言处理方面巨大成功的启发，一些研究人员试图检验类似的模型是否可以学习有用的图像表示。图像是一种比文本维度更高、噪声更大、冗余度更高的形态，因此生成式建模比较困难。iGPT[18]和ViT[31]是两个纯粹使用Transformer进行图像分类的模型。</p><h3 id="igpt">4.1.1 iGPT:</h3><p>从原始的图像生成预训练方法开始已经有很长一段时间了，Chen等人[18]结合自监督方法的最新进展重新审视了这类方法。该方法包括一个训练前阶段，然后是一个微调阶段。在训练前，我们探讨了自回归和BERT目标。此外，在自然语言处理中，使用序列转换体系结构代替语言标记来预测像素。当与早期停止结合使用时，预训练可以被视为一个有利的初始化或正则化。在微调过程中，他们将一个小的分类头添加到模型中，用于优化分类目标，并适应所有的权重。</p><p>给定一个由高维数据X组成的未标记数据集X =(x1,...,xn)。他们通过最小化数据的负对数似然来训练模型:</p><div data-align="center"><span class="math inline">\(L_{\text{AR}} = E\lbrack -log(p(x))\rbrack\)</span> (7)</div><p>其中p(x)是图像数据的密度，可以建模为:</p><p><span class="math inline">\(p(x) = \ \prod_{i =1}^{n}{p(x_{\pi_{i}}|x_{\pi_{1}},\ldots,x_{\pi_{i -1}},\theta)}\)</span> (8)</p><p>他们还考虑了BERT目标，标记一个子序列样本M⊂(1,n),每个索引独立且有0.15的概率出现在M中， M叫做BERT遮挡,和模型训练通过最小化的负对数似然"遮挡"元素<spanclass="math inline">\(x_{M}\)</span> ，以 "非遮挡"的<spanclass="math inline">\(x_{\lbrack 1,n\rbrack\backslashM}\)</span>为条件:</p><p><span class="math inline">\(L_{\text{BERT}} = EE\sum_{i \inM}^{}{\lbrack - \log{p\left( x_{i}|x_{\lbrack 1,n\rbrack\backslash M}\right)}\rbrack}\)</span> (9)</p><p>在预训练中，他们选择<span class="math inline">\(L_{\text{AR}}\ 或者\L_{\text{BERT}}\)</span>中的一个，并尽量降低损失。</p><p>他们使用GPT-2[100]公式的Transformer解码器块。特别地，层规范化先于注意力和多层感知器(MLP)操作，并且所有操作都严格地位于残差路径上。跨序列元素的唯一混合发生在注意力操作中，为了确保在训练增强现实目标时进行适当的调节，他们将标准的上三角掩模应用于n×n矩阵的注意力逻辑。当使用BERT目标时，不需要注意逻辑遮挡:在将内容嵌入应用到输入序列之后，它们将位置归零。</p><p>在最后的转换器层之后，他们应用层规范化，并从输出中学习投影，以对每个序列元素的条件分布进行参数化。当训练BERT时，他们只是简单地忽略未加遮挡位置的逻辑。</p><p>在微调过程中，它们平均汇集最终层规范化的输出，跨越序列维度提取每个示例特征的三维向量:</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114108165.png"alt="image-20210615114108165" /><figcaption aria-hidden="true">image-20210615114108165</figcaption></figure><p>他们从<spanclass="math inline">\(f_{L}\)</span>类逻辑中学习投影，用于最小化交叉熵损失<spanclass="math inline">\(L_{\text{CLF}}\)</span>。在实践中，他们根据经验发现联合目标<spanclass="math inline">\(L_{\text{GEN}} +L_{\text{CLF}}\)</span>的效果更好，其中<spanclass="math inline">\(L_{\text{GEN\ }} \in \left\{L_{\text{AR}},L_{\text{BERT}} \right\}\)</span></p><p><img src="C:\Users\Yore\AppData\Roaming\Typora\typora-user-images\image-20210615114117893.png" alt="image-20210615114117893"  /></p><h3 id="vit">4.1.2 ViT:</h3><p>最近，Dosovitskiy等人[31]提出了一种纯净的Transformer模型，即VisionTransformer(ViT)，当直接应用于图像块序列时，它在图像分类任务中表现良好。他们尽可能地遵循原始Transformer的设计。图5显示了ViT的框架。</p><p>为了处理2D图像，图像<span class="math inline">\(x \inR^{H*W*C}\)</span>被重新成形为一系列平坦的2D块<spanclass="math inline">\(x_{p} \inR^{N*(P^{2}*C)}\)</span>。(H，W)是原始图像的分辨率,(P，P)是每个图像块的分辨率，<span class="math inline">\(N = \\frac{\text{HW}}{P^{2}}\)</span>则是Transformer的有效序列长度。由于Transformer在其所有层中使用恒定的宽度，可训练的线性投影将每个矢量化路径映射到模型维度D，其输出被称为块嵌入。</p><p>类似于BERT的[class]记号，可学习的嵌入被用于嵌入块的序列，其在Transformer编码器输出端的状态用作图像表示。在预训练和微调过程中，分类头的大小相同。此外，1D位置嵌入被添加到块嵌入以保留位置信息。他们探索了位置嵌入的不同2D感知变体，这并没有获得比标准1D位置嵌入更大的收益。联合嵌入作为编码器的输入被切断。值得注意的是，视觉Transformer仅采用标准Transformer的编码器，并且Transformer编码器的输出后面是MLP磁头。</p><p>通常情况下，ViT首先在大型数据集上进行预训练，并针对较小的下游任务进行微调。为此，移除预包含的预测头，并附加一个零初始化的D×K前馈层，其中K是下游类别的数量。相比于预训练，通常时是对高分辨率微调有益的。当输入更高分辨率的图像时，图片快大小保持不变，这导致更大的有效序列长度。视觉Transformer可以处理任意序列长度，然而，预先训练的位置嵌入可能不再有意义。作者根据它们在原始图像中的位置对预先训练的位置嵌入进行插值。请注意，分辨率调整和图片块提取是手动将图像2D结构的感应偏差注入视觉Transformer的唯一点。</p><p>当在中型数据集(如ImageNet)上进行训练时，这种模型产生的结果一般，精度比同等规模的ResNets低几个百分点。Transformer缺乏CNNs固有的一些归纳偏差，如翻译等方差和局部性，因此在数据量不足的情况下训练时不能很好地概括。然而，如果模型是在大数据集(14M-300M图像)上训练的，图片就会改变。作者发现大规模训练胜过归纳偏见。Transformer在经过足够规模的预训练并转移到数据点较少的任务时，可以获得出色的效果。在JFT300M数据集上预处理的视觉Transformer接近或超过了多种图像识别基准的先进水平，在ImageNet上达到88.36%的准确率，在CIFAR-10上达到99.50%，在CIFAR-100上达到94.55%，在VTAB套件的19项任务中达到77.16%。iGPT和ViT的详细结果如表3所示。</p><p><img src="C:\Users\Yore\AppData\Roaming\Typora\typora-user-images\image-20210615114130119.png" alt="image-20210615114130119"  /></p><p>总之，iGPT回忆了生成性预训练方法，并将其与自监督方法相结合，结果并不十分令人满意。ViT取得了更好的结果，尤其是当它使用更大的数据集(JFT-300)时。但是ViT的结构和NLP中的Transformer基本相同，如何表述块内和块间的关系仍然是一个具有挑战性的问题。此外，相同大小的块在ViT中被同等对待。众所周知，每个块的复杂性是不同的，这个特点现在还没有被充分利用。</p><h2 id="高级视觉">4.2 高级视觉：</h2><p>近来，人们对采用Transformer进行高级计算机视觉任务越来越感兴趣，例如目标检测[15，155，23]，车道检测[81]和分割[129，126]。在本节中，我们对这些方法进行了回顾。</p><h3 id="目标检测">4.2.1 目标检测：</h3><p>根据采用Transformer架构的模块，基于Transformer的目标检测方法可以粗略地分为基于颈部、基于头部和基于框架的方法。</p><p>像特征金字塔网络(FPN)[77]这样的多尺度特征融合模块(在现代检测框架中被称为颈部)已经被广泛用于目标检测以获得更好的检测性能。张等人[145]提出传统方法不能交互跨尺度特征，因此提出特征金字塔变换(FPT)来充分利用跨空间和尺度的特征交互。FPT由自transformer、地transformer和渲染transformer三种类型的transformer组成，分别对特征金字塔的自层次、自顶向下和自底向上路径的信息进行编码。FPT基本上利用Transformer中的自注意力模块来增强特征金字塔网络的特征融合。</p><p>预测头在目标检测器中起着重要的作用。现有的检测方法通常利用单一的视觉表示(例如，边界框和角点)来预测最终结果。Chi等人[23]提出了桥接视觉表征(BVR)，通过多头注意模块将不同的异质表征组合成一个单一的表征。具体地，主表示被视为查询输入，辅助表示被视为键输入。通过类似于Transformer中的注意力模块，可以获得用于主表示的增强特征，这桥接了来自辅助表示的信息，并且有利于最终的检测性能。</p><p>与上述利用Transformer增强现代探测器特定模块的方法不同，卡里昂[15]重新设计了物体探测框架，并提出了detectiontransformer(DETR)，这是一种简单且完全端到端的物体探测器。DETR将目标检测任务视为一个直观的集合预测问题，摆脱了传统的手工制作组件像anchor生成和非最大抑制(NMS)后处理。如图6所示，DETR从CNN主干开始，从输入图像中提取特征。为了用位置信息来补充图像特征，在被馈送到编码-解码转换器之前，固定位置编码被添加到展平特征。transformer解码器使用来自编码器的嵌入以及N个学习的位置编码(对象查询)，并产生N个输出嵌入，其中N是图像中预先定义的参数和最大数量的对象。最终的预测是用简单的前馈网络(FFN)计算的，它包括边界框坐标和类别标签，以指示对象的特定类别或没有对象。不像原来的Transformer顺序产生预测，DETR同时并行解码N个对象。DETR采用一种二分匹配算法来分配预测对象和真实对象。如等式所示。(11)中，Hungarian损失被用来计算所有匹配对象对的损失函数。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114148519.png" alt="image-20210615114148519"  /></p><p>DETR在目标检测方面表现出令人印象深刻的性能，其准确性和速度与COCO基准上流行且公认的更快的R-CNN基线相当。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114200407.png" alt="image-20210615114200407"  /></p><p>DETR是一个基于Transformer的物体检测框架的新设计，并启发社区开发完全端到端的检测器。然而，普通的DETR也带来了一些挑战，例如，更长的训练时间表和小物体的不良表现。Zhu等人[155]提出的可变形是解决上述问题的一种常用方法，大大提高了检测性能。变形注意模块不是通过Transformer中的原始多头注意来查看图像特征地图上的所有空间位置，而是关注参考点周围的一小组关键位置。这样，计算复杂度大大降低，也有利于快速收敛。更重要的是，可变形的注意模块可以容易地应用于融合多尺度特征。变形DETR比DETR的性能好，训练成本低10倍，推理速度快1.6倍。一些额外的改进也适用于可变形DETR，包括有效的迭代包围盒细化方法和两阶段方案，这导致进一步的性能增益。</p><p>针对计算复杂度较高的问题，Zhang等人[153]提出了一种自适应聚类变换器(ACT)来降低预先训练的的计算开销，无需任何训练过程。ACT使用局部敏感哈希方法自适应地对查询特征进行聚类，并将注意力输出广播给由所选原型表示的查询。通过用建议的ACT代替预先训练的DETR模型的自我注意力模块而无需任何再训练，计算成本可以大大降低，而精度几乎没有下降。此外，通过利用多任务知识提取(MTKD)方法，可以进一步降低性能下降，该方法利用原始transformer提取具有几个微调时期的ACT模块。</p><p>Sun等[117]研究了模型的慢收敛问题，揭示了Transformer解码器中的交叉注意模块是其背后的主要原因。为此，提出了DETR的仅编码器版本，并且在检测精度和训练收敛方面实现了相当大的改进。此外，为了更稳定的训练和更快的收敛，设计了一种新的二分匹配方案。提出了两种基于Transformer的集合预测模型，即TSP-FCOS模型和TSPRCNN模型，这两种模型比原DETR模型具有更好的性能。</p><p>受自然语言处理中预训练transformer方案的启发，Dai等人[28]提出了一种无监督预训练(UP-DETR)的目标检测方法。具体而言，提出了一种新的无监督前置任务随机查询块检测来预处理DETR模型。通过该方案，IP-DETR在相对较小的数据集上，即PASCALVOC，大幅度提高了检测精度。在训练数据充足的COCO基准上，UP-DETR仍优于DETR，证明了无监督预训练方案的有效性。</p><h3 id="分割">4.2.2 分割：</h3><p>DETR[15]可以通过在解码器上附加一个遮挡头来自然地扩展全景分割任务，并获得有竞争力的结果。Wang等人[126]提出了Max-DeepLab，利用遮挡变换直接预测全景图像分割结果，不需要盒子检测等代理子任务。与DETR类似，Max-DeepLab以端到端的方式简化了全景分割任务，并直接预测了一组不重叠的遮挡和相应的标签。利用PQ损失来训练模型。此外，与以前在CNN主干上堆叠transformer的方法不同，Max-DeepLab采用了双路框架，以便更好地将CNN与transformer结合起来。</p><p>Wang等人[129]提出了一种基于变换的视频实例分割(VisTR)模型，该模型以一系列图像作为输入，并产生相应的实例预测结果。提出了一种实例序列匹配策略，将预测与真值相匹配。为了获得每个实例的遮挡序列，VisTR利用实例序列分割模块来累积来自多个帧的遮挡特征并且用3DCNN分割遮挡序列。</p><p>还有一种尝试是将Transformer用于细胞实例分割[95]，其基于DETR全景分割模型。所提出的Cell-DETR还增加了跳跃连接，以在分割头中桥接CNN主干和CNN解码器的特征，以获得更好的融合特征。Cell-DETR展示了显微图像细胞实例分割的最新性能。</p><p>Zhao等[150]设计了一种用于处理点云的新型Transformer架构(PointTransformer)。所提出的自注意力层对于点集的排列是不变的，因此适用于点集处理任务。PointTransformer)在三维点云语义分割任务中表现出很强的性能。</p><h3 id="车道检测">4.2.3 车道检测：</h3><p>在PolyLaneNet[119]的基础上，Liu等人[81]提出了利用Transformer网络学习全局上下文来提高弯道检测的性能。与PolyLaneNet相似，该方法(LSTR)将车道检测视为用多项式拟合车道的任务，并使用神经网络来预测多项式的参数。为了捕捉车道和全局环境的细长结构，LSTR在体系结构中引入了Transformer网络，以处理由卷积神经网络提取的低级特征。此外，LSTR使用Hungarian损失优化网络参数。如[81]所示，LSTR只有PolyLaneNet网络参数的0.2倍，和PolyLaneNet相比高2.82%的精度和3.65倍的FPS。Transformer网络、卷积神经网络和Hungarian损失的结合实现了一个微小、快速、精确的车道检测框架。</p><h2 id="低级视觉">4.3 低级视觉：</h2><p>除了高级视觉任务，很少有工作将Transformer应用于低级视觉领域，如图像超分辨率、生成等。与输出为标签或盒子的分类、分割和检测相比，低层任务往往以图像作为输出(如高分辨率或去噪图像)，这更具挑战性。</p><p>Parmar等人[92]在概括Transformer模型方面迈出了第一步，以制定图像转换和生成任务，并提出了图像Transformer。图像Transformer由两部分组成:用于提取图像表示的编码器和用于生成像素的解码器。对于值为0-255的每个像素，学习256×d维嵌入，用于将每个值编码为d维向量，该向量作为编码器的输入。编码器和解码器的结构与[123]中的相同。解码器中各层的详细结构如下图7所示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114259935.png" alt="image-20210615114259935"  /></p><p>每个输出像素<spanclass="math inline">\(q^{&#39;}\)</span>通过计算输入像素q和先前生成的像素m1、m2，...利用位置嵌入p1、p2，....对于图像条件生成，例如超分辨率和修复，使用编码器-解码器架构，其中编码器的输入是低分辨率图像或损坏的图像。对于无条件和有条件生成(即图像噪声)，只有解码器用于输入噪声矢量。由于解码器的输入是以前生成的像素，在生成高分辨率图像时会带来很大的计算成本，因此提出了一种局部自注意力方案，该方案只使用最近生成的像素作为解码器的输入。结果，图像转换器在图像生成和翻译任务上可以达到与基于CNN的模型的竞争性能，这表明了基于Transformer的模型在低水平视觉任务上的有效性。</p><p>与使用每个像素作为Transformer模型的输入相比，最近的工作使用图片快(像素集)作为输入。Yang等人[135]提出了用于图像超分辨率的纹理变换网络(TTSR)。他们在基于参考的图像超分辨率问题中使用Transformer架构，其目的是将相关纹理从参考图像转移到低分辨率图像。以低分辨率图像和参考图像为查询Q和关键K，相关性<spanclass="math inline">\(r_{i,j}\)</span>的计算通过每一个图像块Q中的<spanclass="math inline">\(q_{i}和K中的K_{i}\)</span>用如下公式计算：</p><p><span class="math inline">\(r_{i,j} = \  &lt;\frac{q_{i}}{||q_{i}||},\frac{k_{i}}{||k_{i}||} &gt;\)</span> (12)</p><p>然后提出了一种高分辨率特征选择模块，根据参考图像选择高分辨率特征V，利用相关性匹配低分辨率图像。计算方法是:</p><p><span class="math inline">\(h_{i} = \\underset{i}{\arg\max}r_{i,j}\)</span> (13)</p><p>那么最相关的图片块是<span class="math inline">\(t_{i} = \v_{\text{hi}}\)</span>，其中T中的<spanclass="math inline">\(t_{i}\)</span>是转移的特征。之后，软注意力模块用于将V转换为低分辨率特征f。软注意力可以通过以下方式计算：</p><p><span class="math inline">\(s_{i} = \ \max_{i}r_{i,j}\)</span>(14)</p><p>因此，将高分辨率纹理图像转换为低分辨率图像的等式可以表述为:</p><p><span class="math inline">\(F_{\text{out}} = F +Conv(Concat(F,T))\bigodot S\)</span> (15)</p><p>其中<spanclass="math inline">\(F_{\text{out}}\)</span>和F代表低分辨率图像的输出和输入特征，S代表软注意，T代表从高分辨率纹理图像转移的特征。通过引入基于Transformer的架构，TTSR可以成功地将纹理信息从高分辨率参考图像传输到低分辨率图像，以完成超分辨率任务。</p><p>上述方法在单个任务上使用Transformer模型，而等人[17]提出图像处理Transformer(IPT)通过使用大规模预训练来充分利用Transformer的优势，并在包括超分辨率、去噪和降额在内的多个图像处理任务中实现最先进的性能。如图8所示。IPT由多头、编码器、解码器和多尾组成。针对不同的图像处理任务，引入了多头多尾结构和任务嵌入。这些特征被分成小块以放入编码器-解码器结构中，然后输出被整形为具有相同大小的特征。由于Transformer模型在大规模预训练方面显示出优势，IPT使用ImageNet数据集进行预训练。具体来说，从ImageNet下载的图像通过手动添加噪声，雨带或者下采样降级为生成的损坏图像，然后将退化图像作为IPT的输入，将干净图像作为输出的优化目标。为了提高IPT模型的泛化能力，引入了自监督方法。然后使用相应的头部、尾部和任务嵌入对每个任务的训练模型进行微调。IPT极大地提高了图像处理任务的性能(例如，图像去噪任务中的2dB)，这证明了基于Transformer的模型在低层视觉领域的巨大潜力。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114317649.png" alt="image-20210615114317649"  /></p><h2 id="视频处理">4.4 视频处理：</h2><p>Transformer在基于序列的任务上表现惊人，尤其是在NLP任务上。在计算机视觉中，空间和时间维度信息在视频任务中受到青睐。因此，Transformer被应用于许多视频任务，如帧合成[83]，动作识别[41]和视频检索[80]。</p><h3 id="高级视频处理">4.4.1 高级视频处理:</h3><p><strong>行人行为识别。</strong>视频人体动作任务是指在视频中识别和定位人体动作。背景在识别人类行为中起着至关重要的作用。Rohit等人提出动作转换器[41]来模拟感兴趣的人和周围事物之间的潜在关系。具体来说，I3D被用作提取高级特征映射的主干。通过感兴趣区域池从中间特征图中提取的特征被视为查询()。从中间特征计算键值（K）。自注意力机制由三部分组成，并输出分类和回归预测。Lohit等人[84]提出了一个可解释的可微模块，称为时空Transformer网络，以减少类内方差和增加类间方差。Fayyaz和Gall提出了一种时空Transformer，用于在弱监督设置下执行动作识别任务。</p><p><strong>人脸对齐。</strong>基于视频的人脸对齐任务旨在定位面部标志。时间相关性和空间信息对最终性能很重要。然而，前一种方法不能捕获连续帧上的时间信息和静止帧上的补充空间信息。因此，Liu等人[80]使用双流Transformer网络来分别学习时间和空间特征。两个流以端到端的方式被联合优化，并且特征被加权以获得最终的预测。</p><p><strong>视频检索。</strong>基于内容的视频检索的关键是找到视频之间的相似性。要克服这些缺点，只需利用视频级特征。Shao等人[110]建议使用Transformer来建模范围语义相关性。此外，引入监督对比学习策略进行硬否定挖掘。基准数据集上的结果证明了性能和速度优势。Gabeur等人[39]提出了一种多模态Transformer来学习不同的跨模态线索，从而表示视频。</p><p><strong>动作识别。</strong>动作识别是指识别一个群体中的一个人的动作。以前解决这个问题的方法是基于单个人的位置。Gavrilyuk等人提出了一个actor-transformer[40]架构来学习表示。actor-transformer将2D和3D网络生成的静态和动态表示作为输入。Transformer的输出是预测的动作。</p><p><strong>视频目标检测。</strong>为了从视频中检测对象，需要全局和局部信息。Chen等人介绍了内存增强的全局-局部聚合(MEGA)[19]来捕获更多的内容。代表性特征提高了整体性能，并解决了无效和不足的问题。Yin等人[138]提出了一种时空transformer来聚集空间和时间信息。与另一个空间特征编码组件一起，这两个组件在3D视频对象检测任务中表现良好。</p><p><strong>多任务学习。</strong>未剪辑的视频通常包含许多与目标任务无关的帧。因此，挖掘相关信息，去除冗余信息至关重要。为了应对未剪辑视频上的多任务学习，Seong等人采用视频多任务Transformer网络[109]来提取信息。对于CoVieW数据集，任务是场景识别、动作识别和重要性分数预测。ImageNet和Places365上的两个预训练网络提取场景特征和对象特征。在类别转换矩阵(CCM)的帮助下，多任务Transformer被堆叠以融合特征。</p><h3 id="低级视频处理">4.4.2 低级视频处理：</h3><p><strong>帧/视频合成。</strong>帧合成任务是指在两个连续帧之间或在一个帧序列之后合成帧。视频合成任务旨在合成视频。Liu等人提出了ConvTransformer[83]，它包括五个部分:特征嵌入、位置编码、编码器、查询解码器和合成前馈网络。与基于LSTM的作品相比，ConvTransformer以更具可并行性的架构实现了更好的效果。Schatz等人[108]使用一个递归的Transformer网络从新的角度合成人类行为。</p><p><strong>视频修复。</strong>视频修复任务旨在完成帧中缺失的区域.这项具有挑战性的任务需要沿着空间和时间维度合并信息。Zeng等人为此任务提出了一个时空Transformer网络[144]。将所有输入帧作为输入，并并行填充它们。该网络的优化采用时空对抗损失函数。</p><h3 id="多模态">4.4.3 多模态：</h3><p><strong>视频字幕/摘要。</strong>视频字幕任务的目标是为未剪辑的视频生成文本。事件检测和描述模块是两个主要部分。Zhou等人[154]提出了一种端到端优化的transformer来解决密集视频字幕任务。编码器将视频转换成表示形式。提议解码器从编码中生成事件提议。字幕解码器用建议屏蔽编码并输出描述。Bilkhu等人[9]使用C3D和I3D网络提取特征，并使用transformer生成预测。该算法在单一总结任务和密集总结任务上都表现良好。Li等人[71]利用基于纠缠注意力(ETA)模块的transformer来处理图像字幕任务。Sun等人[29]提出了一个视觉语言框架来学习没有监督的代表。该模型可以应用于许多任务，包括视频字幕、动作分类等。</p><h2 id="计算机视觉的自注意力">4.5 计算机视觉的自注意力：</h2><p>在上面的章节中，我们已经回顾了将Transformer架构用于可视化任务的方法。自注意力是transformer的关键部分。在这一节中，我们深入研究了基于自注意力的方法在计算机视觉中的挑战性任务，例如，语义分割，实例分割，对象检测，关键点检测和深度估计。我们从第4.5.1节中的自注意力算法开始，并在第4.5.2节中总结了将自注意力用于计算机视觉的现有应用。</p><h3 id="自注意力的一般公式">4.5.1 自注意力的一般公式：</h3><p>用于机器翻译的自注意力模块[123]通过关注所有位置并在嵌入空间中对它们进行加权求和来计算序列中一个位置处的响应，这可以被视为计算机视觉中可应用的非局部过滤操作[128，11]的一种形式。</p><p>我们遵循惯例[128]制定自注意力模块。给定输入信号(例如，图像、序列、视频和特征)<spanclass="math inline">\(X \in R^{n \times c}\)</span> 其中n = h ×w表示特征中的像素数，c表示通道数。输出信号通过如下公式计算：</p><p><span class="math inline">\(y_{i} = \ \frac{1}{C(x_{i})}\\sum_{\forall\ j}^{}{f(x_{i},x_{j})g(x_{j})}\)</span> (16)</p><p>其中)<span class="math inline">\(x_{i} \in R^{1 \timesc}\)</span>和<span class="math inline">\(y_{i} \in R^{1 \timesc}\)</span>分别表示输入信号X和输出信号Y的位置(例如，空间、时间和时空)。下标j是枚举所有位置的索引。成对函数f计算表示关系，如i和所有j之间的亲和力。函数g计算位置j处输入信号的表示。响应用因子<spanclass="math inline">\(C(x_{i})\)</span>归一化。</p><p>请注意，成对函数f有许多选择，例如，高斯函数的简单扩展可用于计算嵌入空间中的相似性,因此，函数f可以表示为</p><p><span class="math inline">\(f\left( x_{i},x_{j} \right) = \e^{\theta(x_{i}){\phi(x_{j})}^{T}}\)</span> (17)</p><p>其中θ和φ可以是任何嵌入层。如果我们考虑线性嵌入形式的θ，φ，g，<spanclass="math inline">\(\theta(X) = \ X \bullet W_{\theta}\)</span>，<spanclass="math inline">\(\phi(X) = \ X \bullet W_{\phi}\)</span>，<spanclass="math inline">\(g(X) = \ X \bullet W_{g}\)</span> 其中<spanclass="math inline">\(W_{\theta} \in R^{c \times d_{k}}\)</span></p><p><span class="math inline">\(W_{\phi} \in R^{c \timesd_{\phi}}\)</span>，<span class="math inline">\(W_{g} \in R^{c \timesd_{v}}\)</span>.并将归一化因子设置为<spanclass="math inline">\(\sum_{\forall\ j}^{}{f\left( x_{i},x_{j}\right)}\)</span></p><p>则公式 16可改写为:</p><p><span class="math inline">\(y_{i} = \\frac{e^{x_{i}w_{\theta,i}w_{\theta,j}^{T}x_{j}^{T}}}{\sum_{j}^{}e^{x_{i}w_{\theta,i}w_{\theta,j}^{T}x_{j}^{T}}}x_{j}w_{g,j}\)</span>(18)</p><p>其中 <span class="math inline">\(W_{\theta,i} \in R^{c \times1}\)</span>是权值矩阵<spanclass="math inline">\(W_{\theta}\)</span>的第i行。对于给定的一个下标i，<spanclass="math inline">\(\frac{1}{C(x_{i})}\text{\ f}\left( x_{i},x_{j}\right)\)</span>成为沿维度j的softmax输出，因此公式可进一步改写为:</p><p><span class="math inline">\(Y = \ \text{soft}\max\left(XW_{\theta}W_{\phi}^{T}X \right)g(X)\)</span> (19)</p><p>其中 <span class="math inline">\(Y \in R^{n \times c}\)</span>是与X大小相同的输出信号，与transformer模型的查询、键和值的表示<spanclass="math inline">\(Q = XW_{q}\text{\ \ }K = XW_{k}\text{\ \ }V =XW_{v}\)</span>相比较，一旦<span class="math inline">\({W_{q} = \text{\W}}_{\text{θ\ }}\ {W_{k} = \text{\ W}}_{\phi}\ {W_{v} = \text{\W}}_{g}\)</span> 公式19可以用如下公式表示：</p><p><span class="math inline">\(Y = \ \text{soft}\max\left( QK^{T}\right)V = Attention(Q,K,V)\ \)</span> (20)</p><p>自注意力模块为机器翻译提出的与以上为计算机视觉提出的非局部过滤操作完全相同。通常，计算机视觉自注意力模块的最终输出信号将被包装为:</p><p><span class="math inline">\(Z = YW_{Z} + X\)</span> (21)</p><p>其中Y是通过公式19产生的。如果<spanclass="math inline">\(W_{Z}\)</span>初始化为零，这个自注意力模块可以插入到任何现有的模型中，而不会破坏它的初始行为。</p><h3 id="视觉任务应用">4.5.2 视觉任务应用：</h3><p>自注意力模块被认为是卷积神经网络体系结构的一个构件，它具有与大感受野有关的低标度特性。构建模块总是用在网络的顶部，以捕捉计算机视觉任务的远程交互。接下来，我们回顾了提出的基于自注意力的图像任务方法，如图像分类、语义分割和目标检测。</p><p><strong>图像分类。</strong>用于分类的可训练注意力包括两个主流:关于使用图像区域的硬注意力[3，87，134]和生成非刚性特征图的软注意力[125，60，43，102]。Ba等人[3]首先提出了用于图像分类任务的视觉注意项，并利用注意来选择输入图像中的相关区域和位置，这也可以降低所提出模型的计算复杂度，减小输入图像的大小。AG-CNN[42]建议通过关注热点图从全局图像中裁剪出一个子区域，用于医学图像分类。SENet[54]提出了软自注意力来重新加权卷积特征的信道响应，而不是使用硬注意力和重新校准特征图的裁剪。Jetley等人[60]使用由相应的估计器生成的注意力图来重新加权深层神经网络中的中间特征。Han等[43]利用属性感知注意力来增强CNN的表征能力。</p><p><strong>语义分割。</strong>PSANet [151]、OCNet [139]、DANet[38]和CFNet[147]是第一批将自注意力模块引入语义分割任务的作品，它们考虑并增强了上下文像素之间的关系和相似性[146、74、46、89、130]。DANet[38]同时利用空间和通道维度上的自注意力模块。A2Net[20]提出将像素分组为一组区域，然后通过将区域表示与生成的注意力权重聚合来增加像素表示。为了减轻自注意力模块中计算像素相似度带来的大量参数，提出了几个工作[140，59，58，75，66]来提高自注意力模块的语义分割效率。例如，CGNL[140]应用径向基函数核函数的泰勒级数来近似像素相似性。CCNet[59]通过两个连续的交叉注意来近似原始的自注意力方案。ISSA[58]建议将密集亲和矩阵分解为两个稀疏亲和矩阵的乘积。还有其他相关的工作使用基于注意力的图形推理模块[76，21，75]来增强局部和全局表示。</p><p><strong>目标检测。</strong>Ramachandran等人[102]提出了一个基于注意力的层来建立一个完全注意力模型，它在COCO[79]基准上优于卷积神经网络。GCNet[13]发现，对于图像内的不同查询位置，由非局部操作建模的全局上下文几乎是相同的，并提出将简化公式和SENet[54]统一为全局上下文建模的通用框架[73，52，34，93]。V o.等人[124]设计了一个双向操作，从一个查询位置收集信息并将其分发到所有可能的位置。Hu等人[53]提出了一种基于自注意力的关系模块，通过一组对象的外观特征之间的相互作用来同时处理一组对象。Chengetal .提出了RelationNet++[23]，它带有一个基于注意力的解码器模块，将其他表示桥接成一个基于单一表示格式的典型对象检测器。</p><p><strong>其他视觉任务。</strong>Zhang等人[148]提出了分辨率方向的注意力模块，以学习用于精确姿态估计的增强的分辨率方向的特征图。Huang等人[57]提出了一种基于transformer的网络[56]，用于3D手-物体姿态估计。Chang等人[16]借助于基于注意力机制的特征融合块，提高了关键点检测模型的准确性并加速了其收敛。</p><h2 id="高效transformer">4.6 高效Transformer：</h2><p>尽管Transformer模型在各种任务中取得了成功，但仍然需要高内存和计算资源，这阻碍了在资源有限的设备(例如，移动电话)上的实现。在这一部分中，我们回顾了压缩和加速transformer模型以实现高效的研究，包括网络剪枝、低秩分解、知识提炼、网络量化、压缩架构设计。表4列出了一些压缩基于transformer的模型的代表性工作。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114528349.png" alt="image-20210615114528349"  /></p><h3 id="修剪和分解">4.6.1 修剪和分解:</h3><p>在基于transformer的预训练模型(如BERT)中，多个注意力操作被并行以独立地模拟不同标记之间的关系[123，29]，尽管所有的头部对于特定的任务都是必需的。Michel等人[85]凭经验观察到，在测试时，大部分注意力可以被转移，而不会显著影响性能。不同层所需的头数量各不相同，对于某些层来说，一个头就足够了。考虑到注意力头上的冗余，在[85]中定义了重要性分数来估计每个头对最终输出的影响，并且可以移除不重要的头以进行有效部署。Dalvi等人[96]从两个角度进一步分析了预训练transformer模型中的冗余，即一般冗余和特定任务冗余。遵循彩票假设等[36]，Prasanna等[96]分析了BERT中的彩票，并表明良好的子网络也存在于基于transformer的模型中。[96]中减少了FFN层和注意力头，以实现高压缩率。</p><p>除了transformer模型的宽度，深度，即层数也可以减少，以加快推理过程[32]。不同于transformer模型中不同的注意力头可以并行计算，不同的层必须顺序计算，因为下一层的输入取决于前一层的输出。Fan等人[32]提出了一种分层策略来规范模型的训练，然后在测试阶段将整个层一起移除。考虑到不同设备中的可用资源可能不同，Hou等人[51]提出自适应地减小预定义的transformer模型的宽度和深度，并且同时获得具有不同尺寸的多个模型。重要的注意力头和神经元通过一种重新布线机制在不同的子网中共享。</p><p>除了直接丢弃transformer模型中部分模块的剪枝方法之外，矩阵分解旨在基于低秩假设用多个小矩阵来逼近大矩阵。例如，Wang等人[131]在transformer模型中分解标准矩阵乘法，实现更有效的推理。</p><h3 id="知识蒸馏">4.6.2 知识蒸馏：</h3><p>知识蒸馏旨在通过从巨型教师网络转移知识来训练学生网络[48，12，2]。与教师网络相比，学生网络通常具有更薄、更浅的体系结构，更容易部署在资源有限的资源上。神经网络的输出和中间特征也可以用来将有效的信息从教师传递给学生。专注于transformer模型，Mukherjee等人[88]使用预先训练的BERT[29]作为老师来指导小模型的训练，在大量未标记数据的帮助下，王等人[127]在预先训练的教师模型中训练学生网络以模仿自注意力层的输出。值与值之间的点积作为一种新的知识形式被引入来指导学生。在[127]中还引入了一名教师助理[86]，这缩小了大型预先训练的transformer模型和紧凑的学生网络之间的差距，使模拟更加容易。考虑到transformer模型中的各种类型的层(即，自注意力层、嵌入层、预测层)，Jiao等人[62]设计了不同的目标函数来将知识从教师转移到学生。例如，学生模型嵌入层的输出是通过均方误差损失来模拟教师模型的输出。一个可学习的线性变换也被用来将不同的特征映射到同一个空间。对于预测层的输出，采用KL散度来度量不同模型之间的差异。</p><h3 id="量化">4.6.3 量化：</h3><p>量化旨在减少代表网络权重或中间特征的位数[122，137]。通用神经网络的量化方法已经得到了很好的讨论，并获得了与原始网络相当的性能[91，37，6]。最近，如何对transformer模型进行特殊量化备受关注[8，33]。Shridhar等人[112]建议将输入嵌入到二进制高维向量中，然后使用二进制输入表示来训练二进制神经网络。Cheong等人[22]通过低位(例如4位)表示来表示transformer模型中的权重。Zhao等[152]对各种量化方法进行了实证研究，表明k-means量化具有巨大发展潜力。针对机器翻译任务，Prato等人[97]提出了一种完全量化的transformer，这是第一个8位质量模型，没有任何翻译质量损失，如论文所述。</p><h3 id="紧凑结构设计">4.6.4 紧凑结构设计：</h3><p>除了将预先定义的transformer模型压缩成小模型之外，一些研究试图直接设计紧凑的模型[132，61]。Jiang等[61]提出了一种新的基于跨度的动态卷积模型，将全连通层和卷积层结合起来，简化了自注意力的计算，如图9所示。来自不同标记的表示之间的局部相关性是通过卷积运算来计算的，这比标准transformer中的密集全连接层要有效得多。深度方向的卷积也被用来进一步降低计算成本。文献[1]中提出了有趣的汉堡层，利用矩阵分解来替代原有的自注意力层。矩阵分解可以比标准的自注意力操作更有效地计算，同时很好地反映不同标记之间的依赖性。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210615114410293.png" alt="image-20210615114410293"  /></p><p>transformer模型中的自注意力操作计算给定序列(图像识别任务[31]中的块)中不同输入令牌的表示之间的点积，其复杂度为O(N)，其中N是序列的长度。最近，大量的方法集中在降低复杂度到O(N)，使transformer模型可扩展到长序列。例如，Katharopoulos等人[64]将自注意力近似为核特征映射的线性点积，并通过递归神经网络揭示标记之间的关系。Zaheer等人[143]将每个标记视为图中的一个顶点，两个标记之间的内积计算表示为一条边。受图论[113，25]的启发，各种稀疏图被组合在一起以近似transformer模型中的密集图，这也实现了O(N)复杂度。从理论的角度来看，Yun等人[141]证明了一个具有O(N)复杂度的稀疏变换器足以反映令牌之间的任何一种关系，并且可以进行泛逼近，为进一步研究具有O(N)复杂度的transformer提供了理论上的保障。</p><h1 id="结论和未来展望">5. 结论和未来展望:</h1><p>与卷积神经网络相比，Transformer因其优越的性能和巨大的潜力成为计算机视觉领域的研究热点。为了发现和利用Transformer的能力，正如调查中所总结的，近年来已经提出了许多解决方案。这些方法在广泛的视觉任务上表现出优异的性能，包括基本图像分类、高级视觉、低级视觉和视频处理。然而，用于计算机视觉的Transformer的潜力还没有被充分开发，还有几个挑战有待解决。</p><p>虽然研究人员已经提出了许多基于Transformer的模型来处理计算机视觉任务，但这些工作只是初步的解决方案，还有很大的改进空间。例如，ViT[31]中的Transformer架构遵循NLP[123]的标准Transformer。专门针对CV的改进版还有待探索。此外，Transformer在上述任务之外的更多任务上的应用也是必需的。</p><p>此外，大多数现有的可视化Transformer模型都是为处理单一任务而设计的。许多NLP模型，如GPT-3[10]，已经显示了Transformer在一个模型中处理多个任务的能力。CV领域的IPT[17]还能够处理多个低水平视觉任务，如超分辨率、图像去噪和去雾。我们认为，更多的任务只能在一个模型中涉及。</p><p>最后，同样重要的是，为CV开发高效的Transformer模型也是一个公开的问题。Transformer模型通常很大，计算量也很大，例如，基本的ViT模型[31]需要18B的浮点运算来处理图像。相比之下，轻量级CNN模型GhostNet[44，45]仅用约600MFLOPs就能实现类似的性能。虽然已经提出了几种压缩Transformer的方法，但是它们的复杂性仍然很大。而这些原本是为NLP设计的方法，可能并不适合CV。因此，高效的Transformer模型是在资源有限的设备上部署可视化Transformer的代理。</p><h1 id="参考文献">6. 参考文献：</h1><p>[1] Anonymous. Is attention better than matrix decomposition?InSubmitted to International Conference on Learning Representations, 2021.under review. 15</p><p>[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep?Advances in neural information processing systems,27:2654–2662, 2014.14</p><p>[3] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.Multiple objectrecognition with visual attention. In International Conference onLearning Representations, 2014.13</p><p>[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layernormalization. arXiv preprint arXiv:1607.06450,2016. 4</p><p>[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machinetranslation by jointly learning to align and translate. arXiv preprintarXiv:1409.0473, 2014. 1</p><p>[6] Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant:Quantizedneural networks via proximal operators. arXiv preprint arXiv:1810.00861,2018. 14</p><p>[7] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrainedlanguage model for scientific text. arXiv preprint arXiv:1903.10676,2019. 5</p><p>[8] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,Vivek Menon,Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bitquantization of transformer neural machine language translation model.arXiv preprint arXiv:1906.00532, 2019. 14</p><p>[9] Manjot Bilkhu, Siyang Wang, and Tushar Dobhal. Attention is allyou need for videos: Self-attention based video summarization usinguniversal transformers. arXiv preprint arXiv:1906.02792, 2019. 12</p><p>[10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, GirishSastry, Amanda Askell, et al.Language models are few-shot learners.arXiv preprint arXiv:2005.14165, 2020. 1, 5, 15</p><p>[11] Antoni Buades, Bartomeu Coll, and J-M Morel. A nonlocalalgorithm for image denoising. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 60–65, 2005. 12</p><p>[12] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil.Model compression. In Proceedings of the 12th ACM SIGKDD internationalconference on Knowledge discovery and data mining, pages 535–541, 2006.14</p><p>[13] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet:Non-local networks meet squeeze-excitation networks and beyond. InProceedings of the IEEE International Conference on Computer VisionWorkshops, 2019.13</p><p>[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko.End-to-end objectdetection with transformers. arXiv preprint arXiv:2005.12872, 2020.2</p><p>[15] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko.End-to-end objectdetection with transformers. In ECCV,2020. 7, 8, 9</p><p>[16] Yuan Chang, Zixuan Huang, and Qiwei Shen. The same size dilatedattention network for keypoint detection. In International Conference onArtificial Neural Networks,pages 471–483, 2019. 13</p><p>[17] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng,Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,and Wen Gao. Pre-trainedimage processing transformer.arXiv preprint arXiv:2012.00364, 2020. 2,10, 15</p><p>[18] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun,David Luan, and Ilya Sutskever. Generative pretraining from pixels. InInternational Conference on Machine Learning, pages 1691–1703. PMLR,2020. 1, 2, 6,8</p><p>[19] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Memory enhancedglobal-local aggregation for video object detection. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 10337–10346,2020. 11</p><p>[20] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, andJiashi Feng. A2-nets: Double attention networks.Advances in neuralinformation processing systems, pages352–361, 2018. 13</p><p>[21] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng,Jiashi Feng, and Yannis Kalantidis. Graphbased global reasoningnetworks. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pages 433–442, 2019. 13</p><p>[22] Robin Cheong and Robel Daniel. transformers. zip: Compressingtransformers with pruning and quantization. Technical report, tech.rep., Stanford University, Stanford, California, 2019. 14</p><p>[23] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++:Bridgingvisual representations for object detection via transformer decoder.Advances in Neural Information Processing Systems, 2020. 7, 13</p><p>[24] Yung-Sung Chuang, Chi-Liang Liu, and Hung-Yi Lee.Speechbert:Cross-modal pre-trained language model for end-to-end spoken questionanswering. arXiv preprint arXiv:1910.11559, 2019. 5</p><p>[25] Fan Chung and Linyuan Lu. The average distances in random graphswith given expected degrees. Proceedings of the National Academy ofSciences, 99(25):15879–15882,2002. 15</p><p>[26] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and YoshuaBengio. Empirical evaluation of gated recurrent neural networks onsequence modeling. arXiv preprint arXiv:1412.3555, 2014. 4</p><p>[27] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher DManning. Electra: Pre-training text encoders as discriminators ratherthan generators. arXiv preprint arXiv:2003.10555, 2020. 5</p><p>[28] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.UP-DETR:unsupervised pre-training for object detection with transformers. arXivpreprint arXiv:2011.09094, 2020.2, 9</p><p>[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectional transformers forlanguage understanding. In NAACL-HLT (1),2019. 1, 4, 5, 12, 13, 14</p><p>[30] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang,Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language modelpre-training for natural language understanding and generation. InAdvances in Neural Information Processing Systems, pages13063–13075,2019. 5</p><p>[31] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,DirkWeissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, MatthiasMinderer, Georg Heigold,Sylvain Gelly, et al. An image is worth 16x16words:Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. 1, 2, 6, 8, 15</p><p>[32] Angela Fan, Edouard Grave, and Armand Joulin. Reducingtransformer depth on demand with structured dropout.arXiv preprintarXiv:1909.11556, 2019. 14</p><p>[33] Chaofei Fan. Quantized transformer. Technical report,Technicalreport, Stanford University, Stanford, California,2019. URL https . . .. 14</p><p>[34] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai.Few-shotobject detection with attention-rpn and multirelation detector. InProceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 4013–4022, 2020. 13</p><p>[35] Robert B Fisher. Cvonline: The evolving, distributed,nonproprietary, on-line compendium of computer vision. Re-trievedJanuary 28, 2006 from http://homepages. inf. ed. ac.uk/rbf/CVonline,2008. 2</p><p>[36] Jonathan Frankle and Michael Carbin. The lottery tickethypothesis: Finding sparse, trainable neural networks. arXiv preprintarXiv:1803.03635, 2018. 14</p><p>[37] Joshua Fromm, Meghan Cowan, Matthai Philipose, Luis Ceze, andShwetak Patel. Riptide: Fast end-to-end binarized neural networks.Proceedings of Machine Learning and Systems, 2:379–389, 2020. 14</p><p>[38] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, ZhiweiFang, and Hanqing Lu. Dual attention network for scene segmentation. InProceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 3146–3154, 2019. 13</p><p>[39] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.Multi-modal transformer for video retrieval. In European Conference onComputer Vision (ECCV), pages 214–229, 2020. 11</p><p>[40] Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees GMSnoek. Actor-transformers for group activity recognition. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 839–848, 2020. 11</p><p>[41] Rohit Girdhar, Joao Carreira, Carl Doersch, and AndrewZisserman. Video action transformer network. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pages 244–253,2019. 11</p><p>[42] Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, LiangZheng, and Yi Yang. Diagnose like a radiologist:Attention guidedconvolutional neural network for thorax disease classification. In arXivpreprint arXiv:1801.09927,2018. 13</p><p>[43] Kai Han, Jianyuan Guo, Chao Zhang, and MingjianZhu.Attribute-aware attention model for fine-grained representationlearning. In Proceedings of the 26th ACM international conference onMultimedia, pages 2040–2048, 2018.13</p><p>[44] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, andChang Xu. Ghostnet: More features from cheap operations. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1580–1589, 2020. 15</p><p>[45] Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, andTong Zhang. Model rubik’s cube: Twisting resolution, depth and width fortinynets. Advances in Neural Information Processing Systems, 33, 2020.15</p><p>[46] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao.Adaptive pyramid context network for semantic segmentation. InProceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 7519–7528, 2019. 13</p><p>[47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deepresidual learning for image recognition. pages 770–778, 2016. 1</p><p>[48] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling theknowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.14</p><p>[49] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.Neural computation, 9(8):1735–1780, 1997. 1</p><p>[50] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.Neural computation, 9(8):1735–1780, 1997. 4</p><p>[51] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and QunLiu. Dynabert: Dynamic bert with adaptive width and depth. Advances inNeural Information Processing Systems, 33, 2020. 14</p><p>[52] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and TyngLuh Liu.One-shot object detection with co-attention and co-excitation. InAdvances in Neural Information Processing Systems, pages 2725–2734,2019. 13</p><p>[53] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei.Relation networks for object detection. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pages 3588–3597,2018. 13</p><p>[54] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 7132–7141, 2018. 13</p><p>[55] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert:Modeling clinical notes and predicting hospital readmission. arXivpreprint arXiv:1904.05342, 2019. 5</p><p>[56] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan.Handtransformer: Non-autoregressive structured modeling for 3d hand poseestimation. In European Conference on Computer Vision, pages 17–33,2020. 13</p><p>[57] Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and JunsongYuan. Hot-net: Non-autoregressive transformer for 3d hand-object poseestimation. In Proceedings of the 28th ACM International Conference onMultimedia, pages 3136–3145, 2020. 13</p><p>[58] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang,Xilin Chen, andJingdong Wang. Interlaced sparse self-attention for semanticsegmentation. arXiv preprint arXiv:1907.12273, 2019. 13</p><p>[59] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, YunchaoWei, and Wenyu Liu. Ccnet: Criss-cross attention for semanticsegmentation. In Proceedings of the IEEE International Conference onComputer Vision, pages 603–612, 2019. 13</p><p>[60] Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip HS Torr.Learn to pay attention. In International Conference on LearningRepresentations, 2018. 13</p><p>[61] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, JiashiFeng, and Shuicheng Yan. Convbert: Improving bert with span-baseddynamic convolution. Advances in Neural Information Processing Systems,33, 2020. 2, 14, 15</p><p>[62] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for naturallanguage understanding. In Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 4163–4174, Nov. 2020. 2, 13, 14</p><p>[63] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,LukeZettlemoyer, and Omer Levy. Spanbert: Improving pre-training byrepresenting and predicting spans. Transactions of the Association forComputational Linguistics,8:64–77, 2020. 4</p><p>[64] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,andFranc¸ois Fleuret. Transformers are rnns: Fast autoregressivetransformers with linear attention. In International Conference onMachine Learning, pages 5156–5165.PMLR, 2020. 15</p><p>[65] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenetclassification with deep convolutional neural networks. In NeurIPS,pages 1097–1105, 2012. 1</p><p>[66] Saumya Kumaar, Ye Lyu, Francesco Nex, and Michael Ying Yang.Cabinet: Efficient context aggregation network for low-latency semanticsegmentation.arXiv preprint arXiv:2011.00993, 2020. 13</p><p>[67] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervisedlearning of language representations. arXiv preprint arXiv:1909.11942,2019. 13</p><p>[68] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.Gradient-based learning applied to document recognition. Proceedings ofthe IEEE, 86(11):2278–2324,1998. 1</p><p>[69] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, SunkyuKim, Chan Ho So, and Jaewoo Kang. Biobert:a pre-trained biomedicallanguage representation model for biomedical text mining.Bioinformatics, 36(4):1234–1240,2020. 5</p><p>[70] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.Bart: Denoising sequence-to-sequence pre-training for natural languagegeneration, translation, and comprehension. arXiv preprintarXiv:1910.13461, 2019. 5</p><p>[71] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. Entangledtransformer for image captioning. In 2019 IEEE/CVF InternationalConference on Computer Vision (ICCV), pages 8928–8937, 2019. 12</p><p>[72] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,andKai-Wei Chang. Visualbert: A simple and performant baseline for visionand language. arXiv preprint arXiv:1908.03557, 2019. 5</p><p>[73] Wei Li, Kai Liu, Lizhe Zhang, and Fei Cheng. Object detectionbased on an adaptive attention mechanism. Scientific Reports, pages1–13, 2020. 13</p><p>[74] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang,Zhouchen Lin, andHong Liu. Expectation-maximization attention networks for semanticsegmentation. In Proceedings of the IEEE International Conference onComputer Vision, pages 9167–9176, 2019. 13</p><p>[75] Yin Li and Abhinav Gupta. Beyond grids: Learning graphrepresentations for visual recognition. Advances in Neural InformationProcessing Systems, pages 9225–9235, 2018.13</p><p>[76] Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and Eric PXing. Symbolic graph reasoning meets convolutions.Advances in NeuralInformation Processing Systems, pages 1853–1863, 2018. 13</p><p>[77] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,BharathHariharan, and Serge Belongie. Feature pyramid networks for objectdetection. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 2117–2125, 2017. 7</p><p>[78] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,and PiotrDollar. Focal loss for dense object detection. In ICCV, 2017. 13</p><p>[79] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, PietroPerona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoftcoco: Common objects in context. In European conference on computervision, pages 740–755, 2014. 13</p><p>[80] Hao Liu, Jiwen Lu, Jianjiang Feng, and Jie Zhou. Twostreamtransformer networks for video-based face alignment. IEEE transactionson pattern analysis and machine intelligence, 40(11):2546–2554, 2017.11</p><p>[81] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. Endto-endlane shape prediction with transformers. In WACV,2021. 7, 9</p><p>[82] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and VeselinStoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019. 4, 5</p><p>[83] Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, ChunguoLi, and Luxi Yang. Convtransformer: A convolutional transformer networkfor video frame synthesis. arXiv preprint arXiv:2011.10185, 2020. 11</p><p>[84] Suhas Lohit, Qiao Wang, and Pavan Turaga. Temporal transformernetworks: Joint learning of invariant and discriminative time warping.In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR),pages 12426–12435, 2019. 11</p><p>[85] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen headsreally better than one? In Advances in Neural Information ProcessingSystems, pages 14014–14024, 2019. 2,14</p><p>[86] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine,Akihiro Matsukawa, and Hassan Ghasemzadeh.Improved knowledgedistillation via teacher assistant. In Proceedings of the AAAIConference on Artificial Intelligence, volume 34, pages 5191–5198, 2020.14</p><p>[87] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrentmodels of visual attention. Advances in neural information processingsystems, pages 2204–2212, 2014. 13</p><p>[88] Subhabrata Mukherjee and Ahmed Hassan Awadallah.Xtremedistil:Multi-stage distillation for massive multilingual models. In Proceedingsof the 58th Annual Meeting of the Association for ComputationalLinguistics, pages 2221–2234, 2020. 14</p><p>[89] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee,MattiasHeinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils YHammerla, Bernhard Kainz, et al. Attention u-net: Learning where to lookfor the pancreas. arXiv preprint arXiv:1804.03999, 2018. 13</p><p>[90] Ankur Parikh, Oscar Tackstr om, Dipanjan Das, and JakobUszkoreit. A decomposable attention model for natural languageinference. In Proceedings of the 2016 Conference on Empirical Methods inNatural Language Processing, pages 2249–2255, 2016. 1</p><p>[91] Eunhyeok Park and Sungjoo Yoo. Profit: A novel training methodfor sub-4-bit mobilenet models. In European Conference on ComputerVision, pages 430–446. Springer,2020. 14</p><p>[92] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser,Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. ICML,2018. 2, 9, 10</p><p>[93] Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier,and Maguelonne Heritier. Spotnet: Self-attention multi-task network forobject detection. In 2020 17th Conference on Computer and Robot Vision(CRV), pages 230–237, 2020. 13</p><p>[94] Matthew E Peters, Mark Neumann, Robert L Logan IV,Roy Schwartz,Vidur Joshi, Sameer Singh, and Noah A Smith. Knowledge enhancedcontextual word representations. arXiv preprint arXiv:1909.04164, 2019.5</p><p>[95] Tim Prangemeier, Christoph Reich, and HeinzKoeppl.Attention-based transformers for instance segmentation of cellsin microstructures. arXiv preprint arXiv:2011.09763,2020. 9</p><p>[96] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When bert playsthe lottery, all tickets are winning. arXiv preprint arXiv:2005.00561,2020. 14</p><p>[97] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fullyquantized transformer for machine translation. In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing:Findings, pages 1–14, 2020. 2, 14</p><p>[98] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, andXuanjing Huang. Pre-trained models for natural language processing: Asurvey. arXiv preprint arXiv:2003.08271, 2020. 5, 13</p><p>[99] Alec Radford, Karthik Narasimhan, Tim Salimans, and IlyaSutskever. Improving language understanding by generative pre-training,2018. 5</p><p>[100] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,Dario Amodei,and Ilya Sutskever. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9,2019. 5, 6</p><p>[101] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, SharanNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploringthe limits of transfer learning with a unified text-to-text transformer.arXiv preprint arXiv:1910.10683, 2019. 5</p><p>[102] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello,Anselm Levskaya, and Jonathon Shlens. Standalone self-attention invision models. arXiv preprint arXiv:1906.05909, 2019. 13</p><p>[103] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Fasterr-cnn: Towards real-time object detection with region proposal networks.In Advances in neural information processing systems, pages 91–99, 2015.1</p><p>[104] Frank Rosenblatt. The perceptron, a perceiving and recognizingautomaton Project Para. Cornell Aeronautical Laboratory, 1957. 1</p><p>[105] FRANK ROSENBLATT. Principles of neurodynamics.perceptrons andthe theory of brain mechanisms. Technical report, CORNELL AERONAUTICALLAB INC BUFFALO NY, 1961. 1</p><p>[106] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.Learning internal representations by error propagation. Technicalreport, California Univ San Diego La Jolla Inst for Cognitive Science,1985. 1</p><p>[107] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.Distilbert, a distilled version of bert: smaller, faster, cheaper andlighter. arXiv preprint arXiv:1910.01108, 2019. 13</p><p>[108] Kara Marie Schatz, Erik Quintanilla, Shruti Vyas, and YogeshSingh Rawat. A recurrent transformer network for novel view actionsynthesis. In ECCV (27), pages 410–426, 2020. 11</p><p>[109] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Video multitasktransformer network. In Proceedings of the IEEE International Conferenceon Computer Vision Workshops,pages 0–0, 2019. 11</p><p>[110] Jie Shao, Xin Wen, Bingchen Zhao, and Xiangyang Xue. Temporalcontext aggregation for video retrieval with contrastive learning.11</p><p>[111] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, AmirGholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian basedultra low precision quantization of bert. In AAAI, pages 8815–8821,2020. 13</p><p>[112] Kumar Shridhar, Harshil Jain, Akshat Agarwal, and Denis Kleyko.End to end binarized neural networks for text classification. InProceedings of SustaiNLP: Workshop on Simple and Efficient NaturalLanguage Processing, pages 29–34, 2020. 14</p><p>[113] Daniel A Spielman and Shang-Hua Teng. Spectral sparsificationof graphs. SIAM Journal on Computing, 40(4):981–1025, 2011. 15</p><p>[114] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,Furu Wei, andJifeng Dai. Vl-bert: Pre-training of generic visual-linguisticrepresentations. arXiv preprint arXiv:1908.08530, 2019. 5</p><p>[115] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,andCordelia Schmid. Videobert: A joint model for video and languagerepresentation learning. In Proceedings of the IEEE InternationalConference on Computer Vision, pages 7464–7473, 2019. 5</p><p>[116] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patientknowledge distillation for bert model compression. arXiv preprintarXiv:1908.09355, 2019. 13</p><p>[117] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.Rethinking transformer-based set prediction for object detection. arXivpreprint arXiv:2011.10881, 2020. 2, 9</p><p>[118] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang,and Denny Zhou. Mobilebert: a compact taskagnostic bert forresource-limited devices. arXiv preprint arXiv:2004.02984, 2020. 13</p><p>[119] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, ClaudineBadue, Alberto F De Souza, and Thiago OliveiraSantos. Polylanenet: Laneestimation via deep polynomial regression. arXiv preprintarXiv:2004.10924, 2020. 9</p><p>[120] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.Well-read students learn better: The impact of student initialization onknowledge distillation. arXiv preprint arXiv:1908.08962, 2019. 13</p><p>[121] Shimon Ullman et al. High-level vision: Object recognition andvisual cognition, volume 2. MIT press Cambridge, MA, 1996. 2</p><p>[122] Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving thespeed of neural networks on cpus. 2011. 14</p><p>[123] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,Llion Jones, Aidan N Gomez, Łukasz Kaiser,and Illia Polosukhin.Attention is all you need. Advances in neural information processingsystems, 30:5998–6008, 2017. 1, 2, 3, 4, 9, 12, 14, 15</p><p>[124] Xuan-Thuy Vo, Lihua Wen, Tien-Dat Tran, and Kang-Hyun Jo.Bidirectional non-local networks for object detection.In InternationalConference on Computational Collective Intelligence, pages 491–501,2020. 13</p><p>[125] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li,Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.Residual attentionnetwork for image classification. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 3156–3164, 2017.13</p><p>[126] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille,andLiang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation withmask transformers. arXiv preprint arXiv:2012.00759, 2020. 2, 7, 9</p><p>[127] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and MingZhou. Minilm: Deep self-attention distillation for task-agnosticcompression of pre-trained transformers.arXiv preprint arXiv:2002.10957,2020. 14</p><p>[128] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.Non-local neural networks. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 7794–7803, 2018. 12</p><p>[129] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, BaoshanCheng, Hao Shen, and Huaxia Xia. End-toend video instance segmentationwith transformers. arXiv preprint arXiv:2011.14503, 2020. 2, 7, 9</p><p>[130] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen.Self-supervised equivariant attention mechanism for weakly supervisedsemantic segmentation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 12275–12284, 2020. 13</p><p>[131] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruningof large language models. arXiv preprint arXiv:1910.04732, 2019. 14</p><p>[132] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Litetransformer with long-short range attention.arXiv preprintarXiv:2004.11886, 2020. 15</p><p>[133] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.Bert-of-theseus: Compressing bert by progressive module replacing. arXivpreprint arXiv:2002.02925, 2020. 13</p><p>[134] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, AaronCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show,attend and tell: Neural image caption generation with visual attention.In International conference on machine learning, pages 2048–2057, 2015.13</p><p>[135] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and BainingGuo. Learning texture transformer network for image super-resolution. InProceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition,pages 5791–5800, 2020. 2, 10</p><p>[136] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,Russ RSalakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressivepretraining for language understanding. In Advances in neuralinformation processing systems, pages 5753–5763, 2019. 5</p><p>[137] Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu,Dacheng Tao, and Chang Xu. Searching for lowbit weights in quantizedneural networks. arXiv preprint arXiv:2009.08695, 2020. 14</p><p>[138] Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and RuigangYang. Lidar-based online 3d video object detection with graph-basedmessage passing and spatiotemporal transformer attention. In 2020IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 11495–11504, 2020. 11</p><p>[139] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network forscene parsing. arXiv preprint arXiv:1809.00916,2018. 13</p><p>[140] Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, andFuxin Xu. Compact generalized non-local network. In Advances in NeuralInformation Processing Systems, pages 6510–6519, 2018. 13</p><p>[141] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli,Ankit SinghRawat, Sashank J Reddi, and Sanjiv Kumar. o(n) connections areexpressive enough: Universal approximability of sparse transformers.arXiv preprint arXiv:2006.04862, 2020. 15</p><p>[142] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.13</p><p>[143] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie,Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, QifanWang, Li Yang, et al. Big bird: Transformers for longer sequences. arXivpreprint arXiv:2007.14062, 2020. 15</p><p>[144] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning jointspatial-temporal transformations for video inpainting. In EuropeanConference on Computer Vision, pages 528–543. Springer, 2020. 2, 12</p><p>[145] Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang,XianshengHua, and Qianru Sun. Feature pyramid transformer. In ECCV, 2020. 7</p><p>[146] Fan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu,Feifei Ma, Junyu Han, and Errui Ding. Acfnet: Attentional class featurenetwork for semantic segmentation. In Proceedings of the IEEEInternational Conference on Computer Vision, pages 6798–6807, 2019.13</p><p>[147] Hang Zhang, Han Zhang, Chenguang Wang, and Junyuan Xie.Co-occurrent features in semantic segmentation. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, pages548–557, 2019. 13</p><p>[149] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,Maosong Sun, andQun Liu. Ernie: Enhanced language representation with informativeentities. arXiv preprint arXiv:1905.07129, 2019. 5</p><p>[150] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and VladlenKoltun. Point transformer. arXiv preprint arXiv:2012.09164, 2020. 9</p><p>[151] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen ChangeLoy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attentionnetwork for scene parsing. In Proceedings of the European Conference onComputer Vision (ECCV), pages 267–283, 2018. 13</p><p>[152] Zihan Zhao, Yuncong Liu, Lu Chen, Qi Liu, Rao Ma, and Kai Yu.An investigation on different underlying quantization schemes forpre-trained language models. In CCF International Conference on NaturalLanguage Processing and Chinese Computing, pages 359–371. Springer,2020.14</p><p>[153] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and HaoDong. End-to-end object detection with adaptive clustering transformer.arXiv preprint arXiv:2011.09315, 2020. 2, 8</p><p>[154] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, andCaiming Xiong. End-to-end dense video captioning with maskedtransformer. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8739–8748, 2018. 2, 12</p><p>[155] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, andJifeng Dai. Deformable detr: Deformable transformers for end-to-endobject detection. arXiv preprint arXiv:2010.04159, 2020. 2, 7, 8</p><p>[156] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, RaquelUrtasun, Antonio Torralba, and Sanja Fidler.Aligning books and movies:Towards story-like visual explanations by watching movies and readingbooks. In Proceedings of the IEEE international conference on computervision, pages 19–27, 2015. 4</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vision Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>conda使用</title>
      <link href="post/d6d0.html"/>
      <url>post/d6d0.html</url>
      
        <content type="html"><![CDATA[<h1 id="conda基本配置">1. conda基本配置：</h1><h2 id="配置镜像源">1.1 配置镜像源：</h2><p>首先需要配置一个清华的镜像源，用来解决下载速度慢的问题</p><pre class="shell"><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes</code></pre><p>上面命令执行完之后会在C:.condarc文件，打开会发现刚才的配置信息已经有了。</p><p>也可以直接在该文件里面配置，比如继续添加几个镜像源：</p><pre class="shell"><code>channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/   - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/  - defaultsshow_channel_urls: true</code></pre><span id="more"></span><h1 id="conda常用命令">2. conda常用命令：</h1><p>在windows下找到Prompt打开进入conda环境，默认一开始进入base环境下</p><h2 id="创建虚拟环境">2.1 创建虚拟环境：</h2><p>创建一个虚拟环境名叫yore，使用python3.9解释器的虚拟环境-n是虚拟环境名字，python = 3.9是指定特定版本</p><pre class="shell"><code>conda create -n yore python=3.9</code></pre><h2 id="激活退出虚拟环境">2.2 激活、退出虚拟环境：</h2><pre class="shell"><code># 激活或者说进入指定的虚拟环境conda activate yore# 从当前虚拟环境退出，到base下conda deactivate</code></pre><h2 id="查看虚拟环境列表">2.3 查看虚拟环境列表：</h2><pre class="shell"><code>conda env list</code></pre><h2 id="删除虚拟环境">2.4 删除虚拟环境：</h2><pre class="shell"><code>conda remove -n yore --all</code></pre><h2 id="复制虚拟环境">2.5 复制虚拟环境：</h2><p>将已存在的name1虚拟环境复制一份 命名为name2</p><pre class="shell"><code>conda create -n name2 --clone name1</code></pre><h2 id="包安装命令">2.6 包安装命令：</h2><pre class="shell"><code>#1. 升级全部库conda upgrade --all#2. 升级一个包conda update packagename#3. 安装程序包conda install packagename#4. 安装指定版本的程序包conda install numpy=1.19#5. 移除程序包conda remove packagename#6. 查看所有包conda list</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制</title>
      <link href="post/d0b.html"/>
      <url>post/d0b.html</url>
      
        <content type="html"><![CDATA[<p>最近在看注意力机制，开始是从Google 2017年的那篇Attention is all youneed的论文读起，在大概了解了自注意力机制以后，还是有很多疑问，所以就查阅了很多资料，试图梳理一下注意力机制的发展脉络。下面列出的参考资料都是我认为很不错的内容，统一在开篇列出。本文并不涉及太多深入的讲解，更多的是想从宏观的角度出发，对注意力机制的发展过程做一个简要的梳理，比如每一个阶段遇到什么问题？是如何解决的？解决的方案本身又存在什么问题？正是这些问题推动了它的进一步的发展。</p><span id="more"></span><p>参考资料：</p><ul><li><p>论文：</p><p>① [attention is all you need] https://arxiv.org/abs/1706.03762</p><p>② [A Survey on Visual Transformer]https://arxiv.org/abs/2012.12556</p><p>③ [Attention Mechanism. 2015.ICLR]https://arxiv.org/pdf/1409.0473.pdf</p></li><li><p>文章：</p><p>① https://mp.weixin.qq.com/s/hn4EMcVJuBSjfGxJ_qM3Tw</p><p>② https://samaelchen.github.io/deep_learning_step6/</p><p>③ https://www.jianshu.com/p/b2b95f945a98</p><p>④ https://zhuanlan.zhihu.com/p/50915723</p><p>⑤ https://blog.csdn.net/qq_32241189/article/details/81591456</p><p>⑥ https://blog.csdn.net/u014595019/article/details/52826423</p><p>⑦ http://jalammar.github.io/illustrated-transformer/</p><p>⑧https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#self-attention</p><p>⑨ https://samaelchen.github.io/deep_learning_step6/</p></li><li><p>视频：</p><p>① https://www.bilibili.com/video/BV1Wv411h7kN?p=23</p><p>② https://www.bilibili.com/video/BV1Wv411h7kN?p=24</p><p>③ https://www.bilibili.com/video/BV1Wv411h7kN?p=27</p></li></ul><h1 id="background">1. Background:</h1><p>对于卷积神经网络CNN来说，不论是图像分类、目标识别或者其他任务，每个输入的数据都是独立的，不同输入之间是没有联系的。但是在某些场景中，我们的数据就可能是有相互依赖关系的，这就是序列数据，比如在机器翻译中，语音识别或者视频处理等任务中，数据的前后会存在一定的依赖关系。​<br />​针对这种有上下文依赖关系的数据，提出了RNN这种网络结构，RNN的核心思想即是将数据按时间轴展开，每一时刻数据均对应相同的神经单元，且上一时刻的结果能传递至下一时刻。至此便解决了输入输出变长且存在上下文依赖的问题。</p><h2 id="rnn">1.1 RNN:</h2><p>最简单的RNN示例图：</p><p>其网络结构本质上是一个小的全连接循环神经网络，它的训练过程与一般的DNN网络训练方法类似，采用BP算法完成权重参数更新。而且需要注意的是对于RNN网络来说，不同输入的对应的参数是共享的。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210512161549727.png" alt="image-20210512161549727"/></p><h2 id="deep-rnn">1.2 Deep RNN:</h2><p>深层循环神经网络(DeepRNN)是循环神经网络的一种变种。为了增强模型的表达能力，可以在网络中设置多个循环层，将每层循环网络的输出传给下一层进行处理。对于普通的RNN，每一时刻的输入到输出之间只有一个全连接层，即在输入到输出的路径上是一个很浅的神经网络，因此从输入中提取抽象信息的能力将受到限制。而对于DeepRNN如下图所示，在每一时刻的输入到输出之间有多个循环体，网络因此可以从输入中抽取更加高层的信息。和卷积神经网络类似，每一层的循环体中参数是一致的，而不同层中的参数可以不同。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210511195311618.png" alt="image-20210511195311618" style="zoom:50%;" /></p><h2 id="bidirectional-rnn">1.3 Bidirectional RNN:</h2><p>对于一般的RNN网络，在t时刻的输出是考虑前面t-1个时刻的输入以及当前时刻的输入<spanclass="math display">\[x_t\]</span>确定的。而在某些应用中，例如语音识别，由于协同发音和词与词之间的语义依赖，当前声音作为音素的正确解释可能取决于未来几个音素。因此，<strong>网络当前时刻t输出的预测值<spanclass="math display">\[y_t\]</span>可能依赖整个输入序列</strong>，所以就出现了双向RNN这个结构。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210511195534052.png" alt="image-20210511195534052" style="zoom:50%;" /></p><p>相比于单向的RNN，BidirectionalRNN的每一个输出都考虑了整个sequence的信息。</p><h2 id="rnn的不足">1.4 RNN的不足：</h2><p>虽然RNN对于序列数据给出了较好的解决方案，然而对于较长的序列输入，为解决<strong>长期依赖问题</strong>，我们一般需要较深的神经网络，但是同一般的深度网络一样，RNN也存在优化困难的问题，即<strong>训练时间长</strong>，同时，在方向传播过程中，随着网络层数的加深，也会出现<strong>梯度消失和梯度爆炸的问题</strong>。对于梯度消失问题，由于相互作用的梯度呈指数减少，因此长期依赖信号将会变得非常微弱，而容易受到短期信号波动的影响，对此可行的解决方法包括网络跨层连接、ESN、回声状态网络或引入leakyunit。而对于梯度爆炸问题，我们一般采取梯度截断的方法。 ​RNN非常擅长处理输入是序列数据的问题，但是它有一个问题就是<strong>不能并行化计算</strong>。</p><p>​ 我在这里对长期依赖问题做了一个简单的推导，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210512161549727.png" alt="image-20210512161549727" style="zoom: 67%;" /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210512164932035.png" alt="image-20210512164932035" style="zoom: 67%;" /></p><p>通过链式求导的计算过程就可以发现，越是早期的输入，越是会累积更多的梯度相乘，这种连乘就会导致早期的输入对当前时刻产生的影响越微弱。</p><h2 id="lstm">1.5 LSTM：</h2><p>针对长期依赖的问题，LSTM采用了一种新的网络结构，每一个LSTM单元不仅接受此时刻的输入数据<span class="math display">\[x_t\]</span>和上一时刻的状态信息<spanclass="math display">\[y_{t-1}\]</span>，其还需建立一个机制能保留前面远处结点的重要信息不会被丢失。通过设计“门”结构实现保留信息和选择信息功能（遗忘门、输入门），其实从宏观上来理解就是通过这种机制让网络遗忘以前不重要的信息，而能够保留之前重要的信息。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210512170237998.png" alt="image-20210512170237998" style="zoom:50%;" /></p><h2 id="gru">1.6 GRU:</h2><p>LSTM结构比较复杂，所以研究人员也在考虑如何简化它，GRU与LSTM最大的区别是其将输入门和遗忘门合并为更新门（更新门决定隐状态保留放弃部分）</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/G5lM5EChnq-compress.jpg" alt="G5lM5EChnq-compress" style="zoom:50%;" /></p><h2 id="encoder-decoder框架">1.7 Encoder-Decoder框架:</h2><p>在上面的讨论中，我们均考虑的是输入输出序列等长的问题，然而在实际中却大量存在输入输出序列长度不等的情况，如机器翻译、语音识别、问答系统等。这时我们便需要设计一种映射可变长序列至另一个可变长序列的RNN网络结构---Encoder-Decoder框架。</p><p>简单来说，一个RNN解决不了的问题，我们用两个RNN试一试，当然这里并不是说必须要用RNN,还可以有基于CNN、基于Transformer的Seq2Seq模型。</p><p>Encoder-Decoder框架是机器翻译（MachineTranslation）模型的产物，其于2014年Cho etal.在Seq2Seq循环神经网络中首次提出。Seq2Seq模型的<strong>基本思想</strong>非常简单一一使用一个循环神经网络读取输入句子，将整个句子的信息<strong>压缩</strong>到一个固定维度（注意是<strong>固定维度</strong>，下文的注意力集中机制将在此做文章）的编码中；再使用另一个循环神经网络读取这个编码，将其“解压”为目标语言的一个句子。这两个循环神经网络分别称为编码器（Encoder）和解码器（Decoder）。</p><p>这只是大概的思想，具体实现的时候，编码器和解码器都不是固定的,可选的有CNN/RNN/BiRNN/GRU/LSTM等等，你可以自由组合。比如说，你在编码时使用BiRNN,解码时使用RNN，或者在编码时使用RNN,解码时使用LSTM等等</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/LfxAJmqSs1-compress.jpg" alt="LfxAJmqSs1-compress" style="zoom:50%;" /></p><p>针对Encoder-Decoder框架，我们在网络中间计算出的语义表示是一个固定长度的向量，那么对于不同长度的输入数据，这个固定长度的向量是否都能够很好的表示每个输入的语义信息呢？答案是不能。Encoder-Decoder框架在输入长度比较大的序列数据，会出现性能显著下降的问题。<strong>其实这就是因为用固定长度的向量去概括长句子的所有语义细节十分困难</strong>。</p><p>为了克服这一个问题，发表在ICLR 2015的论文[NEURAL MACHINE TRANSLATIONBY JOINTLY LEARNING TO ALIGN ANDTRANSLATE]提出了自适应的选择编码向量的部分相关语义细节片段进行解码翻译的方案。也就是首次提出了<strong>注意力机制</strong>。</p><p>2014年Sutskever等人提出seq2seq模型（Encoder-Decoder框架）首次实现了End-to-End的机器翻译。2015年，Bahdandu等人发明了注意力机制缓解了长句子性能急剧下降的问题，并将其简单的应用于多种语言的翻译任务中。</p><h1 id="attention-mechanism">2. Attention Mechanism:</h1><p><strong>AttentionMechanism最早引入至自然语言中是为解决机器翻译中随句子长度（超过50）增加其性能显著下降的问题</strong>，现已广泛应用于各类序列数据的处理中。</p><p>相比于之前的encoder-decoder模型，attention模型最大的区别就在于它不在要求编码器将所有输入信息都编码进一个固定长度的向量之中。相反，此时编码器需要将输入编码成一个向量的序列，而在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行进一步处理。这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210512204629225.png" alt="image-20210512204629225" style="zoom:50%;" /></p><p>注意力机制的主要亮点在于对于seq2seq模型中编码器将整个句子压缩为一个固定长度的向量c，而当句子较长时其很难保存足够的语义信息，而Attention允许解码器根据当前不同的翻译内容，查阅输入句子的部分不同的单词或片段，以提高每个词或者片段的翻译精确度。具体做法为解码器在每一步的解码过程中，将查询编码器的隐藏状态。对于整个输入序列计算每一位置（每一片段）与当前翻译内容的相关程度，即权重。再根据这个权重对各输入位置的隐藏状态进行加权平均得到“context”向量（Encoder-Decoder框架向量c），该结果包含了与当前翻译内容最相关的原文信息。同时在解码下一个单词时，将context作为额外信息输入至RNN中，这样网络可以时刻读取原文中最相关的信息，而不必完全依赖于上一时刻的隐藏状态。对比社seq2seq,Attention本质上是通过加权平均，计算可变的上下文向量c。【<strong>挑重点</strong>】</p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513163542822.png" alt="image-20210513163542822" style="zoom:50%;" /></p><p>在注意力机制之后，针对机器翻译领域得到了进一步的发展，2016年谷歌针对多语言翻译任务提出了一种新的解决方案：singlemodel to translating multiplelanguages，更为强大的是其模型能够实现Zero-shot translation.2016年Google提出了GNMT,主要是针对网络的性能和翻译质量做进一步改进.</p><h1 id="transformer">3. Transformer:</h1><p>虽然GNMT在多语言的翻译问题上取得了很好的效果，然而由于其网络结构是基于传统的RNN、LSTM等序列建模的方式，其很难实现并行，训练时间较长。Google于2017年提出了一种全新的AttentionMechanism即Transformer.</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513160159216.png"alt="image-20210513160159216" /><figcaption aria-hidden="true">image-20210513160159216</figcaption></figure><p>从上图可以看出，我们希望self-attention能够在处理每一个输入的数据时能够考虑整个序列信息，同时最重要的是它可以并行计算，来提升训练速度。</p><p>self-attention与RNN的不同：</p><ol type="1"><li><p>self-attention考虑整个输入的信息，而RNN只考虑了左边的输入信息</p><p>双向RNN可以看作是考虑了整个输入信息</p></li><li><p>RNN很难考虑早期的输入信息</p></li><li><p>RNN不能并行处理</p></li></ol><h2 id="self-attention计算过程">3.1 self-attention计算过程：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513190101208.png" alt="image-20210513190101208" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210517113525617.png" alt="image-20210517113525617" style="zoom: 50%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513190136252.png" alt="image-20210513190136252" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513190146514.png" alt="image-20210513190146514" style="zoom:50%;" /></p><h2 id="multi-head">3.2 Multi-head：</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513194036821.png" alt="image-20210513194036821" style="zoom:50%;" /></p><p>多头注意力的提出，主要是考虑了不同输入之间的相关性可能不只是一种，还存在其他的相关性，所以尝试使用不同的矩阵对来表示不同的相关性。</p><h2 id="positional-encoding">3.3 Positional Encoding:</h2><p><img src="https://cdn.jsdelivr.net/gh/IYoreI/PicGo@main/img/image-20210513194628329.png" alt="image-20210513194628329" style="zoom:50%;" /></p><p>从上面的计算过程中，你会发现所有的输入之间似乎并没有体现出位置的差异，有一句话是这么说的“天涯若比邻”。所以我们需要在网络中体现出位置的差异，在Attentionis all youneed这篇论文中，作者就提出了位置编码的解决方案，通过给每一个输入添加一个位置信息来代表位置的差异。而这个位置编码是如何产生的呢？可以是手工设计也可以是学习出来的，所以这里也是一个可调整的空间。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建记录</title>
      <link href="post/96e0.html"/>
      <url>post/96e0.html</url>
      
        <content type="html"><![CDATA[<p>​最初计划是采购了阿里云服务器来学习搭建自己的博客网站，经过一段时间折腾，也终于搭建起来了，该文档记录了在搭建过程中实用的一些工具以及遇到的各种问题。运行了一段时间发现数据库不稳定，经过排查大概定位到是由于没有足够内存可以分配导致进程挂掉。阿里云的服务器采购的是最低配的，所以暂时没有做进一步处理了。近期时间我又在github上尝试搭建自己的网站，搭建成功后，可能会将域名解析到gtihub.io的网站上。</p><h1 id="初始化配置">1. 初始化配置：</h1><h2 id="nginx安装">1.1 nginx安装：</h2><ul><li><p>安装必须依赖：</p><pre class="shell"><code>yum -y install gcc pcre-devel zlib-devel openssl openssl-devel</code></pre></li><li><p>官网下载nginx：https://nginx.org/download/</p></li><li><p>到自定义目录下解压/etc/nginx/，并执行./configure命令成功后，再执行 make install命令</p><div class="sourceCode" id="cb2"><preclass="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./configure</span></span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> install   </span></code></pre></div></li></ul><span id="more"></span><ul><li><p>nginx启动命令在 /usr/local/sbin/nginx</p></li><li><p>如果需要可以在/etc/profile 配置环境变量</p><pre class="shell"><code>NGINX_HOME=&#39;/usr/local/nginx/sbin&#39;  source /etc/profile#执行nginx启动nginx# 关闭nginx命令：nginx -s stop# 目录设置为root用户组chown -R root:root /etc/nginx/nginx-1.19.8</code></pre></li><li><p>设置开机自启动:</p><pre><code>#1. 创建启动脚本：vi /etc/init.d/nginx# 脚本内容如下，注意修改自己的安装路径PATH和NAME#! /bin/bash# chkconfig: - 85 15PATH=/usr/local/nginxDESC=&quot;nginx daemon&quot;NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidSCRIPTNAME=/etc/init.d/$NAMEset -e[ -x &quot;$DAEMON&quot; ] || exit 0do_start() &#123;$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;&#125;do_stop() &#123;$DAEMON -s stop || echo -n &quot;nginx not running&quot;&#125;do_reload() &#123;$DAEMON -s reload || echo -n &quot;nginx can&#39;t reload&quot;&#125;case &quot;$1&quot; instart)echo -n &quot;Starting $DESC: $NAME&quot;do_startecho &quot;.&quot;;;stop)echo -n &quot;Stopping $DESC: $NAME&quot;do_stopecho &quot;.&quot;;;reload|graceful)echo -n &quot;Reloading $DESC configuration...&quot;do_reloadecho &quot;.&quot;;;restart)echo -n &quot;Restarting $DESC: $NAME&quot;do_stopdo_startecho &quot;.&quot;;;*)echo &quot;Usage: $SCRIPTNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2exit 3;;esacexit 0#3. 设置执行权限：hmod a+x /etc/init.d/nginx#4. 注册服务：chkconfig --add nginx#5. 设置开机自启动：chkconfig nginx on</code></pre></li></ul><h2 id="mysql安装">1.2 mysql安装：</h2><p>​ 参考资料： https://www.jianshu.com/p/276d59cbc529</p><ul><li><p>检查是否安装mysql：</p><pre class="shell"><code>rpm -qa | grep mysql#如果不出现任何内容说明没有安装，如果有安装则参考如下步骤完全卸载：rpm -e --nodeps mysql-libs-5.1.73-5.el6_6.x86_64#2. 查询所有mysql对应的文件夹：whereis mysql#3. 删除相关目录或文件rm -rf /usr/lib64/mysql /usr/share/mysql</code></pre></li><li><p>检查mysql用户组和用户是否存在，若不存在则创建：</p><pre class="shell"><code>cat /etc/group | grep mysqlcat /etc/passwd |grep mysql#添加groupadd mysqluseradd -r -g mysql mysql</code></pre></li><li><p>从官网下载mysql5.7的安装包[从官网选择合适版本：https://downloads.mysql.com/archives/community/]：</p><pre class="shell"><code>wget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.32-linux-glibc2.12-x86_64.tar.gz#解压后移动到/usr/local目录下,重命名为mysqlmv mysql-5.7.32-linux-glibc2.12-x86_64/ /usr/local/cd /usr/local/mv mysql-5.7.32-linux-glibc2.12-x86_64/ mysql/</code></pre></li><li><p>创建data目录：</p><pre class="shell"><code>cd /usr/local/mysqlmkdir data# 更改mysql目录下所有的目录及文件夹所属的用户组和用户，以及权限chown -R mysql:mysql /usr/local/mysqlchmod -R 755 /usr/local/mysql</code></pre></li><li><p>编译安装初始化：</p><pre class="shell"><code>#在mysql的bin目录下执行：./mysqld --initialize --user=mysql --datadir=/usr/local/mysql/data --basedir=/usr/local/mysql#初始化后会生成临时的数据库密码，要保存： sski+dj=7VXg</code></pre><ul><li><p>初始化遇到错误：</p><p>./mysqld: error while loading shared libraries: libaio.so.1: cannotopen shared object file: No such file or directory</p><pre class="shell"><code>#1. 检查该链接库有没有安装rpm -qa|grep libaio #2. 如果没有该库文件，则安装：yum install  libaio-devel.x86_64</code></pre></li></ul></li><li><p>修改配置文件my.cnf:</p><pre class="shell"><code>#1. 设置数据文件存储路径datadir=/usr/local/mysql/data#2. 配置端口号port=3306#3.设置数据库默认字符编码character_set_server=utf8</code></pre></li><li><p>测试启动mysql：</p><pre class="shell"><code>/usr/local/mysql/support-files/mysql.server start</code></pre><ul><li><p>启动遇到错误：</p><ol type="1"><li>The server quit without updating PID file(/vusr/local/mysq[FAILED]zbp159tpwttz612qarv11z.pid).</li></ol><p>本错误是由于my.cnf文件中datadir路径配置不正确导致的，/usr 误写为/vusr....</p><ol start="2" type="1"><li>如果不是由于配置错误导致的，可以尝试下面解决方法：</li></ol><pre class="shell"><code>#查询服务ps -ef|grep mysql | grep -v grepps -ef|grep mysqld | grep -v grep#结束进程kill -9 PID#启动服务 /usr/local/mysql/support-files/mysql.server start</code></pre><ol start="3" type="1"><li>mysqld_safe error: log-error set to '/var/log/mariadb/mariadb.log',however file don't exists:</li></ol><pre class="shell"><code># 因为没有路径也没有权限，所以创建此路径并授权给mysql用户mkdir /var/log/mariadbtouch /var/log/mariadb/mariadb.log# 用户组及用户chown -R mysql:mysql /var/log/mariadb/#尝试重新启动：/usr/local/mysql/support-files/mysql.server start</code></pre><ol start="4" type="1"><li>mysqld_safe Directory '/var/lib/mysql' for UNIX socket file don'texists.：</li></ol><pre class="shell"><code># 该路径是配置在my.cnf中的socket参数的值，一样是因为没有权限mkdir /var/lib/mysql# 用户组及用户chown -R mysql:mysql /var/lib/mysql#尝试重新启动：/usr/local/mysql/support-files/mysql.server start</code></pre></li></ul></li><li><p>添加软连接并重启mysql</p><pre class="shell"><code>ln -s /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqlln -s /usr/local/mysql/bin/mysql /usr/bin/mysqlservice mysql restart</code></pre></li><li><p>登录mysql，修改临时密码：</p><pre><code>1.# mysql -uroot -p</code></pre><p>结果无法登录，提示： ERROR 2002 (HY000): Can't connect to local MySQLserver through socket '/tmp/mysql.sock' (2)</p><pre><code># 结果方法：通过find / -name mysql.sock 查看该文件的位置# 通过ln -s 做软连接ln -s /var/lib/mysql/mysql.sock /tmp/mysql.sock</code></pre><p>修改临时密码：</p><pre class="mysql"><code>set password for root@localhost = password(&quot;root&quot;);</code></pre></li><li><p>开放远程连接：</p><pre class="mysql"><code>use mysql;update user set user.Host=&#39;%&#39; where user.User=&#39;root&#39;;flush privileges;</code></pre></li><li><p>设置开机自启动：</p><pre class="shell"><code>cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqldchmod +x /etc/init.d/mysqldchkconfig --add mysqldchkconfig --list</code></pre></li></ul><h2 id="jdk安装">1.3 jdk安装：</h2><ul><li><p>查询要安装jdk的版本：</p><pre class="shell"><code>yum -y list java*</code></pre></li><li><p>安装jdk1.8：</p><pre class="shell"><code>yum install -y java-1.8.0-openjdk.x86_64</code></pre></li><li><p>查询jdk版本：</p><pre class="shell"><code>java -version</code></pre></li></ul><h1 id="centos7搭建wordpress">2. centos7搭建wordpress:</h1><h2 id="安装web服务">2.1 安装web服务：</h2><ul><li><p>安装apache web服务器：</p><p>注意： apache和nginx二选一即可。</p><pre class="shell"><code>yum insall httpd# 安装目录为： /etc/httpd# 启动关闭命令：/usr/sbin/apachectl start /usr/sbin/apachectl stop# 查看默认80端口监听状态：netstat -antlp | grep 80# 也可以通过网址访问查看是否运行正常# 可以在/etc/httpd/conf/httpd.conf 下查看首页配置信息。# 通过一下命令查找首页位置：find / -name index.html# 卸载httpd：yum erase httpd</code></pre></li></ul><h2 id="安装php">2.2 安装php：</h2><ul><li><p>安装php环境：</p><pre class="shell"><code>yum install phpyum install php-mysqlyum install  php-gd php-imap php-ldap php-odbc php-pear php-xml php-xmlrpc# 安装完成后php -v 显示版本是5.4 需要升级到7.x</code></pre><ul><li><p>php版本升级操作：</p><pre class="shell"><code>#1. 查看当前 PHP 相关的安装包 yum list installed | grep php#2. 更换 RPM 源  centos7执行如下命令rpm -Uvh https://mirror.webtatic.com/yum/el7/epel-release.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm#3. 关闭 php-fpm 和 nginx 服务service php-fpm stopnginx -s quit#4. 删除已经安装的php相关包yum remove php*# 安装php7.4：#1. 添加EPEL和REMI存储库yum install epel-releaseyum -y install https://rpms.remirepo.net/enterprise/remi-release-7.rpm#2. 可以启用PHP 7.4 Remi存储库yum -y install yum-utilsyum repolist all |grep phpyum-config-manager --enable remi-php74#3. 安装php7.4yum install php  php-cli php-fpm php-mysqlnd php-zip php-devel php-gd php-mcrypt php-mbstring php-curl php-xml php-pear php-bcmath php-json php-redis#4. 查看版本：php -v</code></pre></li><li><p>测试：</p><p>在/var/www/html下新建一个文件test.php，内容如下：</p><?phpphpinfo();?><p>然后重启一下httpd服务，在浏览器访问ip/test.php查看</p></li></ul></li><li><p>pfp-fpm操作命令：</p><pre class="shell"><code>#fpm默认占用的是9000端口yum install php-fpm;#查看是否安装成功php-fpm -v;#启动php-fpmsystemctl start php-fpmservice php-fpm status</code></pre></li><li><p>php安装问题：</p><ul><li><p>安装路径：</p><p>如果采用RPM包安装，安装路径应在/etc/目录下。php的配置文件:/etc/php.ini</p></li><li><p>php-fpm启动失败问题：</p><ul><li><ol type="1"><li><p>配置文件修改：</p><p>相关配置文件路径： /etc/php.ini /etc/php-fpm.conf/etc/php-fpm.d/www.conf</p><pre class="shell"><code># php.inicgi.fix_pathinfo=0# www.conf[www]listen = /tmp/php-cgi-74.socklisten.backlog = -1listen.allowed_clients = 127.0.0.1listen.owner = wwwlisten.group = wwwlisten.mode = 0666user = wwwgroup = wwwpm = dynamicpm.max_children = 100pm.start_servers = 20pm.min_spare_servers = 20pm.max_spare_servers = 70request_terminate_timeout = 100request_slowlog_timeout = 30slowlog = /var/log/slow.log</code></pre></li></ol></li><li><ol start="2" type="1"><li>php-fpm启动失败：</li></ol><p>ERROR: [pool www] cannot get uid for user 'www'：错误信息提示没有www用户</p><pre class="shell"><code># 解决方法： 添加www用户useradd www</code></pre><p>ERROR: [pool www] please specify user and group o... root</p><p>提示需要验证用户和用户组，因为原来是www用户组，现在改为了root</p><pre class="shell"><code># 解决方法1：# 通过php-fpm命令启动指定-R参数  不是systemctl start fpm/usr/sbin/php-fpm -R </code></pre></li></ul></li></ul></li></ul><h2 id="安装wordpress">2.3 安装wordpress：</h2><ul><li><p>从官网下载wordpress,并解压到/var/www/html/目录下</p><pre class="shell"><code>wget https://cn.wordpress.org/latest-zh_CN.tar.gz</code></pre></li><li><p>修改配置信息：</p><pre><code>#1. 复制配置文件cp wp-config-sample.php wp-config.php#2. 配置数据库信息vim  wp-config.php#3. 需要现在mysql中创建好数据库 wordpresscreate database wordpress</code></pre></li><li><p>访问网址： http://xxxxx/wp-admin/install.php 开始安装。</p></li></ul><h1 id="网站搭建问题">3. 网站搭建问题：</h1><h2 id="nginx使用问题">3.1 nginx使用问题：</h2><ul><li><p>修改了配置，确始终不生效的问题：由于nginx又两个配置文件，一个是安装的一个运行的，所以一定要确保修改的是运行的配置文件。</p><pre class="shell"><code># 通过命令查找find / -name nginx.conf</code></pre></li><li><p>解决了首页请求路径的配置问题，访问首页出现了500状态码，查看nginx错误日志如下：</p><p>rewrite or internal redirection cycle while internally redirecting to"/"</p><p>还是nginx配置文件不当引起的问题。</p><pre class="shell"><code># 正确的配置文件如下：server&#123;    listen 80;    server_name your_server-name; # 你的域名    location / &#123;       root /var/www/html; # 你的网站根目录       index index.php;    # 首页       #try_files $uri $uri/ /index.php?q=$uri&amp;$args;       try_files $uri $uri/ =404;    &#125;    location ~ \.php$ &#123;        root /var/www/html;        index index.php;        #fastcgi_pass 127.0.0.1:9000;        fastcgi_pass unix:/tmp/php-cgi-74.sock;        fastcgi_index  index.php;        fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name;        include fastcgi_params;        #try_files $uri =404;    &#125;    location ~ /\.ht &#123;        deny  all;    &#125;&#125;</code></pre></li></ul><h2 id="wordpress使用问题">3.2 wordpress使用问题：</h2><ul><li><p>没有wp-content文件的写权限：</p><pre><code>#1. 将文件目录的用户组和httpd的用户组保持一致chown -R root:root wp-content#2. 赋予读写权限chmod -R 777 wp-content</code></pre></li><li><p>上传图像时，提示裁剪图像时出现错误：</p><pre class="shell"><code>#1. 原因是缺少 PHP GD 库 确认版本号  centos下执行如下命令：yum install php-gd#2. 查看go.so所在位置rpm -qal | grep gd.so# 查找结果如下：/usr/lib64/php-zts/modules/gd.so/usr/lib64/php/modules/gd.so/usr/lib64/libgd.so.3/usr/lib64/libgd.so.3.0.10#3. 在/etc/php.ini文件中  [gd]配置选项最后添加如下内容extension=/usr/lib64/php-zts/modules/gd.so</code></pre></li><li><p>wordpress安装插件时 出现ftp认证：</p><pre class="shell"><code>#1. 在wp-contenet目录下创建tmp目录cd /var/www/html/wp-content/mkdir tmp# 设置tmp目录为777chmod 777 tmp/#2. 修改wp-config.php文件 在原有的ABSPATH下新增如下四行配置if ( ! defined( &#39;ABSPATH&#39; ) ) &#123;        define( &#39;ABSPATH&#39;, __DIR__ . &#39;/&#39; );&#125;define(&#39;WP_TEMP_DIR&#39;, ABSPATH.&#39;wp-content/tmp&#39;);define(&quot;FS_METHOD&quot;, &quot;direct&quot;);define(&quot;FS_CHMOD_DIR&quot;, 0777);define(&quot;FS_CHMOD_FILE&quot;, 0777);</code></pre></li><li><p>在线安装插件失败问题：</p><p>nginx配置文件新增如下参数：</p><pre class="shell"><code>large_client_header_buffers 4 16k;client_max_body_size 30m;client_body_buffer_size 128k;fastcgi_connect_timeout 300;fastcgi_read_timeout 300;fastcgi_send_timeout 300;fastcgi_buffer_size 64k;fastcgi_buffers   4 32k;fastcgi_busy_buffers_size 64k;fastcgi_temp_file_write_size 64k;</code></pre><p><strong>选择离线安装的方式：将插件zip包解压到wp-content/plugins目录下</strong></p></li><li><p>https配置：</p><p>证书申请：</p><p>阿里云购买的服务器提供免费的dv单域名证书，自己可以通过如下方式申请：</p><p>(1)云盾控制台 按步骤申请免费的dv证书或者(2)域名控制台--&gt;管理域名--&gt; 开启ssl证书</p><p>两种方式都可以，签发证书后 下载对应的证书</p><p>证书签发后，在nginx配置ssl证书：</p><ul><li><p>wordpress.conf 添加如下配置信息：</p><pre><code>listen 443 ssl;ssl_certificate      /usr/local/nginx/ssl/XXX.pem; # pem文件位置ssl_certificate_key  /usr/local/nginx/ssl/XXX.key; # key文件位置ssl_session_timeout  5m;ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;ssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_prefer_server_ciphers  on;</code></pre><p>如果启动nginx提示没有ssl模块，则需要先安装一下：</p><pre class="shell"><code>#1. 在nginx解压的目录下，找到configure命令，执行：./configure --with-http_ssl_module#2. 执行make编译make#3. 为了保险可以先备份原来安装后的nginx文件[这个是在安装目录] /usr/local/nginx/sbin/nginxcp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak#4. 关闭nginx服务，替换nginx文件，将刚才在解压目录下编译后生成的./objs/nginx文件替换到安装目录下cp ./objs/nginx /usr/local/nginx/sbin/#5. 查看安装结果：这时会看到又http_ssl_module/usr/local/nginx/sbin/nginx -V#6. 重启nginx</code></pre></li></ul></li></ul><h2 id="wordpress插件">3.3 wordpress插件：</h2><ul><li><p>安装哪些插件：</p><ol type="1"><li><strong>WP-China-Yes</strong> ： 能够解决在线安装的问题</li><li><strong>WP Githuber MD</strong>： markdown语法插件</li><li><strong>WP-PostViews</strong>： 访问量统计插件</li></ol></li><li><p>如何实现网站访问次数统计：</p><ol type="1"><li><p>需要安装插件： <strong>WP-PostViews</strong></p></li><li><p>在外观-&gt;主题编辑器中 找到首页index.php 找到：</p><?php while (have_posts()) : the_post(); ?><p>行 在后面添加如下代码：</p><p>if(function_exists('the_views')) { the_views(); }</p></li></ol></li><li><p>网站隐藏管理路径：</p><p>Hide My WP插件</p><p><strong>https://yore.cc/wp-login.php?hmw_disable=337866</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实用工具记录</title>
      <link href="post/6b2b.html"/>
      <url>post/6b2b.html</url>
      
        <content type="html"><![CDATA[<h1 id="一-常用工具">一、 常用工具：</h1><h2 id="typora-picgo-github">1. Typora + picGo + gitHub</h2><p>​ 搭建自己的图床，方便文档随时访问图片内容。</p><h2 id="xmind">2. Xmind:</h2><p>​ 思维导图</p><h2 id="坚果云">3. 坚果云：</h2><p>​小文件多设备、跨平台同步和共享，同时还支持多人共享，非常方便，个人版每月有免费流量，平时的文件同步基本够用。</p><h2 id="citavi">4. Citavi:</h2><p>​个人目前使用的文献管理工具，可以支持文献归类，在阅读时做批注，目前就用到这些功能。</p><span id="more"></span><h2 id="git私服">5. git私服：</h2><p>​ 基于gogs搭建git私服</p><p>​ 基于mindoc搭建文档管理系统</p>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL命令</title>
      <link href="post/6a5a.html"/>
      <url>post/6a5a.html</url>
      
        <content type="html"><![CDATA[<h1 id="一mysql命令">一、MySQL命令：</h1><p><font color='red'><strong>针对mysql5.7版本</strong></font></p><h2 id="常用命令">1. 常用命令：</h2><h3 id="权限管理相关操作">1.1 权限管理相关操作：</h3><pre class="mysql"><code>SELECT Host,User,Password FROM mysql.user; # 更改用户名密码：update mysql.user set authentication_string=password(&quot;新密码&quot;) where User=&quot;test&quot; and Host=&quot;localhost&quot;;#刷新权限命令:flush privileges;#注意plugin字段，如果发现手动设置了密码后，依然可以无密码进入，需要执行如下命令，设置plugin字段为mysql_native_password&quot;update mysql.user set plugin=&quot;mysql_native_password&quot; where User=&quot;root&quot; and Host=&quot;localhost&quot;;#mysql  数据源配置加密密码实现：调用ConfigTools.encrypt方法获取加密后的密码#新增一行记录 限制ip 用户 访问密码INSERT INTO mysql.user(Host,User,Password) VALUES(&quot;10.10.0.1&quot;,&quot;root&quot;,PASSWORD(&quot;root&quot;));#刷新权限命令:flush privileges;#赋权限命令：   GRANT ALL ON *.* TO root@&#39;10.10.0.1&#39; IDENTIFIED BY &#39;root&#39;;#查看权限：     show Grants for &#39;root&#39;@&#39;10.10.0.1&#39;;</code></pre><span id="more"></span><h3 id="权限分配">1.2 权限分配：</h3><pre class="mysql"><code>GRANT语法：        GRANT 权限 ON 数据库.* TO 用户名@&#39;登录主机&#39; IDENTIFIED BY &#39;密码&#39;  权限：     ALL,ALTER,CREATE,DROP,SELECT,UPDATE,DELETE     新增用户：权限为USAGE,即为：&quot;无权限&quot;,想要创建一个没有权限的用户时,可以指定USAGE  数据库：       *.*              表示所有库的所有表       mylove.*         表示mylove库的所有表       mylove.loves     表示mylove库的loves表   用户名：       MySQL的账户名  登陆主机：       允许登陆到MySQL Server的客户端ip       &#39;%&#39;表示所有ip       &#39;localhost&#39; 表示本机       &#39;10.155.123.55&#39; 特定IP  密码：        MySQL的账户名对应的登陆密码  </code></pre><h3 id="数据导入导出">1.3数据导入导出：</h3><pre class="mysql"><code>导出数据库：  mysqldump -u 用户名 -p  数据库名 &gt; 数据库名.sql导入数据库：    1、首先建空数据库    mysql&gt;create database abc;        2、导入数据库    方法一：    （1）选择数据库    mysql&gt;use abc;    （2）设置数据库编码    mysql&gt;set names utf8;    （3）导入数据（注意sql文件的路径）    mysql&gt;source /home/abc/abc.sql;    方法二：    mysql -u用户名 -p密码 数据库名 &lt; 数据库名.sql    #mysql -uabc_f -p abc &lt; abc.sql指定表数据导出：        mysqldump -u user -p dbname tablename &gt; db.sql</code></pre><h3 id="表结构修改">1.4 表结构修改：</h3><p>mysql 已有表新增字段操作</p><pre class="mysql"><code>#如果想在一个已经建好的表中添加一列，可以用诸如：alter table TABLE_NAME add column NEW_COLUMN_NAME varchar(20);#这条语句会向已有的表中加入新的一列，这一列在表的最后一列位置。如果我们希望添加在指定的一列，可以用：alter table TABLE_NAME add column NEW_COLUMN_NAME varchar(20) not null after COLUMN_NAME;#注意，上面这个命令的意思是说添加新列到某一列后面。如果想添加到第一列的话，可以用：alter table TABLE_NAME add column NEW_COLUMN_NAME varchar(20) not null first;</code></pre><p>查看表字段信息：</p><pre class="mysql"><code>show  full columns from table_name1;</code></pre><h3 id="编码">1.5 编码：</h3><p>查看编码： show variables like 'char%';</p><pre class="mysql"><code>windows修改编码为utf8： my-default.ini文件    复制一份ini文件命名为my.ini添加如下内容：    [client]    default-character-set=utf8    [mysqld]下增加：    character-set-server=utf8    重启mysql：</code></pre><h2 id="调优">5. 调优：</h2><h3 id="使用profiling性能分析工具">5.1 使用profiling性能分析工具：</h3><pre class="mysql"><code># 首先查看profiling 是否是on状态； //show variables;show variables like &#39;profiling&#39;;# 如果是OFF状态,设置值为1set profiling=1;# 执行你的sql语句，然后通过下面的命令查看sql执行耗时show profiles;</code></pre><h3 id="开启慢查询">5.2 开启慢查询：</h3><ul><li><p>临时设置，重启失效:</p><pre class="mysql"><code># 将slow_query_log 全局变量设置为“ON”状态set global slow_query_log=&#39;ON&#39;;# 关闭set global slow_query_log=&#39;OFF&#39;;#置慢查询日志存放的位置(默认在 mysql data 目录下)set global slow_query_log_file=&#39;slow.log&#39;;# 超时1秒的sql语句set global long_query_time=1;#查看变量值show variables like &#39;%query_log&#39;;#注意： 无需重启即可生效，但重启会失效。set global 改的参数是暂时的，想重启不变请配合修改mysql配置文件</code></pre></li><li><p>修改配置文件：</p><pre class="mysql"><code># 修改配置文件my.cnf(Windows 下是my.ini)，在[mysqld]下的下方加入slow_query_log = ONslow_query_log_file = slow.loglong_query_time = 1</code></pre><p>慢查询日志文件记录在my.cnf中配置的data目录下。</p></li></ul><h3 id="explain-分析sql语句">5.3 Explain 分析sql语句：</h3><p>explain 字段解析：</p><pre><code>1. id //select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序2. select_type //查询类型3. table //正在访问哪个表4. partitions //匹配的分区5. type //访问的类型6. possible_keys //显示可能应用在这张表中的索引，一个或多个，但不一定实际使用到7. key //实际使用到的索引，如果为NULL，则没有使用索引8. key_len //表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度9. ref //显示索引的哪一列被使用了，如果可能的话，是一个常数，哪些列或常量被用于查找索引列上的值10. rows //根据表统计信息及索引选用情况，大致估算出找到所需的记录所需读取的行数11. filtered //查询的表行占表的百分比12. Extra //包含不适合在其它列中显示但十分重要的额外信息</code></pre><h3 id="添加查看删除索引">5.4 添加查看删除索引：</h3><pre class="mysql"><code>#----------------添加删除索引-------------------ALTER TABLE `user_class` ADD INDEX user_id_index (`user_id`) CREATE INDEX index_name ON table_name (column_list)drop index index_name on user_class#-------------查看索引----------show index from user_class</code></pre><h2 id="问题集">6. 问题集：</h2><ul><li><p>使用group by 遇到： sql_mode=only_full_group_by 错误提示：</p><pre class="mysql"><code># 查看sql_mode值SELECT @@sql_mode;# 查询结果：ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION# 去掉ONLY_FULL_GROUP_BY 重新赋值#(1)修改全局值，只对修改后新建的数据库生效set @@global.sql_mode =&#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#39;;#(2) 修改当前数据库：set sql_model = &#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#39;;</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git使用记录</title>
      <link href="post/4a02.html"/>
      <url>post/4a02.html</url>
      
        <content type="html"><![CDATA[<h1 id="git使用配置记录">git使用配置记录：</h1><h2 id="初始配置项">1. 初始配置项：</h2><ol type="1"><li><p>修改本地bash路径（不使用默认的）</p><p>针对git2.0以上，“环境变量”中用户变量添加一个“Home”变量，把你要的目录作为值</p></li><li><p>设置你的用户名称与邮件地址</p><pre class="shell"><code>git config --global user.name &quot;your name&quot;git config --global user.email XXX@126.com# 列出当前配置项git config –list# 检查某一项配置  比如 user.namegit config user.name </code></pre><span id="more"></span></li><li><p>中文乱码：</p><pre class="shell"><code>git config --global core.quotepath false          # 显示status编码git config --global gui.encoding utf-8            # 图形界面编码git config --global i18n.commit.encoding utf-8    # 提交信息编码git config --global i18n.logoutputencoding utf-8  # 输出log编码# `git log` 默认使用 `less` 分页，所以需要 `bash` 对 `less` 命令进行 `utf-8` 编码export LESSCHARSET=utf-8# 让 `ls` 命令可以显示中文名称vim %GIT_HOME%\mingw64\share\git\completion\git-completion.bash# 在文件末尾处添加一行alias ls=&quot;ls --show-control-chars --color&quot;</code></pre></li><li><p>配置ssh密钥：</p><pre class="shell"><code>ssh-keygen -t rsa -C &quot;XXX@126.com&quot;</code></pre></li></ol><h2 id="入门使用操作本地仓库">2. 入门使用操作(本地仓库)：</h2><ol type="1"><li><p>创建仓库：</p><pre class="shell"><code>#本地创建一个仓库mkdir git-opcd git-op</code></pre></li><li><p>初始化：</p><pre class="shell"><code># 在git-op目录下执行 git init# 会在当前目录下生成  .git目录  （这个目录是Git来跟踪管理版本库的，勿删！）</code></pre></li><li><p>提交操作：</p><pre class="shell"><code># 在当前目录下创建一个readme.txt文件 写测试内容# 添加文件git add readme.txt # 查看文件状态 git status# 提交文件到本地仓库  -m 后面输入的是本次提交的说明，可以输入任意内容git commit -m &quot;wrote a readme file&quot;#查看文件具体修改内容git diff readme.txt</code></pre></li><li><p>代码回退：</p><pre class="shell"><code># 查看历史提交记录  过滤信息参数   --pretty=onelinegit log# 回退：#1. 回退到上一个版本git reset --hard HEAD^ #2. 回退到往上100个版本git reset --hard HEAD~100 #3. 如果窗口没有关闭 还可以通过版本号  回复到回退之前的状态git reset --hard +版本号#4. 如果关闭了窗口 还可以挽救  通过 git reflog (用来记录你的每一次命令) 来找回对应的commit id</code></pre></li><li><p>修改管理：</p><p>git对代码的管理分为几个区： <font color=red><strong>工作区(add)--&gt; 暂存区 (commit)--&gt; master分支</strong></font></p><p>每次commit操作只会将工作区变化的内容 放到暂存区。</p><p>用 git diff HEAD -- readme.txt命令可以查看工作区和版本库里面最新版本的区别：</p><pre class="shell"><code># 1.撤销工作区的修改git checkout -- readme.txt     # 2.撤销暂存区的修改git reset HEAD  &lt;file&gt;        #可以把暂存区的修改撤销掉（unstage），重新放回工作区：git reset HEAD readme.txt</code></pre><p>（git checkout --file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令）一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。若想要撤销工作区的所有文件的修改，执行： git checkout</p></li><li><p>删除文件：</p><pre class="shell"><code># 1. 删除缓存git rm -r -f --cached ./# 2. 通过rm删除本地文件，这个时候通过git status查看状态，git会提示你工作区和版本库不一致了 哪些文件被删除了# 有两个选择:# ①确实要删除文件：git rm 1.txtgit commit -m &quot;delte file 1.txt&quot;# ②误删，找回文件git checkout -- 1.txt</code></pre></li></ol><h2 id="远程仓库连接">3. 远程仓库连接：</h2><ol type="1"><li><p>初始配置:</p><pre class="shell"><code>#1.  添加远程库git remote add  origin git@github.com:IYoreI/git-op.git            # 远程库的名字就是  origin# 查看当前添加的远程库git remote# 查看当前添加库的详细信息git remote -v#2. 删除远程库git remote rm origin</code></pre></li><li><p>首次推送:</p><pre class="shell"><code>git push -u origin master</code></pre><p>(由于远程库是空的，我们第一次推送master分支时，加上了-u参数,Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令)</p></li><li><p>后续推送：</p><pre class="shell"><code>git push origin master#首次上传 需要确认GitHub的Key  输入yes即可</code></pre></li><li><p>从远程仓库克隆到本地库：</p><pre class="shell"><code>git clone https://github.com/IYoreI/Algorithm.git</code></pre></li><li><p>更新远程代码到本地仓库:</p><pre class="shell"><code>git fetch origin master #从远程的origin仓库的master分支下载代码到本地的origin mastergit pull origin master</code></pre></li></ol><h2 id="分支管理">4. 分支管理：</h2><ol type="1"><li><p>创建分支与合并分支：</p><p>（1） 创建分支dev ```shell git checkout -b dev</p><p>#git checkout命令加上-b参数表示创建并切换，相当于以下两条命令：</p><p>$ git branch dev $ git checkout dev ```</p><p>（2）命令查看当前分支：</p><pre class="shell"><code>git branch          #查看本地分支git branch -a  #查看所有分支（包括远程仓库）</code></pre><p>（3）分支切换：</p><pre class="shell"><code>git checkout master  </code></pre><p>（4）分支合并、dev分支的工作成果合并到master分支</p><pre class="shell"><code>git merge dev       （git merge命令用于合并指定分支到当前分支）</code></pre><p>（5）合并完成之后就可以删除 dev分支了 ```shell #删除本地分支： gitbranch -d dev</p><p>#删除远程分支： git push origin --delete dev 【git push origin --参数远程分支名称】 ```</p></li><li><p>解决冲突：</p><p>当我们基于同一个master版本创建出不同的分支并且在不同分支上都对同一个文件进行了修改 这时提交时就会出现冲突</p></li><li><p>合并分支：</p><p>通常，合并分支时，如果可能，Git会用Fastforward模式，但是这种模式下，删除分支后，会丢掉发扽之信息，如果想要强制禁用Fastforward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。</p><p>git merge --no-ff -m "merge with no-ff" dev</p></li><li><p>保存现场：</p><p>当在开发任务过程中，遇到紧急bug需要修复，此时你的dev分支只完成了部分开发任务不能提交，但是占用着工作区，可以使用stash命令保存现场。</p><pre class="shell"><code># Git还提供了一个stash功能，可以把当前工作现场“储藏”起来git stash#临时任务开发完成后，切换回dev分支，恢复现场：    (1) 用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除；    (2) 用git stash pop，恢复的同时把stash内容也删了：#你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令：git stash apply stash@&#123;0&#125;</code></pre></li><li><p>更新特定提交：</p><pre><code>#从master 更新一个特定的提交到dev3Git专门提供了一个cherry-pick命令，让我们能复制一个特定的提交到当前分支git cherry-pick 4c805e2</code></pre></li><li><p>创建远程的dev分支到本地:</p><pre class="shell"><code>git checkout -b dev origin/dev</code></pre></li><li><p>推送分支</p><pre class="shell"><code>#把新建的本地分支push到远程服务器，远程分支与本地分支同名（当然可以随意起名):git push origin dev:dev</code></pre></li><li><p>版本回退:</p><pre><code>#1 查看提交日志git log  #2 目标版本号  命令将版本回退：git reset --hard # 推送git push # 强推git push origin master -f</code></pre></li></ol><h2 id="出坑指南">5. 出坑指南：</h2><ol type="1"><li><p>换行符问题：</p><p>执行git add file 时 提示 warning: LF will be replaced by CRLF inreadme.txt.</p><p>解决方法：</p><pre class="shell"><code>$ rm -rf .git  // 删除.git  $ git config --global core.autocrlf false  //禁用自动转换 #然后重新执行$ git init    $ git add 文件名</code></pre></li><li><p>使用git push origin master向github推送代码，出现git@github.com:Permission denied</p><p>（1）查看调试信息 # ssh -v git@github.com（2）查看~/.ssh/id_rsa.pub文件 more ~/.ssh/id_rsa.pub （3）把本机ssh-key拷贝到你github网站的ssh key里，在github的右上角edit yourprofile 里找到ssh key，然后add sshkey，把东西拷贝到key就可以了，title随便填</p></li><li><p>git push -u origin master</p><p>! [rejected] master -&gt; master (fetch first)</p><p>分析：出现这个问题是因为github中的README.md文件不在本地代码目录中，可以通过如下命令进行代码合并</p><pre class="shell"><code>git pull --rebase origin master#然后 git push origin master</code></pre></li><li><p>git merge origin/master 出现 already up to date</p><p>git pull 也无用 通过git log 发现最近一次提交出现冲突</p></li></ol><h1 id="github多人协同操作记录">github多人协同操作记录：</h1><h2 id="fork项目基本操作">1. fork项目基本操作：</h2><p>​ 参考： https://www.cnblogs.com/eyunhua/p/8463200.html</p><p>​ <strong>1.1 通过fetch命令拉取原仓库的更新</strong></p><ul><li><p>配置当前fork仓库的源地址：</p><pre class="shell"><code>git remote add upstream &lt;原仓库github地址&gt;</code></pre></li><li><p>获取原仓库的更新。使用fetch更新，fetch后会被存储在一个本地分支upstream/master上：</p><pre class="shell"><code>git fetch upstream</code></pre></li><li><p>合并到本地分支。切换到本地master分支，合并upstream/master分支：</p><pre class="shell"><code>git merge upstream/master</code></pre></li><li><p>这时候使用git log就能看到原仓库的更新了：</p><pre class="shell"><code>git log</code></pre></li><li><p>如果需要自己github上的fork的仓库需要保持同步更新，执行gitpush进行推送:</p><pre class="shell"><code>git push origin master</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tool </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
